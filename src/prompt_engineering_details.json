{
    "name": "Introduction to Prompt Engineering",
    "modules": [
        {
            "name": "Introduction to Prompts",
            "concepts": [
                {
                    "name": "What are Prompts?",
                    "description": "Understanding the fundamental concept of prompts in the context of Large Language Models.",
                    "tasks": [
                        {
                            "name": "Defining Prompts",
                            "description": "Explain what a prompt is in simple terms. Highlight that prompts are inputs (usually text) that guide the model to generate a desired output.",
                            "type": "learning_material",
                            "details": {
                                "content": "When thinking about a large language model input and output, a text prompt (sometimes accompanied by other modalities such as image prompts) is the input the model uses to predict a specific output. You don't need to be a data scientist or a machine learning engineer - everyone can write a prompt. However, crafting the most effective prompt can be complicated. Many aspects of your prompt affect its efficacy: the model you use, the model's training data, the model configurations, your word-choice, style and tone, structure, and context all matter. Therefore, prompt engineering is an iterative process. Inadequate prompts can lead to ambiguous, inaccurate responses, and can hinder the model's ability to provide meaningful output.\n\nWhen you chat with the Gemini chatbot,\u00b9 you basically write prompts, however this whitepaper focuses on writing prompts for the Gemini model within Vertex Al or by using the API, because by prompting the model directly you will have access to the configuration such as temperature etc.\n\nRemember how an LLM works; it's a prediction engine. The model takes sequential text as an input and then predicts what the following token should be, based on the data it was trained on. The LLM is operationalized to do this over and over again, adding the previously predicted token to the end of the sequential text for predicting the following token. The next token prediction is based on the relationship between what's in the previous tokens and what the LLM has seen during its training.\n\nWhen you write a prompt, you are attempting to set up the LLM to predict the right sequence of tokens. Prompt engineering is the process of designing high-quality prompts that guide LLMs to produce accurate outputs. This process involves tinkering to find the best prompt, optimizing prompt length, and evaluating a prompt's writing style and structure in relation to the task. In the context of natural language processing and LLMs, a prompt is an input provided to the model to generate a response or prediction."
                            }
                        },
                        {
                            "name": "Prompt Examples",
                            "description": "Provide examples of prompts, like asking a question, starting a story, or giving instructions. Ask them to come up with more examples",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Based on your understanding of prompts, provide three original examples of prompts you could use with a large language model.  Explain briefly what you expect the model to do in response to each of your prompts.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Prompt Quality Assessment",
                                            "criteria": [
                                                {
                                                    "name": "Relevance",
                                                    "description": "The prompt is appropriate for a language model and aligns with common LLM tasks.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Clarity",
                                                    "description": "The prompt is clearly worded and leaves little room for misinterpretation.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Creativity",
                                                    "description": "The prompt demonstrates original thought and explores diverse applications of language models.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Explanation",
                                                    "description": "The explanation of the expected model response is clear, logical, and accurate.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "This question tests the learner's understanding of what constitutes a prompt and their ability to formulate effective prompts for different LLM applications. Refer to the text to understand the context in which prompts are used."
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Why is Prompt Engineering Important?",
                    "description": "Explaining why effective prompting is crucial for obtaining useful and accurate outputs from LLMs.",
                    "tasks": [
                        {
                            "name": "The Importance of Well-Crafted Prompts",
                            "description": "Explain how a poorly written prompt can lead to ambiguous, inaccurate, or unhelpful responses. Emphasize the iterative nature of prompt engineering and its role in refining LLM outputs.",
                            "type": "learning_material",
                            "details": {
                                "content": "When thinking about a large language model input and output, a text prompt (sometimes accompanied by other modalities such as image prompts) is the input the model uses to predict a specific output. You don't need to be a data scientist or a machine learning engineer - everyone can write a prompt. However, crafting the most effective prompt can be complicated. Many aspects of your prompt affect its efficacy: the model you use, the model's training data, the model configurations, your word-choice, style and tone, structure, and context all matter. Therefore, prompt engineering is an iterative process. Inadequate prompts can lead to ambiguous, inaccurate responses, and can hinder the model's ability to provide meaningful output."
                            }
                        },
                        {
                            "name": "Identifying Bad Prompts",
                            "description": "Provide examples of ineffective prompts and ask learners to identify the issues (e.g., ambiguity, lack of context).",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "objective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Which of the following prompts is most likely to yield an ambiguous or unhelpful response from an LLM?",
                                        "correct_answer": "Tell me about New York.",
                                        "scorecard": null,
                                        "context": "A well-crafted prompt provides enough context and clear instructions to guide the LLM.  A poorly written prompt can lead to ambiguous, inaccurate, or unhelpful responses. Consider this while answering the questions."
                                    },
                                    {
                                        "question_type": "objective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Identify the primary issue with the following prompt:\n\n'Write something.'",
                                        "correct_answer": "It lacks specific instructions or context.",
                                        "scorecard": null,
                                        "context": "A well-crafted prompt provides enough context and clear instructions to guide the LLM.  A poorly written prompt can lead to ambiguous, inaccurate, or unhelpful responses. Consider this while answering the questions."
                                    },
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Consider the following two prompts for generating a product description:\n\nPrompt A: 'Describe this product.'\nPrompt B: 'Write a concise and engaging product description for a high-end noise-canceling headphone, highlighting its comfort, sound quality, and battery life.'\n\nExplain why Prompt B is likely to be more effective than Prompt A.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Prompt Effectiveness Assessment",
                                            "criteria": [
                                                {
                                                    "name": "Clarity",
                                                    "description": "The response clearly articulates the difference in effectiveness between the two prompts.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Specificity",
                                                    "description": "The response accurately identifies how Prompt B's specificity contributes to its superior effectiveness.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Relevance",
                                                    "description": "The response focuses on the prompt characteristics that directly impact the LLM's output quality.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "A well-crafted prompt provides enough context and clear instructions to guide the LLM.  A poorly written prompt can lead to ambiguous, inaccurate, or unhelpful responses. Consider this while answering the questions."
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Factors Affecting Prompt Efficacy",
                    "description": "Exploring the different components that influence the quality of a prompt and its results.",
                    "tasks": [
                        {
                            "name": "Key Factors",
                            "description": "Discuss factors such as the choice of LLM, its training data, model configurations (temperature, top-k, etc.), word choice, style, tone, structure, and context.",
                            "type": "learning_material",
                            "details": {
                                "content": "When thinking about a large language model input and output, a text prompt (sometimes accompanied by other modalities such as image prompts) is the input the model uses to predict a specific output. You don't need to be a data scientist or a machine learning engineer - everyone can write a prompt. However, crafting the most effective prompt can be complicated. Many aspects of your prompt affect its efficacy: the model you use, the model's training data, the model configurations, your word-choice, style and tone, structure, and context all matter. Therefore, prompt engineering is an iterative process. Inadequate prompts can lead to ambiguous, inaccurate responses, and can hinder the model's ability to provide meaningful output."
                            }
                        },
                        {
                            "name": "Prompt Refinement Challenge",
                            "description": "Present a scenario where the LLM output is not ideal, and ask learners to modify different aspects of the prompt (word choice, structure, tone) to improve the result.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "You are tasked with generating a short marketing tagline for a new brand of artisanal coffee. The initial prompt was:\n\n\"Write a tagline for our new coffee.\"\n\nThe LLM responded with:\n\n\"Good Coffee.\"\n\nThis tagline is not very effective. Describe three distinct modifications you could make to the original prompt, focusing on word choice, structure, and tone, to elicit a more compelling and creative tagline from the LLM. For each modification, explain why you believe it would improve the output.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Prompt Refinement",
                                            "criteria": [
                                                {
                                                    "name": "Clarity",
                                                    "description": "The response clearly identifies specific modifications to the original prompt.",
                                                    "min_score": 0,
                                                    "max_score": 3
                                                },
                                                {
                                                    "name": "Justification",
                                                    "description": "The response provides logical and well-reasoned justifications for why each modification would improve the LLM's output.",
                                                    "min_score": 0,
                                                    "max_score": 3
                                                },
                                                {
                                                    "name": "Creativity",
                                                    "description": "The proposed modifications demonstrate creativity in approaching the prompt engineering task.",
                                                    "min_score": 0,
                                                    "max_score": 2
                                                }
                                            ]
                                        },
                                        "context": "The goal of this question is to assess the learner's ability to analyze a suboptimal LLM output and strategically refine the prompt to achieve a better result. Learners should consider factors like specifying the desired tone (e.g., sophisticated, playful), adding context about the target audience or brand values, and restructuring the prompt to guide the LLM more effectively."
                                    }
                                ]
                            }
                        }
                    ]
                }
            ]
        },
        {
            "name": "Configuring LLM Output",
            "concepts": [
                {
                    "name": "Output Length",
                    "description": "Understanding how to control the length of the LLM's response.",
                    "tasks": [
                        {
                            "name": "Managing Token Generation",
                            "description": "Explain how the number of tokens to generate impacts computation, energy consumption, and response times.  Explain the meaning of Token in Layman terms",
                            "type": "learning_material",
                            "details": {
                                "content": "An important configuration setting is the number of tokens to generate in a response. Generating more tokens requires more computation from the LLM, leading to higher energy consumption, potentially slower response times, and higher costs.\n\nReducing the output length of the LLM doesn't cause the LLM to become more stylistically or textually succinct in the output it creates, it just causes the LLM to stop predicting more tokens once the limit is reached. If your needs require a short output length, you'll also possibly need to engineer your prompt to accommodate.\n\nOutput length restriction is especially important for some LLM prompting techniques, like ReAct, where the LLM will keep emitting useless tokens after the response you want.\n\nBe aware, generating more tokens requires more computation from the LLM, leading to higher energy consumption and potentially slower response times, which leads to higher costs.\n\nIn layman's terms, a token can be thought of as a basic unit of text that the LLM processes. It could be a word, part of a word, or even a punctuation mark. The number of tokens to generate is like setting a budget for how much the LLM can 'speak' or write in response to a prompt. A higher token limit allows for longer and more detailed responses, but it comes at the cost of increased processing, energy use, and time. Conversely, a lower token limit results in shorter responses and lower costs, but it might require careful prompt engineering to ensure the LLM conveys the necessary information within the limited space."
                            }
                        },
                        {
                            "name": "Adjusting Output Length",
                            "description": "Give examples where different output lengths might be desired (e.g., short summary vs. detailed explanation). Ask learners to modify prompts to produce outputs of varying lengths.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Imagine you are using an LLM to generate product descriptions for an e-commerce website. \n\n1.  Provide a prompt that will generate a short, concise description suitable for a product listing.\n2.  Provide another prompt that will generate a detailed and engaging description suitable for the product's dedicated page.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Prompt Effectiveness",
                                            "criteria": [
                                                {
                                                    "name": "Clarity",
                                                    "description": "The prompt is clearly written and easy to understand.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Relevance",
                                                    "description": "The prompt is directly relevant to the task of generating product descriptions.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Specificity",
                                                    "description": "The prompt specifies the desired output length and style for each description type.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Creativity",
                                                    "description": "The prompt encourages creative and engaging descriptions.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "The goal is to demonstrate the learner's understanding of how to modify prompts to achieve different output lengths and levels of detail. Reference the section on output length and provide examples of prompts used for different purposes (e.g., summaries vs. detailed explanations)."
                                    },
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "You're using an LLM to write social media posts for a marketing campaign.  Sometimes you need very short, attention-grabbing copy, and other times you need longer posts that explain the product in more detail.\n\n1.  Craft a prompt to generate a tweet (short and concise).\n2.  Craft a prompt to generate a Facebook post (longer and more detailed).",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Prompt Quality",
                                            "criteria": [
                                                {
                                                    "name": "Brevity",
                                                    "description": "The tweet prompt creates content concise enough to fit within the character limit of Twitter.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Engagement",
                                                    "description": "The tweet prompt creates content interesting enough to capture attention.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Informativeness",
                                                    "description": "The Facebook prompt returns detail-rich content suitable for use as an engaging advertisement.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Readability",
                                                    "description": "The Facebook prompt returns content which is readable and grammatically correct.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "The goal is to write effective prompts for different social media formats and demonstrate a mastery of generating varying text lengths."
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Sampling Controls: Temperature",
                    "description": "Learning about the temperature setting and how it affects the randomness of the output.",
                    "tasks": [
                        {
                            "name": "Temperature Explained",
                            "description": "Explain how temperature controls the degree of randomness in token selection. Explain that lower temperatures lead to deterministic responses, while higher temperatures introduce more diversity and unexpected results.",
                            "type": "learning_material",
                            "details": {
                                "content": "Temperature controls the degree of randomness in token selection. Lower temperatures are good for prompts that expect a more deterministic response, while higher temperatures can lead to more diverse or unexpected results. A temperature of 0 (greedy decoding) is deterministic: the highest probability token is always selected (though note that if two tokens have the same highest predicted probability, depending on how tiebreaking is implemented you may not always get the same output with temperature 0).\n\nTemperatures close to the max tend to create more random output. And as temperature gets higher and higher, all tokens become equally likely to be the next predicted token.\n\nThe Gemini temperature control can be understood in a similar way to the softmax function used in machine learning. A low temperature setting mirrors a low softmax temperature (T), emphasizing a single, preferred temperature with high certainty. A higher Gemini temperature setting is like a high softmax temperature, making a wider range of temperatures around the selected setting more acceptable. This increased uncertainty accommodates scenarios where a rigid, precise temperature may not be essential like for example when experimenting with creative outputs."
                            }
                        },
                        {
                            "name": "Creative Writing vs. Factual Recall",
                            "description": "Ask learners to experiment with different temperature settings for different tasks (e.g., creative story generation vs. answering a factual question).",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Imagine you are working on two different projects:\n\n1.  **Creative Writing:** You're using a Large Language Model (LLM) to generate a short story. You want the story to be imaginative and full of surprises.\n2.  **Factual Recall:** You're building a question-answering system that needs to provide accurate and consistent answers to factual questions.\n\nFor each project, which temperature setting (Low, Medium, High) would be most suitable? Explain why.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Temperature Setting Appropriateness",
                                            "criteria": [
                                                {
                                                    "name": "Creative Writing Setting",
                                                    "description": "Correctly identifies a high temperature setting as appropriate for creative writing, explaining that it encourages diversity and unexpected results.",
                                                    "min_score": 0,
                                                    "max_score": 2
                                                },
                                                {
                                                    "name": "Factual Recall Setting",
                                                    "description": "Correctly identifies a low temperature setting as appropriate for factual recall, explaining that it leads to deterministic and consistent responses.",
                                                    "min_score": 0,
                                                    "max_score": 2
                                                },
                                                {
                                                    "name": "Reasoning",
                                                    "description": "Provides clear and logical reasons for choosing the temperature settings based on the specific needs of each project.",
                                                    "min_score": 0,
                                                    "max_score": 3
                                                }
                                            ]
                                        },
                                        "context": "The temperature controls the degree of randomness in token selection. Lower temperatures are good for prompts that expect a more deterministic response, while higher temperatures can lead to more diverse or unexpected results. A temperature of 0 (greedy decoding) is deterministic: the highest probability token is always selected (though note that if two tokens have the same highest predicted probability, depending on how tiebreaking is implemented you may not always get the same output with temperature 0)."
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Sampling Controls: Top-K and Top-P",
                    "description": "Understanding Top-K and Top-P and how they can be used to restrict the output.",
                    "tasks": [
                        {
                            "name": "Top-K and Top-P Explained",
                            "description": "Explain how Top-K and Top-P restrict the predicted next token to come from tokens with the top predicted probabilities.  Explain how these settings control randomness and diversity.",
                            "type": "learning_material",
                            "details": {
                                "content": "Top-K and top-P (also known as nucleus sampling) are two sampling settings used in LLMs to restrict the predicted next token to come from tokens with the top predicted probabilities. Like temperature, these sampling settings control the randomness and diversity of generated text.\n\n\u2022 Top-K sampling selects the top K most likely tokens from the model's predicted distribution. The higher top-K, the more creative and varied the model's output; the lower top-K, the more restive and factual the model's output. A top-K of 1 is equivalent to greedy decoding.\n\n\u2022 Top-P sampling selects the top tokens whose cumulative probability does not exceed a certain value (P). Values for P range from 0 (greedy decoding) to 1 (all tokens in the LLM's vocabulary)."
                            }
                        },
                        {
                            "name": "Experimenting with Sampling",
                            "description": "Provide scenarios and ask learners to choose appropriate temperature, Top-K, and Top-P values to achieve specific output characteristics (e.g., coherent but slightly creative, very factual and precise).",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "objective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "You are developing a chatbot that should provide factual answers with minimal creativity. Which sampling parameters would be most appropriate?\n\n a) High temperature, high Top-K, high Top-P\n b) Low temperature, low Top-K, low Top-P\n c) Low temperature, low Top-K, high Top-P\n d) High temperature, low Top-K, low Top-P",
                                        "correct_answer": "b) Low temperature, low Top-K, low Top-P",
                                        "scorecard": null,
                                        "context": "Low temperature ensures deterministic output. Low Top-K restricts to the most likely tokens. Low Top-P narrows the token selection further, promoting factual accuracy."
                                    },
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Describe a scenario where using a high Top-K value (e.g., 40) might be preferred over a low Top-K value (e.g., 1) when generating text with a language model. Explain why the higher value would be more suitable in that situation.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Top-K Value Justification",
                                            "criteria": [
                                                {
                                                    "name": "Relevance",
                                                    "description": "Explanation clearly relates to the impact of Top-K on text generation.",
                                                    "min_score": 0,
                                                    "max_score": 3
                                                },
                                                {
                                                    "name": "Understanding",
                                                    "description": "Demonstrates an understanding of how Top-K affects creativity and determinism.",
                                                    "min_score": 0,
                                                    "max_score": 3
                                                },
                                                {
                                                    "name": "Clarity",
                                                    "description": "Explanation is easy to understand.",
                                                    "min_score": 0,
                                                    "max_score": 2
                                                }
                                            ]
                                        },
                                        "context": "A high Top-K is useful for creative tasks where a varied output is desired. A lower Top-K is more appropriate for factual outputs."
                                    },
                                    {
                                        "question_type": "objective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "You are using a language model to brainstorm creative marketing slogans. Which of the following parameter configurations would likely produce the MOST diverse and unexpected results?\n\n a) Temperature: 0.1, Top-K: 5, Top-P: 0.2\n b) Temperature: 0.5, Top-K: 20, Top-P: 0.5\n c) Temperature: 0.9, Top-K: 40, Top-P: 0.9\n d) Temperature: 0.0, Top-K: 1, Top-P: 0.1",
                                        "correct_answer": "c) Temperature: 0.9, Top-K: 40, Top-P: 0.9",
                                        "scorecard": null,
                                        "context": "Higher values for temperature, Top-K and Top-P introduce more randomness and allow for a wider range of tokens to be selected."
                                    },
                                    {
                                        "question_type": "objective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "What does a Top-K value of 1 achieve in language model output generation?",
                                        "correct_answer": "Equivalent to greedy decoding (deterministic).",
                                        "scorecard": null,
                                        "context": "Setting Top-K to 1 means the model will always choose the most likely token, leading to a very predictable, non-creative output."
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Putting it all Together",
                    "description": "Combining the different configuration options for optimal results.",
                    "tasks": [
                        {
                            "name": "Combining Settings",
                            "description": "Explain how the different sampling settings interact and influence each other.  Emphasize that the optimal settings depend on the specific application and desired outcome.",
                            "type": "learning_material",
                            "details": {
                                "content": "Choosing between top-K, top-P, temperature, and the number of tokens to generate, depends on the specific application and desired outcome, and the settings all impact one another. It's also important to make sure you understand how your chosen model combines the different sampling settings together.\n\nIf temperature, top-K, and top-P are all available (as in Vertex Studio), tokens that meet both the top-K and top-P criteria are candidates for the next predicted token, and then temperature is applied to sample from the tokens that passed the top-K and top-P criteria. If only top-K or top-P is available, the behavior is the same but only the one top-K or P setting is used.\n\nIf temperature is not available, whatever tokens meet the top-K and/or top-P criteria are then randomly selected from to produce a single next predicted token.\n\nAt extreme settings of one sampling configuration value, that one sampling setting either cancels out other configuration settings or becomes irrelevant.\n\n*   If you set temperature to 0, top-K and top-P become irrelevant\u2013the most probable token becomes the next token predicted. If you set temperature extremely high (above 1-generally into the 10s), temperature becomes irrelevant and whatever tokens make it through the top-K and/or top-P criteria are then randomly sampled to choose a next predicted token.\n*   If you set top-K to 1, temperature and top-P become irrelevant. Only one token passes the top-K criteria, and that token is the next predicted token. If you set top-K extremely high, like to the size of the LLM's vocabulary, any token with a nonzero probability of being the next token will meet the top-K criteria and none are selected out.\n*   If you set top-P to 0 (or a very small value), most LLM sampling implementations will then only consider the most probable token to meet the top-P criteria, making temperature and top-K irrelevant. If you set top-P to 1, any token with a nonzero probability of being the next token will meet the top-P criteria, and none are selected out.\n\nAs a general starting point, a temperature of .2, top-P of .95, and top-K of 30 will give you relatively coherent results that can be creative but not excessively so. If you want especially creative results, try starting with a temperature of .9, top-P of .99, and top-K of 40. And if you want less creative results, try starting with a temperature of .1, top-P of .9, and top-K of 20. Finally, if your task always has a single correct answer (e.g., answering a math problem), start with a temperature of 0.\n\nNOTE: With more freedom (higher temperature, top-K, top-P, and output tokens), the LLM might generate text that is less relevant.\n\nWARNING: Have you ever seen a response ending with a large amount of filler words? This is also known as the \"repetition loop bug\", which is a common issue in Large Language Models where the model gets stuck in a cycle, repeatedly generating the same (filler) word, phrase, or sentence structure, often exacerbated by inappropriate temperature and top-k/top-p settings. This can occur at both low and high temperature settings, though for different reasons. At low temperatures, the model becomes overly deterministic, sticking rigidly to the highest probability path, which can lead to a loop if that path revisits previously generated text. Conversely, at high temperatures, the model's output becomes excessively random, increasing the probability that a randomly chosen word or phrase will, by chance, lead back to a prior state, creating a loop due to the vast number of available options. In both cases, the model's sampling process gets \"stuck,\" resulting in monotonous and unhelpful output until the output window is filled. Solving this often requires careful tinkering with temperature and top-k/top-p values to find the optimal balance between determinism and randomness."
                            }
                        },
                        {
                            "name": "Troubleshooting Configuration",
                            "description": "Present scenarios where the LLM produces undesirable output (e.g., repetitive text, nonsensical answers) and ask learners to adjust the configuration settings to improve the results.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "objective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "An LLM is generating repetitive text, often getting stuck in a loop of the same phrases. Which configuration setting is most likely causing this, and how should you adjust it?\n\nA) Temperature: Lower it.\nB) Top-K: Increase it.\nC) Output Length: Decrease it.\nD) Top-P: Set it to 0.",
                                        "correct_answer": "A) Temperature: Lower it.",
                                        "scorecard": null,
                                        "context": "Lowering the temperature makes the output more deterministic, reducing randomness and the likelihood of repetitive loops. Option D is incorrect because setting top-P to 0 has the same effect as temperature to 0, which exacerbates the problem. Options B and C are incorrect because they don't directly address the core issue of the LLM getting stuck in a loop. See page 12 for explanation about repetition loop bug, and page 9 for explanation on temperature controls for LLMs. "
                                    },
                                    {
                                        "question_type": "objective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "An LLM is producing nonsensical and irrelevant responses. You want to encourage more coherent and focused output. Which combination of configuration adjustments would be most effective?\n\nA) High temperature, high top-K, high output length\nB) Low temperature, low top-K, low output length\nC) High temperature, low top-K, low output length\nD) Low temperature, high top-K, high output length",
                                        "correct_answer": "B) Low temperature, low top-K, low output length",
                                        "scorecard": null,
                                        "context": "This combination reduces randomness (low temperature), limits the number of possible tokens (low top-K), and prevents the model from wandering off-topic (low output length). Option A promotes creative but incoherent output. Options C and D are mixes of high and low settings that are less likely to produce focused and coherent results. See page 11 for explanation on combining configurations and pages 9-10 on temperature, top-K, and output length. "
                                    },
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "You are using an LLM for creative writing and want to generate highly varied and imaginative stories. However, the stories often lack coherence and stray from the initial topic. Describe how you would adjust the temperature, top-K, and top-P settings to balance creativity with coherence. Explain your reasoning for each setting.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Creativity vs. Coherence",
                                            "criteria": [
                                                {
                                                    "name": "Temperature",
                                                    "description": "Explanation of temperature setting and its impact on creativity (higher temperature) versus focus (lower temperature).",
                                                    "min_score": 0,
                                                    "max_score": 3
                                                },
                                                {
                                                    "name": "Top-K/Top-P",
                                                    "description": "Explanation of Top-K/Top-P settings and their impact on narrowing down or widening the range of potential tokens. Discuss balancing diversity and relevance.",
                                                    "min_score": 0,
                                                    "max_score": 3
                                                },
                                                {
                                                    "name": "Justification",
                                                    "description": "Clear reasoning for the chosen configuration based on the goal of creative but coherent output.",
                                                    "min_score": 0,
                                                    "max_score": 4
                                                }
                                            ]
                                        },
                                        "context": "The goal is to find a balance between creative outputs that are varied but are not too far deviated from the topic. Refer to pages 9-10 of the reference material to give feedback about the sampling configurations in LLMs and what the effects of such configurations can bring about."
                                    }
                                ]
                            }
                        }
                    ]
                }
            ]
        },
        {
            "name": "Basic Prompting Techniques",
            "concepts": [
                {
                    "name": "Zero-Shot Prompting",
                    "description": "Understanding how to prompt the LLM without providing any examples.",
                    "tasks": [
                        {
                            "name": "Zero-Shot Definition",
                            "description": "Explain that a zero-shot prompt only provides a description of the task and some text for the LLM to get started with, without any examples.",
                            "type": "learning_material",
                            "details": {
                                "content": "A zero-shot prompt is the simplest type of prompt. It only provides a description of a task and some text for the LLM to get started with. This input could be anything: a question, a start of a story, or instructions. The name zero-shot stands for 'no examples'."
                            }
                        },
                        {
                            "name": "Zero-Shot Scenarios",
                            "description": "Ask learners to create zero-shot prompts for different tasks (e.g., summarization, translation, question answering).",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Imagine you are tasked with summarizing a scientific research paper about a novel drug. Formulate a zero-shot prompt that instructs an LLM to summarize the paper, highlighting its key findings, methodology, and potential implications.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Zero-Shot Prompt Quality",
                                            "criteria": [
                                                {
                                                    "name": "Clarity",
                                                    "description": "The prompt is clearly and concisely written, leaving no room for ambiguity.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Specificity",
                                                    "description": "The prompt specifically instructs the LLM to focus on key findings, methodology, and implications.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Task Definition",
                                                    "description": "The prompt clearly defines the task of summarizing a scientific research paper.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Zero-Shot Appropriateness",
                                                    "description": "The prompt is designed to work without any examples, relying solely on the LLM's pre-existing knowledge.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "This question tests the ability to create an effective zero-shot prompt for a summarization task.  A good prompt should clearly define the task, specify the desired output (key findings, methodology, implications), and be concise. The prompt should not include any examples."
                                    },
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "You need to translate an English sentence into French using an LLM. Create a zero-shot prompt that achieves this, bearing in mind that no example translations can be given to the model.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Zero-Shot Prompt Quality",
                                            "criteria": [
                                                {
                                                    "name": "Clarity",
                                                    "description": "The prompt is clearly and concisely written, leaving no room for ambiguity.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Specificity",
                                                    "description": "The prompt clearly instructs the LLM to translate from English to French",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Task Definition",
                                                    "description": "The prompt clearly defines the task of translating an English sentence into French.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Zero-Shot Appropriateness",
                                                    "description": "The prompt is designed to work without any examples, relying solely on the LLM's pre-existing knowledge.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "This question assesses the ability to create a zero-shot prompt to translate a sentence from English to French. The prompt should be concise, clearly define the task of translating, and not include examples."
                                    },
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Imagine you want an LLM to generate a short poem about the concept of 'artificial intelligence' using a zero-shot prompt.  Formulate a suitable prompt.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Zero-Shot Prompt Quality",
                                            "criteria": [
                                                {
                                                    "name": "Clarity",
                                                    "description": "The prompt is clearly and concisely written, leaving no room for ambiguity.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Specificity",
                                                    "description": "The prompt clearly instructs the LLM to create a poem.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Task Definition",
                                                    "description": "The prompt clearly defines the task of writing a short poem about artificial intelligence.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Zero-Shot Appropriateness",
                                                    "description": "The prompt is designed to work without any examples, relying solely on the LLM's pre-existing knowledge.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "This question is designed to test the learner's understanding on how to use Zero-Shot prompting for creative use-cases. The students are expected to come up with descriptive and yet concise prompts."
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "One-Shot and Few-Shot Prompting",
                    "description": "Improving prompts by providing examples to the LLM.",
                    "tasks": [
                        {
                            "name": "One-Shot and Few-Shot Explained",
                            "description": "Explain the difference between one-shot (single example) and few-shot (multiple examples) prompting.  Explain that examples help the model understand the desired output structure or pattern.",
                            "type": "learning_material",
                            "details": {
                                "content": "When creating prompts for AI models, it is helpful to provide examples. These examples can help the model understand what you are asking for. Examples are especially useful when you want to steer the model to a certain output structure or pattern.\n\nA one-shot prompt, provides a single example, hence the name one-shot. The idea is the model has an example it can imitate to best complete the task.\n\nA few-shot prompt provides multiple examples to the model. This approach shows the model a pattern that it needs to follow. The idea is similar to one-shot, but multiple examples of the desired pattern increases the chance the model follows the pattern."
                            }
                        },
                        {
                            "name": "Providing Examples",
                            "description": "Provide a task and ask learners to create one-shot and few-shot prompts to guide the LLM.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Imagine you're tasked with creating a prompt for an LLM to generate different types of content for a photography blog. The goal is to get the LLM to generate engaging introductions for blog posts.  \n\nFirst, create a one-shot prompt. Provide an example introduction for a blog post about landscape photography. Next, create a few-shot prompt with two more example introductions about portrait and wildlife photography.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Prompt Quality Assessment",
                                            "criteria": [
                                                {
                                                    "name": "Relevance",
                                                    "description": "The examples provided are relevant to the task of generating blog post introductions and related to photography.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Clarity",
                                                    "description": "The prompts are clear, well-structured, and easy to understand for the LLM.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Format",
                                                    "description": "The prompts are correctly formatted as one-shot and few-shot examples.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Creativity",
                                                    "description": "The examples provided are creative and engaging, suitable for attracting readers to the blog post.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "This exercise tests the learner's ability to create effective prompts using one-shot and few-shot learning. The prompts should guide the LLM to generate specific, relevant, and engaging content."
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "System, Contextual and Role Prompting",
                    "description": "Guiding the LLM by setting context, defining a role or providing system-level instructions.",
                    "tasks": [
                        {
                            "name": "System, Context and Roles",
                            "description": "Explain the different types of prompting, defining System Prompt, Contextual Prompt and Role Prompt. Give examples for each.",
                            "type": "learning_material",
                            "details": {
                                "content": "System, contextual and role prompting are all techniques used to guide how LLMs generate text, but they focus on different aspects:\n\n*   **System prompting** sets the overall context and purpose for the language model. It defines the 'big picture' of what the model should be doing, like translating a language, classifying a review etc.\n\n*   **Contextual prompting** provides specific details or background information relevant to the current conversation or task. It helps the model to understand the nuances of what's being asked and tailor the response accordingly.\n\n*   **Role prompting** assigns a specific character or identity for the language model to adopt. This helps the model generate responses that are consistent with the assigned role and its associated knowledge and behavior.\n\nThere can be considerable overlap between system, contextual, and role prompting. E.g. a prompt that assigns a role to the system, can also have a context.\n\nHowever, each type of prompt serves a slightly different primary purpose:\n\n*   System prompt: Defines the model's fundamental capabilities and overarching purpose.\n\n*   Contextual prompt: Provides immediate, task-specific information to guide the response. It's highly specific to the current task or input, which is dynamic.\n\n*   Role prompt: Frames the model's output style and voice. It adds a layer of specificity and personality.\n\n**System Prompting Examples:**\n\n*   `"
                            }
                        },
                        {
                            "name": "Combine the Types",
                            "description": "Ask learners to rewrite some prompts using these techniques for improved results.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Rewrite the following prompt to incorporate both a system and contextual prompt:\n\n\"Write a short story.\"",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Prompt Quality Assessment",
                                            "criteria": [
                                                {
                                                    "name": "System Context",
                                                    "description": "Clearly defines the overall purpose or goal of the prompt. Does the system prompt provide useful instructions?",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Contextual Details",
                                                    "description": "Provides specific details or background information relevant to the current task or conversation. Does the contextual prompt significantly provide a better direction to the model?",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Creativity & Engagement",
                                                    "description": "Rewritten prompt inspires creativity and provides clear, engaging direction.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "The rewritten prompt should guide the LLM to create a story that is more focused and relevant by specifying system-level instructions as well as providing background information."
                                    },
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Improve the following prompt by adding a role prompt:\n\n\"Explain the theory of relativity.\"",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Role Prompt Enhancement",
                                            "criteria": [
                                                {
                                                    "name": "Role Definition",
                                                    "description": "The role assigned for the prompt should be appropriate to give a better output.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Relevance",
                                                    "description": "To what extent does the rewritten prompt provide a better output?",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Clarity",
                                                    "description": "The rewritten prompt should be clear and concise.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "The rewritten prompt should be specific to a role and it should provide clear guidance for what kind of response to generate."
                                    },
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Combine all three prompting types to improve this prompt:\n\n\"Summarize this document.\"",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Combined Prompt Effectiveness",
                                            "criteria": [
                                                {
                                                    "name": "System Goal",
                                                    "description": "Specifies the overall task and purpose.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Contextual Relevance",
                                                    "description": "Provides details relevant to the task, such as document source or audience.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Role Appropriateness",
                                                    "description": "The defined role is consistent with the task and enhances relevance.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "The rewritten prompt should include system-level instructions, a role to play and additional context to yield a more targeted and effective response."
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Step-Back Prompting",
                    "description": "Improving the performance by asking a general question before a specific task.",
                    "tasks": [
                        {
                            "name": "Step-Back prompting",
                            "description": "Explain what Step-Back prompting is and when it can be useful. Provide an example.",
                            "type": "learning_material",
                            "details": {
                                "content": "Step-back\u00ae prompting is a technique for improving the performance by prompting the LLM to first consider a general question related to the specific task at hand, and then feeding the answer to that general question into a subsequent prompt for the specific task. This \u2018step back' allows the LLM to activate relevant background knowledge and reasoning processes before attempting to solve the specific problem.\n\nBy considering the broader and underlying principles, LLMs can generate more accurate and insightful responses. Step-back prompting encourages LLMs to think critically and apply their knowledge in new and creative ways. It changes the final prompt doing the task by utilizing more knowledge in the LLM's parameters than would otherwise come into play when the LLM is prompted directly.\n\nIt can help to mitigate biases in LLM responses, by focusing on general principles instead of specific details, step-back prompting.\n\nHere is an example:\n\nLet\u2019s say we want to write a storyline for a level of a first-person shooter video game. We can use the following prompt for this:\n\nWrite a one paragraph storyline for a new level of a first-person shooter video game that is challenging and engaging.\n\nWhen you set the temperature to 1, you might end up with all kinds of creative writings for a storyline, but it's also quite random and generic. So let's take a step back:\n\nBased on popular first-person shooter action games, what are 5 fictional key settings that contribute to a challenging and engaging level storyline in a first-person shooter video game?\n\n1. **Abandoned Military Base**: A sprawling, post-apocalyptic military complex crawling with mutated soldiers and rogue robots, ideal for challenging firearm combat.\n2. **Cyberpunk City**: A neon-lit, futuristic urban environment with towering skyscrapers and dense alleyways, featuring cybernetically enhanced enemies and hacking mechanics.\n3. **Alien Spaceship**: A vast alien vessel stranded on Earth, with eerie corridors, zero-gravity sections, and extraterrestrial creatures to encounter.\n4. **Zombie-Infested Town**: A desolate town overrun by hordes of aggressive zombies, featuring intense close-quarters combat and puzzle-solving to find safe passage.\n5. **Underwater Research Facility**: A deep-sea laboratory flooded with water, filled with mutated aquatic creatures, and requiring stealth and underwater exploration skills to survive.\n\nYeah those topics seem like a good fit for a first-person video game. Let's go back to the original prompt, but this time we include the answer of the step back as context and see what it will return.\n\nContext: 5 engaging themes for a first person shooter video game:\n1. **Abandoned Military Base**: A sprawling, post-apocalyptic military complex crawling with mutated soldiers and rogue robots, ideal for challenging firearm combat.\n2. **Cyberpunk City**: A neon-lit, futuristic urban environment with towering skyscrapers and dense alleyways, featuring cybernetically enhanced enemies and hacking mechanics.\n3. **Alien Spaceship**: A vast alien vessel stranded on Earth, with eerie corridors, zero-gravity sections, and extraterrestrial creatures to encounter.\n4. **Zombie-Infested Town**: A desolate town overrun by hordes of aggressive zombies, featuring intense close-quarters combat and puzzle-solving to find safe passage.\n5. **Underwater Research Facility**: A deep-sea laboratory flooded with water, filled with mutated aquatic creatures, and requiring stealth and underwater exploration skills to survive.\n\nTake one of the themes and write a one paragraph storyline for a new level of a first-person shooter video game that is challenging and engaging.\n\nIn the heart of a murky abyss, lies a dilapidated underwater research facility, standing as a testament to human ambition and its disastrous consequences. Shrouded in darkness, pulsating with the hum of malfunctioning machinery, and haunted by the echoes of drowned souls, the facility is now a breeding ground for a nightmare borne from the deep sea. The player, an elite marine equipped with advanced diving gear and experimental weaponry, is tasked with venturing into this aquatic realm of terror. Armed with resilience and resourcefulness, they must navigate treacherous corridors, solve cryptic puzzles, and confront gruesome sea monstrosities that lurk in the depths. From grappling with the crushing pressure of the deep sea to outsmarting cunning aquatic predators, every moment in this uncharted underworld tests the limits of human endurance and courage."
                            }
                        },
                        {
                            "name": "Step-Back Prompting challenge",
                            "description": "Ask learners to rewrite a prompt using this technique to improve the quality of the answer.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Rewrite the following prompt using the step-back prompting technique to improve the quality of the answer:\n\n'Write a product description for a new noise-cancelling headphone.'",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Prompt Quality Assessment",
                                            "criteria": [
                                                {
                                                    "name": "General Question",
                                                    "description": "The prompt includes a general question related to the product before asking for a specific description.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Context Inclusion",
                                                    "description": "The rewritten prompt includes the answer to the general question as context for the specific description task.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Improvement",
                                                    "description": "The rewritten prompt is likely to produce a more detailed and relevant product description compared to the original prompt.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "The goal is to apply step-back prompting to generate a better product description. Consider asking a broad question first like 'What are the key features and benefits consumers seek in noise-cancelling headphones?' and use the answer to guide the product description."
                                    }
                                ]
                            }
                        }
                    ]
                }
            ]
        },
        {
            "name": "Advanced Prompting Techniques",
            "concepts": [
                {
                    "name": "Chain of Thought (CoT) Prompting",
                    "description": "Improve the reasoning capabilities of LLMs by generating intermediate reasoning steps.",
                    "tasks": [
                        {
                            "name": "CoT Prompting explained",
                            "description": "Explain that CoT prompting encourages the LLM to explain its reasoning process step by step, leading to more accurate answers. Explain why is this useful.",
                            "type": "learning_material",
                            "details": {
                                "content": "Chain of Thought (CoT) prompting is a technique designed to enhance the reasoning capabilities of Large Language Models (LLMs) by encouraging them to generate intermediate reasoning steps. Instead of directly providing an answer, the LLM is prompted to explain its reasoning process step by step, which leads to more accurate answers. This approach is particularly useful for complex tasks that require a degree of reasoning before responding, making it a beneficial strategy even with zero-shot prompting.\n\nCoT prompting offers several advantages. First, it is a low-effort yet highly effective method that works well with off-the-shelf LLMs, eliminating the need for fine-tuning. Second, CoT prompting enhances interpretability, allowing you to understand the LLM's thought process and identify any malfunctions in its reasoning. Furthermore, CoT appears to improve robustness across different LLM versions, reducing performance drift compared to prompts that do not use reasoning chains.\n\nBy making the reasoning process explicit, CoT helps LLMs to think through problems more thoroughly, reducing the likelihood of superficial or incorrect answers. The step-by-step reasoning not only aids in arriving at the correct solution but also offers insights into how the model processes information, making it easier to diagnose and correct any logical flaws.\n\nHowever, it's worth noting that including chain of thought reasoning in the LLM response leads to a greater amount of output tokens, which in turn makes the prediction more costly and time intensive."
                            }
                        },
                        {
                            "name": "CoT Examples",
                            "description": "Provide examples of CoT prompts and ask learners to create their own for complex tasks that require reasoning.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Develop a Chain of Thought (CoT) prompt for solving the following word problem:\n\nProblem: A farmer has 15 sheep. All but 8 die. How many are left?",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "CoT Prompt Quality Assessment",
                                            "criteria": [
                                                {
                                                    "name": "Clarity",
                                                    "description": "The prompt is clearly and concisely worded, making it easy for the LLM to understand the task.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "CoT Structure",
                                                    "description": "The prompt effectively guides the LLM to generate intermediate reasoning steps before providing the final answer.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Relevance",
                                                    "description": "The reasoning steps are relevant to solving the problem and lead to the correct answer.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Accuracy",
                                                    "description": "The final answer is accurate and consistent with the reasoning steps.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "This question evaluates the learner's understanding of how to craft a CoT prompt.  The ideal response should include clear, step-by-step instructions for the LLM to arrive at the correct solution.  The question tests the learner's ability to apply CoT prompting to a simple word problem but focuses on the prompt engineering aspect rather than the mathematical solution itself.  Learners should think about how to instruct the model to 'think' its way through the problem."
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Self-Consistency",
                    "description": "Generating diverse reasoning paths and selecting the most consistent answer.",
                    "tasks": [
                        {
                            "name": "Self-Consistency defined",
                            "description": "Explain Self-Consistency, how it combines sampling and majority voting to generate diverse reasoning paths and select the most consistent answer, and how it can be applied. Explain that this approach give more accurate and coherent results.",
                            "type": "learning_material",
                            "details": {
                                "content": "While large language models have shown impressive success in various NLP tasks, their ability to reason is often seen as a limitation that cannot be overcome solely by increasing model size. As we learned in the previous Chain of Thought prompting section, the model can be prompted to generate reasoning steps like a human solving a problem. However CoT uses a simple 'greedy decoding' strategy, limiting its effectiveness. Self-consistency combines sampling and majority voting to generate diverse reasoning paths and select the most consistent answer. It improves the accuracy and coherence of responses generated by LLMs.\n\nSelf-consistency gives a pseudo-probability likelihood of an answer being correct, but obviously has high costs."
                            }
                        },
                        {
                            "name": "Self-Consistency Implementations",
                            "description": "Ask learners to describe some scenarios where this technique can be useful. Ask them to extend the prompt to implement self-consistency. Explain the potential cost/benefit tradeoffs.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Describe a scenario where self-consistency prompting could be particularly useful. Explain how you would modify a prompt to implement the self-consistency technique in that scenario, considering the potential cost/benefit tradeoffs.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Self-Consistency Implementation and Tradeoffs",
                                            "criteria": [
                                                {
                                                    "name": "Relevance",
                                                    "description": "The scenario provided should be relevant to the concept of prompt engineering and Large Language Models. It should demonstrate an understanding of tasks that LLMs are commonly used for.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Implementation",
                                                    "description": "The prompt modification should demonstrate an understanding of how to implement self-consistency, including generating multiple reasoning paths and selecting the most consistent answer.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Tradeoffs",
                                                    "description": "The explanation of potential cost/benefit tradeoffs should consider factors such as increased computational cost and potential improvements in accuracy and coherence.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Clarity",
                                                    "description": "The explanation should be clear, concise, and easy to understand.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "This question assesses the learner's ability to apply the concept of self-consistency to a practical scenario and to consider the tradeoffs involved in using this technique. The reference material provides the context for understanding self-consistency and its potential benefits and costs (increased computational cost)."
                                    },
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Consider an email classification system. The goal is to classify incoming emails as either IMPORTANT or NOT IMPORTANT. Create a prompt and explain how to extend the prompt to implement self-consistency. What are the potential cost and benefits of implementing self consistency for this classification task?",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Email Classification with Self-Consistency",
                                            "criteria": [
                                                {
                                                    "name": "Prompt Quality",
                                                    "description": "The initial prompt should be clear and well-defined, providing sufficient context for the LLM to classify emails effectively.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Self-Consistency Implementation",
                                                    "description": "The explanation of how to extend the prompt to implement self-consistency should demonstrate a clear understanding of the technique.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Cost/Benefit Analysis",
                                                    "description": "The cost/benefit analysis should consider factors such as increased computational cost, potential improvements in accuracy, and the specific requirements of the email classification task.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Practicality",
                                                    "description": "The proposed implementation should be practical and feasible, considering the real-world constraints of an email classification system.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "The purpose of this question is to assess the learners understanding of the usefulness and practical implementation of self-consistency. The reference material on self-consistency needs to be used to provide an adequate answer to this question."
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Tree of Thoughts (ToT)",
                    "description": "Exploring multiple different reasoning paths simultaneously.",
                    "tasks": [
                        {
                            "name": "ToT Prompting",
                            "description": "Explain that ToT prompting generalizes CoT prompting by allowing LLMs to explore multiple reasoning paths simultaneously, rather than just following a single linear chain of thought.",
                            "type": "learning_material",
                            "details": {
                                "content": "Now that we are familiar with chain of thought and self-consistency prompting, let's review Tree of Thoughts (ToT). It generalizes the concept of CoT prompting because it allows LLMs to explore multiple different reasoning paths simultaneously, rather than just following a single linear chain of thought. This is depicted in Figure 1."
                            }
                        },
                        {
                            "name": "Reasoning comparison",
                            "description": "Ask to compare ToT prompting and CoT prompting and explain where ToT prompting can provide additional value compared to CoT prompting.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Compare and contrast Tree of Thoughts (ToT) prompting with Chain of Thought (CoT) prompting. In what scenarios does ToT prompting offer a significant advantage over CoT prompting, and why?",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "ToT vs CoT Comparison",
                                            "criteria": [
                                                {
                                                    "name": "Understanding",
                                                    "description": "Demonstrates a clear understanding of both ToT and CoT prompting techniques.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Comparison",
                                                    "description": "Accurately compares the methodologies of ToT and CoT.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Value",
                                                    "description": "Clearly articulates the added value of ToT over CoT in specific scenarios.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Examples",
                                                    "description": "Provides relevant examples where ToT excels compared to CoT.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "ToT generalizes CoT prompting because it allows LLMs to explore multiple different reasoning paths simultaneously, rather than just following a single linear chain of thought.ToT is well-suited for complex tasks that require exploration."
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "ReAct (Reason & Act)",
                    "description": "Reasoning combined with external tools.",
                    "tasks": [
                        {
                            "name": "ReAct Prompting",
                            "description": "Explain ReAct prompting. Detail that ReAct enables LLMs to solve complex tasks using natural language reasoning combined with external tools (search, code interpreter etc.).",
                            "type": "learning_material",
                            "details": {
                                "content": "Reason and act (ReAct) [10]\u00b9\u00b3 prompting is a paradigm for enabling LLMs to solve complex tasks using natural language reasoning combined with external tools (search, code interpreter etc.) allowing the LLM to perform certain actions, such as interacting with external APIs to retrieve information which is a first step towards agent modeling.\n\nReAct mimics how humans operate in the real world, as we reason verbally and can take actions to gain information. ReAct performs well against other prompt engineering approaches in a variety of domains.\n\nReAct prompting works by combining reasoning and acting into a thought-action loop. The LLM first reasons about the problem and generates a plan of action. It then performs the actions in the plan and observes the results. The LLM then uses the observations to update its reasoning and generate a new plan of action. This process continues until the LLM reaches a solution to the problem."
                            }
                        },
                        {
                            "name": "Advantages of ReAct Prompting",
                            "description": "Detail the advantages of using this technique and ask where it can be applied.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Explain the advantages of using ReAct prompting and provide examples of where it can be effectively applied. Consider various domains and tasks when illustrating its versatility.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "ReAct Prompting Assessment",
                                            "criteria": [
                                                {
                                                    "name": "Comprehension",
                                                    "description": "Demonstrates a clear and accurate understanding of the advantages of ReAct prompting.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Examples",
                                                    "description": "Provides relevant and diverse examples of scenarios where ReAct prompting can be applied, showcasing its versatility.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Clarity",
                                                    "description": "Presents the information in a clear, concise, and well-organized manner.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Completeness",
                                                    "description": "Addresses all aspects of the prompt, including both the advantages and application scenarios of ReAct prompting.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "ReAct prompting works by combining reasoning and acting into a thought-action loop. The LLM first reasons about the problem and generates a plan of action. It then performs the actions in the plan and observes the results. The LLM then uses the observations to update its reasoning and generate a new plan of action. This process continues until the LLM reaches a solution to the problem. It enables LLMs to solve complex tasks using natural language reasoning combined with external tools (search, code interpreter etc.) allowing the LLM to perform certain actions, such as interacting with external APIs to retrieve information which is a first step towards agent modeling."
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Automatic Prompt Engineering (APE)",
                    "description": "Use a model to generate prompts, evaluate them and repeat the process.",
                    "tasks": [
                        {
                            "name": "APE Prompting",
                            "description": "Explain APE prompting. Detail that this alleviates the need for human input and enhances the model's performance in various tasks.",
                            "type": "learning_material",
                            "details": {
                                "content": "At this point you might realize that writing a prompt can be complex. Wouldn't it be nice to automate this (write a prompt to write prompts)? Well, there's a method: Automatic Prompt Engineering (APE). This method not only alleviates the need for human input but also enhances the model's performance in various tasks."
                            }
                        },
                        {
                            "name": "APE Prompting usage",
                            "description": "Ask in what scenarios is APE prompting useful. Ask if the process should be completely automated or if there's value for human intervention in the process.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "objective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "In what scenarios is Automatic Prompt Engineering (APE) useful?",
                                        "correct_answer": "APE is particularly useful in scenarios where you want to train a chatbot for a specific task, and you need to figure out all the various ways customers could phrase their requests.",
                                        "scorecard": null,
                                        "context": "APE is useful for tasks like training a chatbot to handle customer orders. It helps in identifying various ways customers might phrase their requests, ensuring the chatbot is well-prepared for real-world interactions and can provide appropriate responses."
                                    }
                                ]
                            }
                        }
                    ]
                }
            ]
        },
        {
            "name": "Code Prompting",
            "concepts": [
                {
                    "name": "Prompts for Writing Code",
                    "description": "Using prompts to generate code snippets in different programming languages.",
                    "tasks": [
                        {
                            "name": "Code Generation",
                            "description": "Explain how to use prompts to generate code in a specific programming language, asking for a particular functionality or task. Explain how this process works.",
                            "type": "learning_material",
                            "details": {
                                "content": "Gemini can also be a developer and help you with writing code in any programming language of choice. As a developer this can help you speed up the process of writing code.\n\nImagine a folder on your machine with hundreds of files that needs to be renamed. Renaming each file would take you a lot of time. You know a little Bash, and could write a script to automate this, but that may also take awhile. So let's write a prompt. You can write the prompt in the public consumer chatbot Gemini, or if you are more concerned about confidentiality, you can write these prompts within your Google Cloud account and open Vertex Al Studio. The advantage of Vertex Al Studio is that you can configure the temperature etc."
                            }
                        },
                        {
                            "name": "Code Prompting Examples",
                            "description": "Ask learners to come up with some common scenarios where prompting to produce code could be useful, and to write the appropriate prompts to do that.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Describe a scenario where prompting to produce code could be useful. Include the programming language, the desired functionality, and the prompt you would use.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Prompt Effectiveness",
                                            "criteria": [
                                                {
                                                    "name": "Clarity",
                                                    "description": "The scenario and desired functionality are clearly described.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Relevance",
                                                    "description": "The prompt is directly related to the desired functionality.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Specificity",
                                                    "description": "The prompt is specific enough to guide the LLM towards a useful code snippet.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "This question tests the learner's ability to identify practical uses for code prompting and to formulate effective prompts."
                                    },
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Imagine you need a function to sort a list of dictionaries based on a specific key. What prompt would you use to generate this function in Python?",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Python Code Prompt",
                                            "criteria": [
                                                {
                                                    "name": "Completeness",
                                                    "description": "The prompt includes all necessary information for the LLM to generate the code.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Efficiency",
                                                    "description": "The prompt encourages the LLM to generate efficient code.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Readability",
                                                    "description": "The prompt encourages the LLM to generate readable code.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "This question focuses on generating a specific type of code in Python using prompts."
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Prompts for Explaining Code",
                    "description": "Using prompts to understand and explain existing code.",
                    "tasks": [
                        {
                            "name": "Code Explanation",
                            "description": "Explain how prompts can be used to ask the LLM to explain what a piece of code does, providing a human-readable description of its functionality.",
                            "type": "learning_material",
                            "details": {
                                "content": "As a developer when you work in teams you have to read someone else's code. Gemini can help you with this as well. Let's take the code output from Table 16, remove the comments and ask the large language model to explain what's happening, See Table 17:"
                            }
                        },
                        {
                            "name": "Explain Code challenge",
                            "description": "Provide learners with some code snippets and ask them to write prompts that would generate a clear explanation of the code's purpose and functionality.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "You are working in a team and encounter the following Bash script. Write a prompt that you could use with a large language model to get a clear and concise explanation of what the script does:\n\n```bash\n#!/bin/bash\n\necho \"Enter the folder name: \"\nread folder_name\n\nif [ ! -d \"$folder_name\" ]; then\necho \"Folder does not exist.\"\nexit 1\nfi\n\nfiles=( \"$folder_name\"/* )\n\nfor file in \"${files[@]}\"; do\nnew_file_name=\"draft_$(basename \"$file\")\"\nmv \"$file\" \"$new_file_name\"\ndone\n\necho \"Files renamed successfully.\"\n```",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Prompt Quality",
                                            "criteria": [
                                                {
                                                    "name": "Clarity",
                                                    "description": "The prompt is easy to understand and avoids ambiguity.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Conciseness",
                                                    "description": "The prompt is brief and to the point, avoiding unnecessary words.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Completeness",
                                                    "description": "The prompt includes all necessary information for the LLM to generate a useful explanation.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Specificity",
                                                    "description": "The prompt is specific about the type of explanation desired (e.g., purpose, functionality).",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "This question assesses the learner's ability to craft a prompt that effectively instructs a language model to explain a given code snippet. The ideal prompt should guide the LLM to provide a human-readable description of the script's functionality, input requirements, and potential use cases. This builds on prompt engineering best practices, where it is important that the prompts should be clear, concise, and specific with the instructions given to the language model to achieve the best and most appropriate response."
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Prompts for Translating Code",
                    "description": "Using prompts to translate code from one language to another.",
                    "tasks": [
                        {
                            "name": "Translate Code",
                            "description": "Explain how prompts can be used to ask the LLM to translate code from one programming language to another, facilitating code migration or cross-platform development.",
                            "type": "learning_material",
                            "details": {
                                "content": "The bash code from Table 16 seems to work fine. However this script could really be reusable when it would prompt me for file names, ideally it should work as a separate application with an Ul. As a starting point, Python would be a better language for a (web) application than Bash. LLMs can help with translating code from one language to another. See the example in Table 18:"
                            }
                        },
                        {
                            "name": "Code Translation Examples",
                            "description": "Provide code snippets in one language and ask learners to write prompts that would translate them to another language.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "coding",
                                        "answer_type": null,
                                        "coding_languages": [
                                            "Python"
                                        ],
                                        "content": "Write a prompt that would translate the following Python code to Javascript:\n\n```python\ndef factorial(n):\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\nprint(factorial(5))\n```",
                                        "correct_answer": null,
                                        "scorecard": null,
                                        "context": "This question tests the student's ability to formulate a prompt that instructs the LLM to translate Python code to Javascript. The prompt should be clear and specific, guiding the LLM towards generating accurate Javascript code. The Python code calculates the factorial of a number."
                                    },
                                    {
                                        "question_type": "coding",
                                        "answer_type": null,
                                        "coding_languages": [
                                            "SQL"
                                        ],
                                        "content": "Write a prompt that would translate the following SQL query to MongoDB aggregation pipeline:\n\n```sql\nSELECT city, COUNT(*) AS customer_count\nFROM Customers\nGROUP BY city\nHAVING COUNT(*) > 10;\n```",
                                        "correct_answer": null,
                                        "scorecard": null,
                                        "context": "This question assesses the student's understanding of prompting for database query translation. The SQL query groups customers by city and counts the number of customers in each city, filtering for cities with more than 10 customers. The student needs to craft a prompt that guides the LLM to generate an equivalent MongoDB aggregation pipeline."
                                    },
                                    {
                                        "question_type": "coding",
                                        "answer_type": null,
                                        "coding_languages": [
                                            "HTML",
                                            "CSS"
                                        ],
                                        "content": "Write a prompt that would translate the following HTML and CSS code to React:\n\n```html\n<div class=\"container\">\n  <h1>Hello, World!</h1>\n  <p>This is a simple webpage.</p>\n</div>\n```\n\n```css\n.container {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n}\n```",
                                        "correct_answer": null,
                                        "scorecard": null,
                                        "context": "This question tests the student's ability to translate front-end code using prompts. The HTML and CSS define a simple webpage with a heading and a paragraph, styled using flexbox. The student needs to create a prompt that guides the LLM to generate the equivalent React component."
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Prompts for Debugging and Reviewing Code",
                    "description": "Using prompts to identify and fix errors in code.",
                    "tasks": [
                        {
                            "name": "Debug Code",
                            "description": "Explain how prompts can be used to ask the LLM to identify errors in code and provide suggestions for improvement.",
                            "type": "learning_material",
                            "details": {
                                "content": "As a developer when you work in teams you have to read someone else's code. Gemini can help you with this as well. Let's take the code output from Table 16, remove the comments and ask the large language model to explain what's happening, See Table 17:"
                            }
                        },
                        {
                            "name": "Code Debugging challenge",
                            "description": "Provide code snippets with errors and ask learners to write prompts that would help identify and fix the bugs.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "coding",
                                        "answer_type": null,
                                        "coding_languages": [
                                            "Python"
                                        ],
                                        "content": "Below is a Python code snippet intended to calculate the area of a rectangle. However, it contains a bug that causes it to produce incorrect results. Your task is to debug the code using prompt engineering. Write a prompt that you would give an LLM that identifies the error in the code and provides a corrected version.\n\n```python\ndef calculate_rectangle_area(width, height):\n    area = width + height  # Incorrect calculation\n    return area\n\n# Example usage:\nwidth = 5\nheight = 10\narea = calculate_rectangle_area(width, height)\nprint(f\"The area of the rectangle is: {area}\")\n```",
                                        "correct_answer": null,
                                        "scorecard": null,
                                        "context": "The Python code should correctly calculate the area of a rectangle, which is width * height. The current code incorrectly adds the width and height. The student's prompt should guide the LLM to identify this error and provide the correct formula for calculating the area of a rectangle. The purpose of this question is to test the learner's ability to construct prompts that can effectively debug code by identifying errors and suggesting corrections. They should be able to use prompt engineering techniques to guide an LLM to the correct solution without explicitly providing the answer. A good prompt should focus the LLM on the mathematical operation being performed and compare it to the correct formula for area calculation. Provide feedback to the students focusing on these points to improve their understanding."
                                    }
                                ]
                            }
                        }
                    ]
                }
            ]
        },
        {
            "name": "Best Practices in Prompt Engineering",
            "concepts": [
                {
                    "name": "Provide Examples",
                    "description": "Using examples (one-shot or few-shot) within a prompt.",
                    "tasks": [
                        {
                            "name": "How to use Examples",
                            "description": "Explain that providing examples is a good teaching tool for LLMs.",
                            "type": "learning_material",
                            "details": {
                                "content": "The most important best practice is to provide (one shot / few shot) examples within a prompt. This is highly effective because it acts as a powerful teaching tool. These examples showcase desired outputs or similar responses, allowing the model to learn from them and tailor its own generation accordingly. It's like giving the model a reference point or target to aim for, improving the accuracy, style, and tone of its response to better match your expectations."
                            }
                        },
                        {
                            "name": "Examples in Prompts",
                            "description": "Ask learners to provide prompts using examples to improve the quality of the generated output.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Imagine you are tasked with creating a prompt for a language model to generate a short story. Demonstrate how you would use examples within the prompt to guide the model towards a specific style and theme. Provide two versions of the prompt: one without examples and one with examples. Explain how the examples influence the output.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Prompt Effectiveness",
                                            "criteria": [
                                                {
                                                    "name": "Clarity",
                                                    "description": "The prompt is clearly written and easy to understand.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Relevance",
                                                    "description": "The examples provided are relevant to the desired style and theme of the story.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Influence",
                                                    "description": "The explanation clearly articulates how the examples influence the output of the language model.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Creativity",
                                                    "description": "The prompt demonstrates a creative approach to guiding the language model.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "This question assesses the learner's ability to apply the concept of providing examples within prompts to effectively guide language models. The learner should demonstrate an understanding of how examples act as a powerful teaching tool and showcase desired outputs, allowing the model to learn and tailor its generation accordingly."
                                    },
                                    {
                                        "question_type": "objective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Which of the following statements best describes the role of examples in prompt engineering?",
                                        "correct_answer": "Examples act as a reference point for the model, improving accuracy, style, and tone.",
                                        "scorecard": null,
                                        "context": "This question tests the learner's understanding of the fundamental principle behind providing examples in prompts. The correct answer should highlight the idea that examples act as a reference point or target for the model to aim for, ultimately improving the quality of the generated output."
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Design with Simplicity",
                    "description": "Keeping prompts concise, clear and easy to understand.",
                    "tasks": [
                        {
                            "name": "Simplicity",
                            "description": "Explain why keeping prompts simple is effective for LLMs",
                            "type": "learning_material",
                            "details": {
                                "content": "Prompts should be concise, clear, and easy to understand for both you and the model. As a rule of thumb, if it's already confusing for you it will likely be also confusing for the model. Try not to use complex language and don't provide unnecessary information."
                            }
                        },
                        {
                            "name": "Simpler Prompts",
                            "description": "Ask learners to simplify some more complex prompts.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Rewrite the following prompt to be more concise, clear and easy to understand:\n\n'I am visiting New York right now, and I'd like to hear more about great locations. I am with two 3 year old kids. Where should we go during our vacation?'",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Prompt Simplification",
                                            "criteria": [
                                                {
                                                    "name": "Clarity",
                                                    "description": "The prompt is easy to understand and free of ambiguity.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Brevity",
                                                    "description": "The prompt is as short as possible while still conveying the necessary information.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Relevance",
                                                    "description": "The simplified prompt is focused on extracting information about good travel locations for tourists with two 3 year old kids.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "The goal is to rewrite the prompt following the principles of simplicity. Some possible rewrites can be 'Act as a travel guide for tourists. Describe great places to visit in New York Manhattan with a 3 year old.'"
                                    },
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "The following prompt is for a language model that generates descriptions for products in an e-commerce catalog. Simplify the prompt, making sure the output is well structured:\n\n'I want you to write a product description for a wireless headphone. You need to include the product name, its category, price, a list of its features and release date.'",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Prompt Simplification",
                                            "criteria": [
                                                {
                                                    "name": "Clarity",
                                                    "description": "The prompt is easy to understand and free of ambiguity.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Brevity",
                                                    "description": "The prompt is as short as possible while still conveying the necessary information.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Relevance",
                                                    "description": "The simplified prompt is focused on extracting information about product description for a wireless headphone.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "The goal is to rewrite the prompt following the principles of simplicity. The simplified prompt should be similar to: Use the following schema to generate a product description for wireless headphones. Add the JSON schema."
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Be Specific About the Output",
                    "description": "Provide specific details in the prompt.",
                    "tasks": [
                        {
                            "name": "Output Specificity",
                            "description": "Explain how providing specific details in the prompt will help the model to focus on what's relevant, improving the overall accuracy.",
                            "type": "learning_material",
                            "details": {
                                "content": "Be specific about the desired output. A concise instruction might not guide the LLM enough or could be too generic. Providing specific details in the prompt (through system or context prompting) can help the model to focus on what's relevant, improving the overall accuracy."
                            }
                        },
                        {
                            "name": "Improved Output",
                            "description": "Ask learners to improve output quality by providing a prompt that has specific instructions about the output format.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "You want to generate a blog post about the history of prompt engineering. Improve the following prompt so that the output blog post has a more specific structure and style:\n\n'Write a blog post about the history of prompt engineering.'",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Prompt Quality Assessment",
                                            "criteria": [
                                                {
                                                    "name": "Specificity",
                                                    "description": "The prompt includes specific details about the desired length (e.g., number of paragraphs), style (e.g., informative, engaging, conversational), and content (e.g., key aspects to cover).",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Clarity",
                                                    "description": "The prompt uses clear and concise language that is easy to understand for both the user and the LLM.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Actionable Verbs",
                                                    "description": "The prompt uses strong verbs that describe the desired action (e.g., 'Generate,' 'Describe,' 'Summarize').",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Positive Instructions",
                                                    "description": "The prompt focuses on positive instructions, telling the model what to do rather than what not to do.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "The goal is to guide the LLM to produce a more focused and high-quality blog post by providing specific instructions. Review the sections on 'Be specific about the output' and 'Use Instructions over Constraints' for guidance."
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Use Instructions over Constraints",
                    "description": "Focus on positive instructions instead of telling the model what not to do.",
                    "tasks": [
                        {
                            "name": "Instructions vs. Constraints",
                            "description": "Explain why providing positive instructions, instead of constraints, leads to better results.",
                            "type": "learning_material",
                            "details": {
                                "content": "Growing research suggests that focusing on positive instructions in prompting can be more effective than relying heavily on constraints. This approach aligns with how humans prefer positive instructions over lists of what not to do. Instructions directly communicate the desired outcome, whereas constraints might leave the model guessing about what is allowed. It gives flexibility and encourages creativity within the defined boundaries, while constraints can limit the model's potential. Also a list of constraints can clash with each other. Constraints are still valuable but in certain situations. To prevent the model from generating harmful or biased content or when a strict output format or style is needed. If possible, use positive instructions: instead of telling the model what not to do, tell it what to do instead. This can avoid confusion and improve the accuracy of the output.  DO:\nGenerate a 1 paragraph blog post about the top 5 video game consoles. Only discuss the console, the company who made it, the year, and total sales.\nDO NOT:\nGenerate a 1 paragraph blog post about the top 5 video game consoles. Do not list video game names. As a best practice, start by prioritizing instructions, clearly stating what you want the model to do and only use constraints when necessary for safety, clarity or specific requirements. Experiment and iterate to test different combinations of instructions and constraints to find what works best for your specific tasks, and document these."
                            }
                        },
                        {
                            "name": "Use Instructions",
                            "description": "Ask learners to rewrite prompts that have constraints by providing positive instructions.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Rewrite the following prompt to use positive instructions instead of constraints:\n\n'Generate a description of a futuristic city. Do not mention flying cars, robots, or aliens.'",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Prompt Rewriting",
                                            "criteria": [
                                                {
                                                    "name": "Clarity",
                                                    "description": "The rewritten prompt is easily understood.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Positivity",
                                                    "description": "The rewritten prompt focuses on positive instructions rather than constraints.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Completeness",
                                                    "description": "The rewritten prompt provides enough information for the model to generate a relevant response.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "The goal is to rewrite the prompt focusing on what the model *should* do, rather than what it *shouldn't* do. For example, instead of saying 'Do not mention flying cars', you could specify details about the city's transportation system, architecture, and inhabitants. If constraints are really required, they should be added after clear instructions about what is expected."
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Control the Max Token Length",
                    "description": "Setting a max token limit.",
                    "tasks": [
                        {
                            "name": "Max Token Length",
                            "description": "Explain that generated LLM responses can be limited by using the max token length parameter.",
                            "type": "learning_material",
                            "details": {
                                "content": "To control the length of a generated LLM response, you can either set a max token limit in the configuration or explicitly request a specific length in your prompt. For example:\n\n\"Explain quantum physics in a tweet length message.\""
                            }
                        },
                        {
                            "name": "Token Length Instructions",
                            "description": "Ask learners to rewrite the prompt to achieve a specific length of generated text.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Rewrite the following prompt so that the generated response is no more than 50 words:\n\n\"Explain quantum physics.\"",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Prompt Quality",
                                            "criteria": [
                                                {
                                                    "name": "Brevity",
                                                    "description": "The prompt is rewritten to ensure the response is no more than 50 words.",
                                                    "min_score": 0,
                                                    "max_score": 3
                                                },
                                                {
                                                    "name": "Clarity",
                                                    "description": "The prompt maintains clarity and does not introduce ambiguity.",
                                                    "min_score": 0,
                                                    "max_score": 3
                                                },
                                                {
                                                    "name": "Effectiveness",
                                                    "description": "The prompt effectively conveys the need for a concise response.",
                                                    "min_score": 0,
                                                    "max_score": 4
                                                }
                                            ]
                                        },
                                        "context": "The goal is to get the student to think about controlling the length of the output by modifying the prompt itself rather than relying solely on max token length parameter."
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Use Variables in Prompts",
                    "description": "Making prompts more dynamic.",
                    "tasks": [
                        {
                            "name": "Variables",
                            "description": "Explain how to use variables to create prompts that can be adjusted according to different inputs.",
                            "type": "learning_material",
                            "details": {
                                "content": "To reuse prompts and make it more dynamic use variables in the prompt, which can be changed for different inputs. E.g. as shown in Table 20, a prompt which gives facts about a city. Instead of hardcoding the city name in the prompt, use a variable. Variables can save you time and effort by allowing you to avoid repeating yourself. If you need to use the same piece of information in multiple prompts, you can store it in a variable and then reference that variable in each prompt. This makes a lot of sense when integrating prompts into your own applications."
                            }
                        },
                        {
                            "name": "Variables in Action",
                            "description": "Ask learners to define prompts that can use different variables to improve the reusability of those prompts.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Imagine you are building a chatbot for a local library. The chatbot should be able to provide information about library hours, available books, and upcoming events. Design three different prompts using variables to retrieve the information requested. For each prompt, clearly indicate the variables you would use and the potential values for those variables.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Prompt Design",
                                            "criteria": [
                                                {
                                                    "name": "Clarity",
                                                    "description": "The prompt is easy to understand and clearly defines the information needed.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Relevance",
                                                    "description": "The prompt effectively uses variables to target specific information about the library.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Completeness",
                                                    "description": "The prompt provides all the necessary details for the LLM to generate a useful response.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Variable Usage",
                                                    "description": "Effective use of variables for reuse of prompts",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "This exercise tests the learner's understanding of how to use variables to make prompts reusable and dynamic. The prompts should be well-defined and use variables effectively to target specific information about the library. Consider the design with simplicity and make sure the questions are easy to understand for both you and the model. As a rule of thumb, if it's already confusing for you it will likely be also confusing for the model. Try not to use complex language and don't provide unnecessary information."
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Experiment with Input Formats and Writing Styles",
                    "description": "Trying out different models, configurations, formats and word choices.",
                    "tasks": [
                        {
                            "name": "Formats and Writing Styles",
                            "description": "Explain how the different formats and writing styles can lead to different results and that trying out different options is valuable.",
                            "type": "learning_material",
                            "details": {
                                "content": "Different models, model configurations, prompt formats, word choices, and submits can yield different results. Therefore, it's important to experiment with prompt attributes like the style, the word choice, and the type prompt (zero shot, few shot, system prompt).\n\nFor example a prompt with the goal to generate text about the revolutionary video game console Sega Dreamcast, can be formulated as a question, a statement or an instruction, resulting in different outputs:\n\n*   Question: What was the Sega Dreamcast and why was it such a revolutionary console?\n*   Statement: The Sega Dreamcast was a sixth-generation video game console released by Sega in 1999. It...\n*   Instruction: Write a single paragraph that describes the Sega Dreamcast console and explains why it was so revolutionary."
                            }
                        },
                        {
                            "name": "Experiment",
                            "description": "Ask learners to write some prompts and try different input formats and writing styles to analyze the results.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Imagine you are tasked with creating a marketing campaign for a new brand of artisanal coffee. The brand prides itself on ethical sourcing, sustainable practices, and unique flavor profiles.  Develop three different prompts to generate taglines for this coffee brand, each employing a distinct input format or writing style:\n\n1.  A question to generate a tagline.\n2.  A statement to generate a tagline.\n3.  An instruction to generate a tagline.\n\nAfter you write each prompt, run them to generate 3 taglines each and write them down. Then analyze and explain which prompt yielded the most compelling and brand-aligned taglines and why.\n",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Marketing Campaign Tagline Evaluation",
                                            "criteria": [
                                                {
                                                    "name": "Clarity",
                                                    "description": "Taglines are clear, concise, and easily understood.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Relevance",
                                                    "description": "Taglines are relevant to the coffee brand's values (ethical sourcing, sustainability, unique flavors).",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Creativity",
                                                    "description": "Taglines are creative, memorable, and capture attention.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Format Adherence",
                                                    "description": "Prompts are written in correct specified format with clear explanation of which one is used.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Results Analysis",
                                                    "description": "Explains which type of prompt (question, statement, instruction) worked better.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "This question requires you to experiment with different prompt formats (question, statement, instruction) to generate marketing taglines. The goal is to identify which format yields the best results in terms of clarity, relevance, and creativity. Pay attention to the structure of your prompts and the language used, and thoroughly analyze the generated taglines to determine the most effective approach. The raw content here is that the brand values are the ethical sourcing, sustainability, and unique flavor profiles."
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "For Few-Shot Prompting with Classification Tasks, Mix Up the Classes",
                    "description": "Ensuring the model is learning to identify key features.",
                    "tasks": [
                        {
                            "name": "Few-Shot prompts",
                            "description": "Explain why the order of examples in the prompt is not important. Explain what you have to consider when you are working on a classification task.",
                            "type": "learning_material",
                            "details": {
                                "content": "Generally speaking, the order of your few-shots examples should not matter much. However, when doing classification tasks, make sure you mix up the possible response classes in the few shot examples. This is because you might otherwise be overfitting to the specific order of the examples. By mixing up the possible response classes, you can ensure that the model is learning to identify the key features of each class, rather than simply memorizing the order of the examples. This will lead to more robust and generalizable performance on unseen data."
                            }
                        },
                        {
                            "name": "Improve the prompt",
                            "description": "Ask learners to rewrite classification prompts by mixing up the classes to make sure the LLM can learn the key features.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "You're creating a few-shot prompt for classifying customer reviews of a restaurant as either 'Positive', 'Negative', or 'Neutral'. The initial prompt is structured as follows:\n\nReview: 'The food was amazing, and the service was excellent!'\nSentiment: Positive\n\nReview: 'The waiter was rude, and the food was cold.'\nSentiment: Negative\n\nReview: 'It was okay, nothing special.'\nSentiment: Neutral\n\nReview: 'The ambiance was great, but the prices were too high.'\nSentiment: Positive\n\nRewrite this prompt to ensure the model learns to identify the key features of each class (Positive, Negative and Neutral) effectively.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Prompt Improvement",
                                            "criteria": [
                                                {
                                                    "name": "Clarity",
                                                    "description": "The rewritten prompt should be clear and easy to understand.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Diversity",
                                                    "description": "The rewritten prompt should have a good mix of examples.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Accuracy",
                                                    "description": "The sentiment labels are correct for the provided review.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Explanation",
                                                    "description": "The rewritten prompt should have a short explanation of why the order was changed.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "The key is to mix up the order of the classes (Positive, Negative, Neutral) to prevent the model from overfitting to a specific sequence.  Also ensure that the model is learning to identify the key features of each class, rather than simply memorizing the order of the examples."
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Adapt to Model Updates",
                    "description": "Adjust prompts to leverage new model features.",
                    "tasks": [
                        {
                            "name": "Model Updates",
                            "description": "Explain the importance of testing prompts with newer model versions.",
                            "type": "learning_material",
                            "details": {
                                "content": "It's important for you to stay on top of model architecture changes, added data, and capabilities. Try out newer model versions and adjust your prompts to better leverage new model features. Tools like Vertex Al Studio are great to store, test, and document the various versions of your prompt."
                            }
                        },
                        {
                            "name": "Adjust prompts",
                            "description": "Ask learners to keep existing prompts and adjust them so they can be used with newer models.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "You've been using a prompt to generate summaries of customer reviews for a product using a model called 'ReviewSummarizer-v1'. This prompt has been working well, providing concise and accurate summaries. A new, improved model 'ReviewSummarizer-v2' is released, promising better accuracy and more nuanced summaries. Describe how you would adapt your existing prompt to take advantage of the new model's capabilities. Include specific examples of adjustments you might make.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Prompt Adaptation",
                                            "criteria": [
                                                {
                                                    "name": "Clarity",
                                                    "description": "The answer clearly explains the process of adapting the prompt.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Specificity",
                                                    "description": "The answer provides specific examples of prompt adjustments.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Relevance",
                                                    "description": "The answer is relevant to the context of adapting prompts for new models.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Completeness",
                                                    "description": "The answer covers a comprehensive approach to prompt adaptation.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "The goal of this question is to assess the learner's understanding of adapting prompts to leverage the capabilities of newer models. Look for evidence that they understand how to modify prompts to take advantage of improved accuracy and nuanced summaries. The prompt outputs can differ across models, across sampling settings, and even across different versions of the same model. Moreover, even across identical prompts to the same model, small differences in output sentence formatting and word choice can occur. The learner should also be aware of new model architecture changes, added data, and capabilities to leverage new model features. "
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Experiment with Output Formats",
                    "description": "Experimenting with the output format.",
                    "tasks": [
                        {
                            "name": "Output Formats",
                            "description": "Explain the importance of considering the output format. Explain how returning data in a structured format can be useful.",
                            "type": "learning_material",
                            "details": {
                                "content": "Besides the prompt input format, consider experimenting with the output format. For non-creative tasks like extracting, selecting, parsing, ordering, ranking, or categorizing data try having your output returned in a structured format like JSON or XML.There are some benefits in returning JSON objects from a prompt that extracts data. In a real-world application I don't need to manually create this JSON format, I can already return the data in a sorted order (very handy when working with datetime objects), but most importantly, by prompting for a JSON format it forces the model to create a structure and limit hallucinations.In summary, benefits of using JSON for your output:Returns always in the same style. Focus on the data you want to receive. Less chance for hallucinations. Make it relationship aware. You get data types. You can sort it.Table 4 in the few-shot prompting section shows an example on how to return structured output.While returning data in JSON format offers numerous advantages, it's not without its drawbacks. The structured nature of JSON, while beneficial for parsing and use in applications, requires significantly more tokens than plain text, leading to increased processing time and higher costs. Furthermore, JSON's verbosity can easily consume the entire output window, becoming especially problematic when the generation is abruptly cut off due to token limits. This truncation often results in invalid JSON, missing crucial closing braces or brackets, rendering the output unusable. Fortunately, tools like the json-repair library (available on PyPI) can be invaluable in these situations. This library intelligently attempts to automatically fix incomplete or malformed JSON objects, making it a crucial ally when working with LLM-generated JSON, especially when dealing with potential truncation issues.Using structured JSON as an output is a great solution, as we've seen multiple times in this paper. But what about input? While JSON is excellent for structuring the output the LLM generates, it can also be incredibly useful for structuring the input you provide. This is where JSON Schemas come into play. A JSON Schema defines the expected structure and data types of your JSON input. By providing a schema, you give the LLM a clear blueprint of the data it should expect, helping it focus its attention on the relevant information and reducing the risk of misinterpreting the input. Furthermore, schemas can help establish relationships between different pieces of data and even make the LLM \"time-aware\" by including date or timestamp fields with specific formats."
                            }
                        },
                        {
                            "name": "Rewrite Output format",
                            "description": "Ask learners to consider existing prompts and rewrite the output format using, for example, JSON.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Consider the prompt: \"Compose a short story about a time traveler visiting ancient Rome.\" Rewrite this prompt to specify that the output should be in JSON format. The JSON should include the keys: `title`, `setting`, `characters`, and `plot_summary`. Provide an example of the JSON output format you expect.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "JSON Conversion",
                                            "criteria": [
                                                {
                                                    "name": "Format",
                                                    "description": "The rewritten prompt requests a JSON output.",
                                                    "min_score": 0,
                                                    "max_score": 1
                                                },
                                                {
                                                    "name": "Keys",
                                                    "description": "Keys specified in the prompt are relevant for the output",
                                                    "min_score": 0,
                                                    "max_score": 1
                                                },
                                                {
                                                    "name": "JSON example",
                                                    "description": "The JSON example illustrates the desired format.",
                                                    "min_score": 0,
                                                    "max_score": 1
                                                }
                                            ]
                                        },
                                        "context": "This question assesses the learner's ability to adapt a prompt to specify a JSON output format. The JSON format should include key elements of the story."
                                    }
                                ]
                            }
                        }
                    ]
                }
            ]
        },
        {
            "name": "Working with Structured Data",
            "concepts": [
                {
                    "name": "JSON Repair",
                    "description": "Fixing incomplete or malformed JSON objects.",
                    "tasks": [
                        {
                            "name": "JSON Objects",
                            "description": "Explain that while returning data in JSON offers advantages, it's not without its drawbacks. JSON's verbosity can easily consume the entire output window, becoming especially problematic when the generation is abruptly cut off due to token limits.",
                            "type": "learning_material",
                            "details": {
                                "content": "While returning data in JSON format offers numerous advantages, it's not without its drawbacks. The structured nature of JSON, while beneficial for parsing and use in applications, requires significantly more tokens than plain text, leading to increased processing time and higher costs. Furthermore, JSON's verbosity can easily consume the entire output window, becoming especially problematic when the generation is abruptly cut off due to token limits."
                            }
                        },
                        {
                            "name": "JSON problems",
                            "description": "Ask learners what problems they can expect when they are using JSON as output. Ask them to provide the name of some tools that can be used to automatically fix incomplete or malformed JSON objects.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "When using JSON as output in LLMs, what potential problems might you encounter, especially concerning output window limitations? Additionally, name a tool that can help resolve these problems.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "JSON Output Assessment",
                                            "criteria": [
                                                {
                                                    "name": "Identification",
                                                    "description": "Accurately identifies potential problems when using JSON as output.",
                                                    "min_score": 0,
                                                    "max_score": 3
                                                },
                                                {
                                                    "name": "Explanation",
                                                    "description": "Clearly explains the reasons behind each identified problem.",
                                                    "min_score": 0,
                                                    "max_score": 3
                                                },
                                                {
                                                    "name": "Tool Suggestion",
                                                    "description": "Suggests a valid tool for resolving identified JSON output problems.",
                                                    "min_score": 0,
                                                    "max_score": 4
                                                }
                                            ]
                                        },
                                        "context": "The structured nature of JSON, while beneficial for parsing and use in applications, requires significantly more tokens than plain text, leading to increased processing time and higher costs. Furthermore, JSON's verbosity can easily consume the entire output window, becoming especially problematic when the generation is abruptly cut off due to token limits. This truncation often results in invalid JSON, missing crucial closing braces or brackets, rendering the output unusable. Fortunately, tools like the `json-repair` library (available on PyPI) can be invaluable in these situations."
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Working with Schemas",
                    "description": "Using JSON Schemas to define the expected structure and data types of your JSON input.",
                    "tasks": [
                        {
                            "name": "JSON Schema usage",
                            "description": "Explain that by providing a schema, you give the LLM a clear blueprint of the data it should expect, helping it focus its attention on the relevant information and reducing the risk of misinterpreting the input.",
                            "type": "learning_material",
                            "details": {
                                "content": "Using structured JSON as an output is a great solution, as we've seen multiple times in this paper. But what about input? While JSON is excellent for structuring the output the LLM generates, it can also be incredibly useful for structuring the input you provide. This is where JSON Schemas come into play. A JSON Schema defines the expected structure and data types of your JSON input. By providing a schema, you give the LLM a clear blueprint of the data it should expect, helping it focus its attention on the relevant information and reducing the risk of misinterpreting the input. Furthermore, schemas can help establish relationships between different pieces of data and even make the LLM \"time-aware\" by including date or timestamp fields with specific formats."
                            }
                        },
                        {
                            "name": "Provide schemas",
                            "description": "Ask learners to provide some example schemas for different cases. What attributes would they define for each schema?",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Imagine you're building an e-commerce platform and want to use JSON schemas to define the structure of product data. Provide schemas for the following cases, focusing on defining appropriate attributes and data types:\n\n1.  **A clothing item (e.g., a t-shirt):**  Consider attributes like size, color, material, fit, and any other relevant details.\n2.  **A book:** Think about attributes such as title, author, ISBN, genre, publication date, and number of pages.\n3.  **A digital product (e.g., a software license):** Include attributes like license key, validity period, supported operating systems, and number of activations.\n4.  **A service (e.g., a subscription to a streaming platform):** Consider attributes such as service name, subscription duration, features included, and billing cycle.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "JSON Schema Design",
                                            "criteria": [
                                                {
                                                    "name": "Completeness",
                                                    "description": "Schema includes all relevant attributes for the given product type.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Data Types",
                                                    "description": "Appropriate data types (string, number, boolean, array, etc.) are assigned to each attribute.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Descriptions",
                                                    "description": "Each attribute has a clear and concise description explaining its purpose.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Formatting",
                                                    "description": "The schema is well-formatted and easy to read.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Realism",
                                                    "description": "The schema follows realistic practices used in a real-world context.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "This question tests the learner's ability to apply the concept of JSON schemas to different real-world scenarios. Evaluate whether they have chosen relevant attributes and appropriate data types for each product. The better they know about JSON and real-world contexts, the better they will be able to answer this question."
                                    }
                                ]
                            }
                        }
                    ]
                }
            ]
        },
        {
            "name": "Collaboration and Documentation",
            "concepts": [
                {
                    "name": "Experiment Together with Other Prompt Engineers",
                    "description": "Find multiple people to make an attempt.",
                    "tasks": [
                        {
                            "name": "Collaboration",
                            "description": "Explain that in a situation where you have to try to come up with a good prompt, you might want to find multiple people to make an attempt.",
                            "type": "learning_material",
                            "details": {
                                "content": "If you are in a situation where you have to try to come up with a good prompt, you might want to find multiple people to make an attempt."
                            }
                        },
                        {
                            "name": "Collaboration Advantages",
                            "description": "What advantages does collaboration brings to the process of prompt engineering?",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "objective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "According to the text, what is the primary advantage of collaborating with multiple people in prompt engineering?",
                                        "correct_answer": "Variance in performance between different prompt attempts.",
                                        "scorecard": null,
                                        "context": "The text states: 'If you are in a situation where you have to try to come up with a good prompt, you might want to find multiple people to make an attempt. When everyone follows the best practices (as listed in this chapter) you are going to see a variance in performance between all the different prompt attempts.'"
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "CoT Best Practices",
                    "description": "Some best practices for Chain of Thought Prompting",
                    "tasks": [
                        {
                            "name": "CoT Prompting",
                            "description": "Explain some best practices for CoT prompting",
                            "type": "learning_material",
                            "details": {
                                "content": "For CoT prompting, putting the answer after the reasoning is required because the generation of the reasoning changes the tokens that the model gets when it predicts the final answer.\n\nWith CoT and self-consistency you need to be able to extract the final answer from your prompt, separated from the reasoning.\n\nFor CoT prompting, set the temperature to 0.\n\nChain of thought prompting is based on greedy decoding, predicting the next word in a sequence based on the highest probability assigned by the language model. Generally speaking, when using reasoning, to come up with the final answer, there's likely one single correct answer. Therefore the temperature should always set to 0.  \n\n"
                            }
                        },
                        {
                            "name": "Review the prompts",
                            "description": "Ask learners to review some existing prompts and adjust them to apply the CoT best practices.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Review the following prompt:\n\n\"I want to generate a description for a new product called 'SmartConnect'.\"\n\nWhat specific changes would you recommend to improve this prompt, taking into account the CoT best practices from the learning material?",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Prompt Improvement Assessment",
                                            "criteria": [
                                                {
                                                    "name": "Reasoning",
                                                    "description": "Demonstrates a clear understanding of CoT principles.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Clarity",
                                                    "description": "Provides specific, actionable suggestions.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Completeness",
                                                    "description": "Addresses multiple aspects of prompt optimization (answer placement, temperature, separation).",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "This question tests the learner's ability to apply the CoT best practices to improve a simple prompt. The student should consider adding more details to guide the LLM, adding the answer after the reasoning, set the temperature to 0, and separate the reasoning from the answer."
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Document the Various Prompt Attempts",
                    "description": "Documenting prompt attempts so you can learn what went well and what did not.",
                    "tasks": [
                        {
                            "name": "Document prompts",
                            "description": "Explain why is important to document your prompt attempts. What's the best way to document the prompts?",
                            "type": "learning_material",
                            "details": {
                                "content": "The last tip was mentioned before in this chapter, but we can't stress enough how important it is: document your prompt attempts in full detail so you can learn over time what went well and what did not.\n\nPrompt outputs can differ across models, across sampling settings, and even across different versions of the same model. Moreover, even across identical prompts to the same model, small differences in output sentence formatting and word choice can occur. (For example, as mentioned previously, if two tokens have the same predicted probability, ties may be broken randomly. This can then impact subsequent predicted tokens.).\n\nWe recommend creating a Google Sheet with Table 21 as a template. The advantages of this approach are that you have a complete record when you inevitably have to revisit your prompting work-either to pick it up in the future (you'd be surprised how much you can forget after just a short break), to test prompt performance on different versions of a model, and to help debug future errors.\n\nBeyond the fields in this table, it's also helpful to track the version of the prompt (iteration), a field to capture if the result was OK/NOT OK/SOMETIMES OK, and a field to capture feedback. If you're lucky enough to be using Vertex Al Studio, save your prompts (using the same name and version as listed in your documentation) and track the hyperlink to the saved prompt in the table. This way, you're always one click away from re-running your prompts.\n\nWhen working on a retrieval augmented generation system, you should also capture the specific aspects of the RAG system that impact what content was inserted into the prompt, including the query, chunk settings, chunk output, and other information.\n\nOnce you feel the prompt is close to perfect, take it to your project codebase. And in the codebase, save prompts in a separate file from code, so it's easier to maintain. Finally, ideally your prompts are part of an operationalized system, and as a prompt engineer you should rely on automated tests and evaluation procedures to understand how well your prompt generalizes to a task.\n\nPrompt engineering is an iterative process. Craft and test different prompts, analyze, and document the results. Refine your prompt based on the model's performance. Keep experimenting until you achieve the desired output. When you change a model or model configuration, go back and keep experimenting with the previously used prompts."
                            }
                        },
                        {
                            "name": "Follow up",
                            "description": "Ask learners to document the prompts they have been using and keep track of the performance of each of them.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Reflect on your recent prompt engineering endeavors. Choose a specific project or task where you utilized prompt engineering. Document the prompt/s that you have been using and track the performance of each of them. What was the initial goal, and what were the key steps you took in crafting and refining your prompts? What specific metrics or criteria did you use to evaluate the performance of your prompts? What did you learn about the effectiveness of different prompting techniques or strategies? What challenges did you encounter during the prompt engineering process, and how did you overcome them?",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Prompt Engineering Documentation",
                                            "criteria": [
                                                {
                                                    "name": "Clarity",
                                                    "description": "The response clearly articulates the prompt engineering project or task, initial goals, and key steps taken.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Detail",
                                                    "description": "The response provides detailed information on the prompts used, metrics for evaluation, and specific results or outcomes achieved.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Insights",
                                                    "description": "The response demonstrates insightful analysis of the effectiveness of different prompting techniques, challenges encountered, and lessons learned.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Organization",
                                                    "description": "The response is well-organized, coherent, and easy to follow, with clear headings or sections as appropriate.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "This question aims to encourage learners to apply the concepts of prompt engineering in practical scenarios and reflect on their experiences. Encourage them to document prompts, track performance metrics, and analyze the effectiveness of different techniques."
                                    }
                                ]
                            }
                        }
                    ]
                }
            ]
        },
        {
            "name": "Conclusion: Prompt Engineering for Real-World Applications",
            "concepts": [
                {
                    "name": "Putting It All Together",
                    "description": "Recap of key concepts and techniques learned throughout the course.",
                    "tasks": [
                        {
                            "name": "Course Summary",
                            "description": "Summarize the main modules and concepts covered in the course.",
                            "type": "learning_material",
                            "details": {
                                "content": "This whitepaper discussed prompt engineering and covered various prompting techniques, including zero-shot prompting, few-shot prompting, system prompting, role prompting, contextual prompting, step-back prompting, chain of thought, self-consistency, and tree of thoughts. It also looked into ways of automating prompts and discussed the challenges of gen AI. The whitepaper concluded with best practices on how to become a better prompt engineer."
                            }
                        },
                        {
                            "name": "Final Quiz",
                            "description": "Comprehensive quiz covering all the topics taught in the course.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "objective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Which of the following is NOT a common configuration setting used to determine how predicted token probabilities are processed to choose a single output token in LLMs?",
                                        "correct_answer": "Output Length",
                                        "scorecard": null,
                                        "context": "The common configuration settings that determine how predicted token probabilities are processed to choose a single output token are: Temperature, top-K, and top-P."
                                    },
                                    {
                                        "question_type": "objective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "What does a higher temperature setting typically result in when using LLMs?",
                                        "correct_answer": "More diverse or unexpected results",
                                        "scorecard": null,
                                        "context": "Higher temperatures lead to more diverse or unexpected results in LLMs, while lower temperatures are good for prompts that expect a more deterministic response."
                                    },
                                    {
                                        "question_type": "objective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Which of the following prompts represents the fewest shot prompt?",
                                        "correct_answer": "Zero-Shot",
                                        "scorecard": null,
                                        "context": "A zero-shot prompt is the simplest type of prompt that only provides a description of a task and some text for the LLM to get started with, without any examples."
                                    },
                                    {
                                        "question_type": "objective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Which of the following statements best describes the purpose of system prompting?",
                                        "correct_answer": "Sets the overall context and purpose for the language model.",
                                        "scorecard": null,
                                        "context": "System prompting sets the overall context and purpose for the language model, defining the 'big picture' of what the model should be doing, like translating a language or classifying a review."
                                    },
                                    {
                                        "question_type": "objective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "In the context of LLMs, what is role prompting?",
                                        "correct_answer": "Assigning a specific character or identity for the language model to adopt.",
                                        "scorecard": null,
                                        "context": "Role prompting assigns a specific character or identity for the language model to adopt. This helps the model generate responses that are consistent with the assigned role and its associated knowledge and behavior."
                                    },
                                    {
                                        "question_type": "objective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Which prompting technique involves prompting the LLM to first consider a general question related to the specific task at hand before answering the actual prompt?",
                                        "correct_answer": "Step-back Prompting",
                                        "scorecard": null,
                                        "context": "Step-back prompting is a technique for improving performance by prompting the LLM to first consider a general question related to the specific task at hand, and then feeding the answer to that general question into a subsequent prompt for the specific task."
                                    },
                                    {
                                        "question_type": "objective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "What is the primary goal of Chain of Thought (CoT) prompting?",
                                        "correct_answer": "Improve the reasoning capabilities of LLMs by generating intermediate reasoning steps.",
                                        "scorecard": null,
                                        "context": "Chain of Thought (CoT) prompting is a technique for improving the reasoning capabilities of LLMs by generating intermediate reasoning steps."
                                    },
                                    {
                                        "question_type": "objective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Which technique combines sampling and majority voting to generate diverse reasoning paths and select the most consistent answer?",
                                        "correct_answer": "Self-Consistency",
                                        "scorecard": null,
                                        "context": "Self-consistency combines sampling and majority voting to generate diverse reasoning paths and select the most consistent answer, improving the accuracy and coherence of responses generated by LLMs."
                                    },
                                    {
                                        "question_type": "coding",
                                        "answer_type": null,
                                        "coding_languages": [
                                            "Python"
                                        ],
                                        "content": "Given the following Python code, debug what is wrong and explain how I can improve the code.\n```python\nimport os\nimport shutil\n\n# Get the folder name from the user\nfolder_name = input(\"Enter the folder name: \")\nprefix = input(\"Enter the string to prepend to the filename: \")\ntext = toUpperCase(prefix)\n\n# Check if the folder exists\nif not os.path.isdir(folder_name):\n    print(\"Folder does not exist.\")\n    exit(1)\n\n# Get a list of all files in the folder\nfiles = os.listdir(folder_name)\n\n# Iterate over the files\nfor file in files:\n    # Create a new file name with the prefix \"draft_\"\n    new_filename = f\"{text}_{file}\"\n\n    # Move the file to the new name\n    shutil.move(os.path.join(folder_name, file),\n                os.path.join(folder_name, new_file_name))\n\n# Print a success message\nprint(\"Files renamed successfully.\")\n```",
                                        "correct_answer": "The code calls the `toUpperCase` function to convert `prefix` to uppercase, but that function is not defined. To fix this issue, you can use the `upper()` method of\nthe string class which converts a given string into uppercase.\nAdditionally, there are a few other improvements that can be made to the code:\n\n1.  The file extension of the new filenames is not kept. It's better to extract the\n    extension from the original filename and append it to the new filename.\n2.  The code assumes that there are no spaces in the folder name. However,\n    it is a good practice to handle spaces gracefully by replacing them with\n    underscores or hyphens.\n3.  Using the 'f` string syntax for string interpolation is more readable and\n    concise than the traditional `+` operator.\n4.  The code doesn't handle errors that might occur during the renaming process.\n    It would be better to wrap the `shutil.move` call in a `try...except` block to\n    catch any potential errors.\n",
                                        "scorecard": null,
                                        "context": "This question aims to check if the student can debug existing code using prompts."
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Capstone Project: Building a Prompt-Powered Application",
                    "description": "Applying prompt engineering skills to develop a practical application.",
                    "tasks": [
                        {
                            "name": "Project Overview",
                            "description": "Introduce the capstone project, outlining the problem statement and desired functionality. (e.g., build a chatbot for a specific purpose, create a content generation tool, etc.)",
                            "type": "learning_material",
                            "details": {
                                "content": "In this capstone project, you will leverage the prompt engineering techniques learned throughout this course to develop a practical application. This project is designed to be a hands-on experience where you can apply your knowledge to solve a real-world problem or create a useful tool.\n\nThe project involves designing, implementing, and evaluating a prompt-powered application. This could be anything from a chatbot that assists with a specific task, to a content generation tool that produces creative text formats, or any other application that creatively uses prompts to achieve a desired functionality.\n\nThe goal of this capstone is to:\n\n1.  Define a clear problem statement and desired functionality for your application.\n2.  Design effective prompts that guide the LLM to produce the desired outputs.\n3.  Implement your application, integrating the prompts and LLM interactions.\n4.  Evaluate the performance of your application, identifying areas for improvement and optimization.\n\nSuccessful completion of this project will demonstrate your mastery of prompt engineering principles and your ability to create innovative solutions using large language models."
                            }
                        },
                        {
                            "name": "Capstone Project",
                            "description": "Learners work individually or in groups to design, implement, and evaluate their prompt-powered application, applying the knowledge and skills acquired throughout the course.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Imagine you are tasked with developing a prompt-powered application to assist students in learning a new language. Describe the key functionalities your application would offer, detailing how prompt engineering techniques would be utilized to enhance the learning experience. Consider features like vocabulary learning, grammar assistance, and cultural insights.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Application Design and Prompt Engineering",
                                            "criteria": [
                                                {
                                                    "name": "Functionality",
                                                    "description": "Clearly defines key functionalities of the language learning application, showing a clear understanding of how the application will facilitate learning.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Prompt Engineering",
                                                    "description": "Demonstrates a deep understanding of prompt engineering techniques, explaining how prompts can be used to dynamically tailor the learning experience.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Practicality",
                                                    "description": "Highlights practical considerations of application design, considering the user interface and potential challenges and limitations.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Creativity",
                                                    "description": "Displays innovation in application design, showcasing novel ideas for language learning that go beyond traditional approaches.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "This question assesses the learner's ability to apply prompt engineering principles to design a practical, real-world application."
                                    },
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Develop a detailed project proposal outlining the steps required to build a prompt-powered chatbot that can provide personalized mental health support. Include specific examples of prompts that would be used to guide conversations, assess user needs, and offer tailored advice or resources.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Chatbot Development and Prompt Application",
                                            "criteria": [
                                                {
                                                    "name": "Project Scope",
                                                    "description": "Clearly defines the project scope, outlining the purpose and intended audience of the mental health chatbot.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Prompt Examples",
                                                    "description": "Provides specific and effective prompt examples, demonstrating how the chatbot can use prompts to guide conversations and assess user needs.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Ethical Considerations",
                                                    "description": "Addresses the ethical considerations of using a chatbot for mental health support, including data privacy, bias, and professional responsibility.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Resourcefulness",
                                                    "description": "Identifies relevant resources that could be offered by the chatbot, showing an understanding of the support available to those in need.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "This question evaluates the learner's ability to design a beneficial application with careful attention to ethical and practical considerations."
                                    },
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Imagine you are creating an application that helps users generate creative writing prompts. Discuss how you would use prompt engineering to ensure that the generated prompts are diverse, original, and tailored to the user's interests. Provide examples of prompts you would use to guide the application.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Creative Writing Prompt Application",
                                            "criteria": [
                                                {
                                                    "name": "Prompt Design",
                                                    "description": "Details how the creative writing prompts are crafted and how they can engage the user.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Diversification",
                                                    "description": "Shows how to create a diverse set of prompt styles to improve the potential writing ideas.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Customization",
                                                    "description": "Explores how the creative writing prompts will be individualized for each writer to improve engagement.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Feedback",
                                                    "description": "Describes methods that can be applied to get feedback from the writers to improve future prompt generations.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "This question tests the learner's ability to implement prompts to improve a writing application."
                                    }
                                ]
                            }
                        }
                    ]
                }
            ]
        }
    ]
}