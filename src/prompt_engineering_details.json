{
    "name": "Introduction to Prompt Engineering",
    "modules": [
        {
            "name": "Introduction to Prompts",
            "concepts": [
                {
                    "name": "What are Prompts?",
                    "description": "Understanding the fundamental concept of prompts in the context of Large Language Models.",
                    "tasks": [
                        {
                            "name": "Defining Prompts",
                            "description": "Explain what a prompt is in simple terms. Highlight that prompts are inputs (usually text) that guide the model to generate a desired output.",
                            "type": "learning_material",
                            "details": {
                                "content": "When thinking about a large language model input and output, a text prompt (sometimes accompanied by other modalities such as image prompts) is the input the model uses to predict a specific output. You don't need to be a data scientist or a machine learning engineer - everyone can write a prompt. However, crafting the most effective prompt can be complicated. Many aspects of your prompt affect its efficacy: the model you use, the model's training data, the model configurations, your word-choice, style and tone, structure, and context all matter. Therefore, prompt engineering is an iterative process. Inadequate prompts can lead to ambiguous, inaccurate responses, and can hinder the model's ability to provide meaningful output.\n\nWhen you chat with the Gemini chatbot,\u00b9 you basically write prompts, however this whitepaper focuses on writing prompts for the Gemini model within Vertex Al or by using the API, because by prompting the model directly you will have access to the configuration such as temperature etc.\n\nRemember how an LLM works; it's a prediction engine. The model takes sequential text as an input and then predicts what the following token should be, based on the data it was trained on. The LLM is operationalized to do this over and over again, adding the previously predicted token to the end of the sequential text for predicting the following token. The next token prediction is based on the relationship between what's in the previous tokens and what the LLM has seen during its training.\n\nWhen you write a prompt, you are attempting to set up the LLM to predict the right sequence of tokens. Prompt engineering is the process of designing high-quality prompts that guide LLMs to produce accurate outputs. This process involves tinkering to find the best prompt, optimizing prompt length, and evaluating a prompt's writing style and structure in relation to the task. In the context of natural language processing and LLMs, a prompt is an input provided to the model to generate a response or prediction.\n\nThese prompts can be used to achieve various kinds of understanding and generation tasks such as text summarization, information extraction, question and answering, text classification, language or code translation, code generation, and code documentation or reasoning."
                            }
                        },
                        {
                            "name": "Prompt Examples",
                            "description": "Provide examples of prompts, like asking a question, starting a story, or giving instructions. Ask them to come up with more examples",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Based on your understanding of prompts, provide three distinct examples of prompts one might use with a Large Language Model. For each example, briefly describe the intended outcome or purpose of the prompt.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Prompt Quality Assessment",
                                            "criteria": [
                                                {
                                                    "name": "Relevance",
                                                    "description": "The prompt is appropriate and applicable to LLMs.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Clarity",
                                                    "description": "The prompt is clear, concise, and easily understood.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Creativity",
                                                    "description": "The prompt demonstrates creative or practical application.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "This question aims to test the student's understanding of the different types of prompts and their ability to formulate effective prompts for various use cases. Assess the relevance, clarity, and potential usefulness of the provided examples."
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Why is Prompt Engineering Important?",
                    "description": "Explaining why effective prompting is crucial for obtaining useful and accurate outputs from LLMs.",
                    "tasks": [
                        {
                            "name": "The Importance of Well-Crafted Prompts",
                            "description": "Explain how a poorly written prompt can lead to ambiguous, inaccurate, or unhelpful responses. Emphasize the iterative nature of prompt engineering and its role in refining LLM outputs.",
                            "type": "learning_material",
                            "details": {
                                "content": "When thinking about a large language model input and output, a text prompt (sometimes accompanied by other modalities such as image prompts) is the input the model uses to predict a specific output. You don't need to be a data scientist or a machine learning engineer - everyone can write a prompt. However, crafting the most effective prompt can be complicated. Many aspects of your prompt affect its efficacy: the model you use, the model's training data, the model configurations, your word-choice, style and tone, structure, and context all matter. Therefore, prompt engineering is an iterative process. Inadequate prompts can lead to ambiguous, inaccurate responses, and can hinder the model's ability to provide meaningful output."
                            }
                        },
                        {
                            "name": "Identifying Bad Prompts",
                            "description": "Provide examples of ineffective prompts and ask learners to identify the issues (e.g., ambiguity, lack of context).",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "objective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Which of the following prompts is most likely to produce an ambiguous or unhelpful response from a Large Language Model?\n\n(a) 'Summarize the key points of the IPCC Sixth Assessment Report focusing on climate change mitigation strategies.'\n(b) 'Write a poem about a rainy day in the style of Emily Dickinson.'\n(c) 'Tell me about cats.'\n(d) 'Explain the theory of general relativity in simple terms, suitable for a high school student.'",
                                        "correct_answer": "c",
                                        "scorecard": null,
                                        "context": "Option (c) is the most ambiguous because it lacks specific instructions or context, making it difficult for the LLM to generate a focused and useful response. The other options provide clear objectives and constraints, leading to more coherent and relevant outputs. This question tests the understanding of ambiguity in prompts and its impact on the quality of LLM outputs. The student must recognize that vague prompts are less effective than those with clear direction. A vague prompt can lead to outputs that are too broad or miss the user's actual intent. Clear and specific prompts help LLMs produce better, more targeted responses. Options (a), (b), and (d) are well-defined and provide enough context for the LLM to generate a useful response. Option (c) is too open-ended and lacks any specific focus, making it the least effective prompt."
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Factors Affecting Prompt Efficacy",
                    "description": "Exploring the different components that influence the quality of a prompt and its results.",
                    "tasks": [
                        {
                            "name": "Key Factors",
                            "description": "Discuss factors such as the choice of LLM, its training data, model configurations (temperature, top-k, etc.), word choice, style, tone, structure, and context.",
                            "type": "learning_material",
                            "details": {
                                "content": "When thinking about a large language model input and output, a text prompt (sometimes accompanied by other modalities such as image prompts) is the input the model uses to predict a specific output. You don't need to be a data scientist or a machine learning engineer - everyone can write a prompt. However, crafting the most effective prompt can be complicated. Many aspects of your prompt affect its efficacy: the model you use, the model's training data, the model configurations, your word-choice, style and tone, structure, and context all matter. Therefore, prompt engineering is an iterative process. Inadequate prompts can lead to ambiguous, inaccurate responses, and can hinder the model's ability to provide meaningful output."
                            }
                        },
                        {
                            "name": "Prompt Refinement Challenge",
                            "description": "Present a scenario where the LLM output is not ideal, and ask learners to modify different aspects of the prompt (word choice, structure, tone) to improve the result.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "You are building a customer service chatbot for a local library. A user inputs the following prompt:\n\n`Tell me about books.`\n\nThe chatbot responds with:\n\n`Books are good. You can read them. Libraries have them.`\n\nIdentify three distinct ways you could refine the original prompt to elicit a more helpful and informative response from the LLM. Explain how each change addresses specific factors influencing prompt efficacy (e.g., word choice, tone, structure, context).",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Prompt Refinement Assessment",
                                            "criteria": [
                                                {
                                                    "name": "Relevance",
                                                    "description": "The proposed refinements should directly address the user's implied needs and be relevant to the library's resources.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Specificity",
                                                    "description": "The refinements should include concrete examples of how to modify the prompt, rather than just abstract suggestions.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Explanation",
                                                    "description": "The explanation of how each change addresses prompt efficacy factors (word choice, tone, structure, context) should be clear, accurate, and insightful.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "This question tests the learner's ability to apply prompt engineering principles to a practical scenario. The goal is to move beyond theoretical understanding and demonstrate the ability to improve LLM outputs through targeted prompt modifications. The student should consider the role of clarity, specificity, and context in guiding the LLM towards a more useful response. Some potentially good responses could be: \n\n1. **Adding Context:**  Refine the prompt to `As a librarian, provide a brief overview of the different genres of books available in our library.` This adds the context of a librarian's role, which helps the LLM tailor its response to library-specific information, and also provides specific instructions on what kind of information to provide.\n2. **Being specific about the desired output:** Refine the prompt to `What are three popular fiction books and three popular non-fiction books currently available at the library, along with their authors and a brief summary of each?`. This explicitly defines the type of books, the number of books, and the specific details to include in the output.\n3. **Adjusting the tone:** Refine the prompt to `In an informative and engaging tone, introduce the diverse collection of books offered at the library, catering to various interests and age groups.` This adjusts the tone of the prompt, encouraging the LLM to adopt a more enthusiastic and welcoming style, and gives the LLM a desired list of parameters to use for generating the information. "
                                    }
                                ]
                            }
                        }
                    ]
                }
            ]
        },
        {
            "name": "Configuring LLM Output",
            "concepts": [
                {
                    "name": "Output Length",
                    "description": "Understanding how to control the length of the LLM's response.",
                    "tasks": [
                        {
                            "name": "Managing Token Generation",
                            "description": "Explain how the number of tokens to generate impacts computation, energy consumption, and response times.  Explain the meaning of Token in Layman terms",
                            "type": "learning_material",
                            "details": {
                                "content": "An important configuration setting is the number of tokens to generate in a response. Generating more tokens requires more computation from the LLM, leading to higher energy consumption, potentially slower response times, and higher costs.\n\nLLMs do not formally predict a single token. Rather, LLMs predict probabilities for what the next token could be, with each token in the LLM's vocabulary getting a probability. Those token probabilities are then sampled to determine what the next produced token will be.\n\nIn layman terms, tokens can be thought of as the building blocks of language that LLMs use to understand and generate text. When you ask an LLM to generate a response, it breaks down your request into a sequence of these tokens. The model then predicts the most likely sequence of tokens to form a relevant and coherent answer. Therefore, the number of tokens to generate is essentially the length of the response."
                            }
                        },
                        {
                            "name": "Adjusting Output Length",
                            "description": "Give examples where different output lengths might be desired (e.g., short summary vs. detailed explanation). Ask learners to modify prompts to produce outputs of varying lengths.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "objective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "In which of the following scenarios would you typically want to limit the output length of an LLM?",
                                        "correct_answer": "Generating a short summary of a news article",
                                        "scorecard": null,
                                        "context": "Limiting output length is useful when you need concise information, such as a summary or a tweet. This helps conserve computational resources and ensures the response is to the point. Choose the option that best exemplifies the need for a short, focused output. Other options can be useful as well but a short summary is the best answer here because it tests the direct understanding of when a short length is useful. If the learner answers incorrectly, explain to them why that scenario can be useful, but a shorter one is preferred, just like in the original example in the prompt engineering document. Be polite!"
                                    },
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "You are using an LLM to generate marketing copy for a new product. Describe how you would modify your prompt to explore variations of different lengths, ranging from a short tweet to a more detailed paragraph. Provide specific examples of how you would change the wording of your prompt to achieve this.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Prompt Length Variation",
                                            "criteria": [
                                                {
                                                    "name": "Instructions",
                                                    "description": "Does the response include detailed instructions to create copy of varying lengths (from a tweet to a paragraph)?",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Prompt Examples",
                                                    "description": "Are there specific examples of prompt wording to achieve different output lengths?",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Understanding",
                                                    "description": "Does the answer demonstrate a good understanding of how to modify prompts for length control?",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "This question aims to assess the learner's practical understanding of how to adjust prompts to control output length. The response should include clear instructions on how to modify the prompt, along with specific examples of prompt wording for different lengths. If they are struggling, refer to the prompt engineering guide in the document provided to help them out!"
                                    },
                                    {
                                        "question_type": "coding",
                                        "answer_type": null,
                                        "coding_languages": [
                                            "Python"
                                        ],
                                        "content": "Write a Python function called `generate_summary` that takes a text document and a desired length (in number of sentences) as input. Use the `transformers` library and a suitable summarization model to generate a summary of the specified length.  Ensure your code can handle different desired lengths (e.g., 1 sentence, 3 sentences, etc.).",
                                        "correct_answer": null,
                                        "scorecard": null,
                                        "context": "This coding exercise challenges learners to apply their understanding of output length control in a practical scenario. It tests their ability to use code to generate summaries of varying lengths, demonstrating both technical proficiency and an understanding of the trade-offs involved. Do not provide a solution as that defeats the purpose of the exercise, but you can point them to online tutorials if they have trouble with coding."
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Sampling Controls: Temperature",
                    "description": "Learning about the temperature setting and how it affects the randomness of the output.",
                    "tasks": [
                        {
                            "name": "Temperature Explained",
                            "description": "Explain how temperature controls the degree of randomness in token selection. Explain that lower temperatures lead to deterministic responses, while higher temperatures introduce more diversity and unexpected results.",
                            "type": "learning_material",
                            "details": {
                                "content": "Temperature controls the degree of randomness in token selection. Lower temperatures are good for prompts that expect a more deterministic response, while higher temperatures can lead to more diverse or unexpected results. A temperature of 0 (greedy decoding) is deterministic: the highest probability token is always selected (though note that if two tokens have the same highest predicted probability, depending on how tiebreaking is implemented you may not always get the same output with temperature 0).\n\nTemperatures close to the max tend to create more random output. And as temperature gets higher and higher, all tokens become equally likely to be the next predicted token.\n\nThe Gemini temperature control can be understood in a similar way to the softmax function used in machine learning. A low temperature setting mirrors a low softmax temperature (T), emphasizing a single, preferred temperature with high certainty. A higher Gemini temperature setting is like a high softmax temperature, making a wider range of temperatures around the selected setting more acceptable. This increased uncertainty accommodates scenarios where a rigid, precise temperature may not be essential like for example when experimenting with creative outputs."
                            }
                        },
                        {
                            "name": "Creative Writing vs. Factual Recall",
                            "description": "Ask learners to experiment with different temperature settings for different tasks (e.g., creative story generation vs. answering a factual question).",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Imagine you're writing a short story. How would the text generated by the LLM change if you set the temperature to 0.2 versus setting it to 0.9? Explain what differences you would expect to see in the generated text, and what settings you would choose depending on the particular goal of your short story.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Short Story Analysis",
                                            "criteria": [
                                                {
                                                    "name": "Understanding",
                                                    "description": "Demonstrates a clear understanding of how temperature affects the LLM's output.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Clarity",
                                                    "description": "The response is clearly articulated and easy to understand.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Insightfulness",
                                                    "description": "Provides insightful observations about the differences in the generated text at different temperature settings.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "The goal of this question is to encourage learners to think critically about how temperature affects the output of an LLM in the context of creative writing, specifically a short story. The students should discuss how a temperature close to 0 leads to more predictable and deterministic results, while a temperature close to 1 leads to more random results that can create surprising outputs at the cost of some coherence. This question also forces students to think about how to choose a temperature given the particular goals of the text generation."
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Sampling Controls: Top-K and Top-P",
                    "description": "Understanding Top-K and Top-P and how they can be used to restrict the output.",
                    "tasks": [
                        {
                            "name": "Top-K and Top-P Explained",
                            "description": "Explain how Top-K and Top-P restrict the predicted next token to come from tokens with the top predicted probabilities.  Explain how these settings control randomness and diversity.",
                            "type": "learning_material",
                            "details": {
                                "content": "Top-K and top-P (also known as nucleus sampling) are two sampling settings used in LLMs to restrict the predicted next token to come from tokens with the top predicted probabilities. Like temperature, these sampling settings control the randomness and diversity of generated text.\n\n\u2022 Top-K sampling selects the top K most likely tokens from the model's predicted distribution. The higher top-K, the more creative and varied the model's output; the lower top-K, the more restive and factual the model's output. A top-K of 1 is equivalent to greedy decoding.\n\n\u2022 Top-P sampling selects the top tokens whose cumulative probability does not exceed a certain value (P). Values for P range from 0 (greedy decoding) to 1 (all tokens in the LLM's vocabulary).\n\nThe best way to choose between top-K and top-P is to experiment with both methods (or both together) and see which one produces the results you are looking for."
                            }
                        },
                        {
                            "name": "Experimenting with Sampling",
                            "description": "Provide scenarios and ask learners to choose appropriate temperature, Top-K, and Top-P values to achieve specific output characteristics (e.g., coherent but slightly creative, very factual and precise).",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "objective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "You want the LLM to generate highly deterministic and predictable outputs for a mathematical calculation. Which of the following sampling parameter configurations would be most suitable?\n\n a) Temperature = 0.9, Top-K = 40, Top-P = 0.99\n b) Temperature = 0.1, Top-K = 20, Top-P = 0.9\n c) Temperature = 0, Top-K = irrelevant, Top-P = irrelevant",
                                        "correct_answer": "c) Temperature = 0, Top-K = irrelevant, Top-P = irrelevant",
                                        "scorecard": null,
                                        "context": "A temperature of 0 forces the model to always pick the most probable token, leading to deterministic output. Top-K and Top-P become irrelevant when temperature is 0, as the model isn't sampling from a distribution, but simply selecting the most likely token."
                                    },
                                    {
                                        "question_type": "objective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "You are using an LLM to brainstorm creative marketing slogans. You want the model to generate diverse and unexpected suggestions. Which of the following sampling parameter configurations would be most appropriate?\n\n a) Temperature = 0.2, Top-K = 5, Top-P = 0.9\n b) Temperature = 0.7, Top-K = 50, Top-P = 0.95\n c) Temperature = 0, Top-K = 1, Top-P = 0.1",
                                        "correct_answer": "b) Temperature = 0.7, Top-K = 50, Top-P = 0.95",
                                        "scorecard": null,
                                        "context": "A higher temperature (0.7) introduces more randomness into the token selection process, while a larger Top-K (50) and Top-P (0.95) allow the model to consider a wider range of less probable tokens. This combination promotes diversity and unexpectedness in the generated slogans, aligning with the goal of creative brainstorming. "
                                    },
                                    {
                                        "question_type": "objective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "You are using an LLM to generate code and you notice that the code being generated has a lot of filler words that are not necessary. Which of the following is the most likely cause of this?\n\n a) The temperature is set too low.\n b) The top-k value is set to 1.\n c) The temperature is set too high.",
                                        "correct_answer": "c) The temperature is set too high.",
                                        "scorecard": null,
                                        "context": "A high temperature causes the model to act more randomly which results in filler words being added to the code being generated."
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Putting it all Together",
                    "description": "Combining the different configuration options for optimal results.",
                    "tasks": [
                        {
                            "name": "Combining Settings",
                            "description": "Explain how the different sampling settings interact and influence each other.  Emphasize that the optimal settings depend on the specific application and desired outcome.",
                            "type": "learning_material",
                            "details": {
                                "content": "Choosing between top-K, top-P, temperature, and the number of tokens to generate, depends on the specific application and desired outcome, and the settings all impact one another. It's also important to make sure you understand how your chosen model combines the different sampling settings together.\n\nIf temperature, top-K, and top-P are all available (as in Vertex Studio), tokens that meet both the top-K and top-P criteria are candidates for the next predicted token, and then temperature is applied to sample from the tokens that passed the top-K and top-P criteria. If only top-K or top-P is available, the behavior is the same but only the one top-K or P setting is used.\n\nIf temperature is not available, whatever tokens meet the top-K and/or top-P criteria are then randomly selected from to produce a single next predicted token.\n\nAt extreme settings of one sampling configuration value, that one sampling setting either cancels out other configuration settings or becomes irrelevant.\n\n*   If you set temperature to 0, top-K and top-P become irrelevant\u2013the most probable token becomes the next token predicted. If you set temperature extremely high (above 1-generally into the 10s), temperature becomes irrelevant and whatever tokens make it through the top-K and/or top-P criteria are then randomly sampled to choose a next predicted token.\n*   If you set top-K to 1, temperature and top-P become irrelevant. Only one token passes the top-K criteria, and that token is the next predicted token. If you set top-K extremely high, like to the size of the LLM's vocabulary, any token with a nonzero probability of being the next token will meet the top-K criteria and none are selected out.\n*   If you set top-P to 0 (or a very small value), most LLM sampling implementations will then only consider the most probable token to meet the top-P criteria, making temperature and top-K irrelevant. If you set top-P to 1, any token with a nonzero probability of being the next token will meet the top-P criteria, and none are selected out.\n\nAs a general starting point, a temperature of .2, top-P of .95, and top-K of 30 will give you relatively coherent results that can be creative but not excessively so. If you want especially creative results, try starting with a temperature of .9, top-P of .99, and top-K of 40. And if you want less creative results, try starting with a temperature of .1, top-P of .9, and top-K of 20. Finally, if your task always has a single correct answer (e.g., answering a math problem), start with a temperature of 0.\n\nNOTE: With more freedom (higher temperature, top-K, top-P, and output tokens), the LLM might generate text that is less relevant.\n\nWARNING: Have you ever seen a response ending with a large amount of filler words? This is also known as the \"repetition loop bug\", which is a common issue in Large Language Models where the model gets stuck in a cycle, repeatedly generating the same (filler) word, phrase, or sentence structure, often exacerbated by inappropriate temperature and top-k/top-p settings. This can occur at both low and high temperature settings, though for different reasons. At low temperatures, the model becomes overly deterministic, sticking rigidly to the highest probability path, which can lead to a loop if that path revisits previously generated text. Conversely, at high temperatures, the model's output becomes excessively random, increasing the probability that a randomly chosen word or phrase will, by chance, lead back to a prior state, creating a loop due to the vast number of available options. In both cases, the model's sampling process gets \"stuck,\" resulting in monotonous and unhelpful output until the output window is filled. Solving this often requires careful tinkering with temperature and top-k/top-p values to find the optimal balance between determinism and randomness."
                            }
                        },
                        {
                            "name": "Troubleshooting Configuration",
                            "description": "Present scenarios where the LLM produces undesirable output (e.g., repetitive text, nonsensical answers) and ask learners to adjust the configuration settings to improve the results.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "objective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "An LLM is generating repetitive text, often getting stuck in a loop of the same words or phrases. Which configuration setting is most likely contributing to this issue, and how should it be adjusted?\n\nA) Temperature: Lower it to reduce randomness.\nB) Top-K: Increase it to allow for more diverse token selection.\nC) Output Length: Reduce it to prevent the model from generating excessively long responses.\nD) Top-P: Decrease it to focus on the most probable tokens.",
                                        "correct_answer": "A",
                                        "scorecard": null,
                                        "context": "The 'repetition loop bug' is often exacerbated by inappropriate temperature settings. A lower temperature encourages more deterministic responses, reducing the likelihood of random loops. See page 12 for details about this issue and temperature adjustments that can help resolve it. The rest of the options will make the problem worse (B, D) or is unrelated to the problem (C)."
                                    },
                                    {
                                        "question_type": "objective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "An LLM is providing nonsensical or irrelevant answers to prompts, even when the prompts are clear and well-defined. Which combination of configuration settings is most likely causing this, and how should they be adjusted?\n\nA) Low temperature, low top-K, low top-P: Increase temperature, top-K, and top-P to allow for more diverse token selection.\nB) High temperature, high top-K, high top-P: Decrease temperature, top-K, and top-P to focus on the most probable tokens.\nC) Low temperature, high top-K, high top-P: Increase temperature and reduce top-K and top-P values.\nD) High temperature, low top-K, low top-P: Reduce temperature and increase top-K and top-P values.",
                                        "correct_answer": "D",
                                        "scorecard": null,
                                        "context": "High temperature with low top-K and low top-P can lead to excessively random and incoherent output. Reducing temperature and increasing top-K and top-P will focus the model on more probable and relevant tokens. See page 12 for guidance on suitable ranges for the temperature, top-k and top-p parameters to prevent this issue. The rest of the options will make the problem worse (A, C) or is unrelated to the problem (B)."
                                    },
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Describe a scenario where setting a very low temperature (close to 0) might be counterproductive, even if you need an accurate answer. What other configuration settings might you adjust to compensate for the low temperature?",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Prompt Engineering Skills",
                                            "criteria": [
                                                {
                                                    "name": "Understanding",
                                                    "description": "Demonstrates a clear understanding of how temperature affects LLM output.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Problem-solving",
                                                    "description": "Identifies a scenario where low temperature is not ideal and suggests appropriate adjustments.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Clarity",
                                                    "description": "Explanation is clear, concise, and easy to understand.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "A very low temperature can lead to a lack of creativity. To compensate, the student should suggest increasing top-K and top-P values to allow for more diverse token selection while still maintaining a degree of determinism. Page 9-11 has all the information about these settings."
                                    }
                                ]
                            }
                        }
                    ]
                }
            ]
        },
        {
            "name": "Basic Prompting Techniques",
            "concepts": [
                {
                    "name": "Zero-Shot Prompting",
                    "description": "Understanding how to prompt the LLM without providing any examples.",
                    "tasks": [
                        {
                            "name": "Zero-Shot Definition",
                            "description": "Explain that a zero-shot prompt only provides a description of the task and some text for the LLM to get started with, without any examples.",
                            "type": "learning_material",
                            "details": {
                                "content": "A zero-shot prompt is the simplest type of prompt. It only provides a description of a task and some text for the LLM to get started with. This input could be anything: a question, a start of a story, or instructions. The name zero-shot stands for 'no examples'."
                            }
                        },
                        {
                            "name": "Zero-Shot Scenarios",
                            "description": "Ask learners to create zero-shot prompts for different tasks (e.g., summarization, translation, question answering).",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Develop a zero-shot prompt for summarizing news articles. The prompt should take a news article as input and return a concise summary of the article's main points. Provide an example input article and the expected summary.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Summarization Quality",
                                            "criteria": [
                                                {
                                                    "name": "Brevity",
                                                    "description": "The summary is concise and avoids unnecessary details.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Accuracy",
                                                    "description": "The summary accurately reflects the main points of the article without introducing any factual errors.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Clarity",
                                                    "description": "The summary is easy to understand and uses clear language.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Relevance",
                                                    "description": "The summary focuses on the most important information and avoids including irrelevant details.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "The purpose of this question is to test the learner's ability to create a prompt that can effectively summarize text without any examples. The example input and expected summary should serve as a guide for evaluating the prompt's effectiveness. A successful prompt should generate summaries that are brief, accurate, clear, and relevant to the input article. Ensure the prompt includes clear instructions and specifies the desired output format. For example, the prompt may start with 'Summarize the following news article:'"
                                    },
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Imagine you want to translate English text to French. Formulate a zero-shot prompt that instructs the LLM to perform this translation. Include a sample English sentence and the desired French translation in your response.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Translation Quality",
                                            "criteria": [
                                                {
                                                    "name": "Accuracy",
                                                    "description": "The translation accurately conveys the meaning of the original English text.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Fluency",
                                                    "description": "The translated text is fluent and natural-sounding in French.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Grammar",
                                                    "description": "The translated text follows correct French grammar rules.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Completeness",
                                                    "description": "All the content has been translated and nothing has been missed out.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "The goal is to test the student's understanding of zero-shot prompting for translation tasks. The prompt should explicitly instruct the LLM to translate the given text from English to French. When evaluating the submission, consider whether the generated prompt clearly specifies the task and includes sufficient information to guide the LLM's response. A good prompt should enable the LLM to produce translations that are accurate, fluent, and grammatically correct. Here's a sample English text: 'Hello, how are you?' and the desired French translation: 'Bonjour, comment allez-vous?'"
                                    },
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Create a zero-shot prompt that allows an LLM to classify customer reviews into 'Positive', 'Negative', or 'Neutral'. Include a sample customer review and the expected classification in your answer.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Classification Accuracy",
                                            "criteria": [
                                                {
                                                    "name": "Relevance",
                                                    "description": "The classification is relevant to the customer review.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Correctness",
                                                    "description": "The classification accurately reflects the sentiment of the customer review.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Clarity",
                                                    "description": "The classification is clear and easy to understand.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "This question assesses the learner's ability to design a zero-shot prompt for text classification. The prompt should instruct the LLM to categorize the provided customer review into one of three categories: 'Positive', 'Negative', or 'Neutral'. A successful prompt will enable the LLM to accurately classify the sentiment expressed in the review. Consider this customer review: 'The product arrived on time and in perfect condition. I am very happy with my purchase!' The expected classification is 'Positive'."
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "One-Shot and Few-Shot Prompting",
                    "description": "Improving prompts by providing examples to the LLM.",
                    "tasks": [
                        {
                            "name": "One-Shot and Few-Shot Explained",
                            "description": "Explain the difference between one-shot (single example) and few-shot (multiple examples) prompting.  Explain that examples help the model understand the desired output structure or pattern.",
                            "type": "learning_material",
                            "details": {
                                "content": "When creating prompts for Al models, it is helpful to provide examples. These examples can help the model understand what you are asking for. Examples are especially useful when you want to steer the model to a certain output structure or pattern.\n\nA one-shot prompt, provides a single example, hence the name one-shot. The idea is the model has an example it can imitate to best complete the task.\n\nA few-shot prompt provides multiple examples to the model. This approach shows the model a pattern that it needs to follow. The idea is similar to one-shot, but multiple examples of the desired pattern increases the chance the model follows the pattern."
                            }
                        },
                        {
                            "name": "Providing Examples",
                            "description": "Provide a task and ask learners to create one-shot and few-shot prompts to guide the LLM.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Develop a one-shot prompt for an LLM that translates English to French. The prompt should include a single example demonstrating the translation.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Translation Prompt Quality",
                                            "criteria": [
                                                {
                                                    "name": "Clarity",
                                                    "description": "The prompt is clear and easy to understand.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Relevance",
                                                    "description": "The example provided is relevant to the translation task.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Completeness",
                                                    "description": "The prompt provides sufficient information for the LLM to perform the translation.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "This question evaluates the learner's ability to create a one-shot prompt. A one-shot prompt includes a single example that the LLM can use to understand the task and desired output. The example should be clear, relevant, and complete enough to guide the LLM in translating English to French. Providing good prompts is extremely crucial for the LLM to provide good responses. "
                                    },
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Formulate a few-shot prompt for an LLM that generates summaries of news articles. Include at least three examples demonstrating the summarization process. The news article should be related to current events.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Summarization Prompt Quality",
                                            "criteria": [
                                                {
                                                    "name": "Diversity",
                                                    "description": "The prompt provides diverse examples of news articles and their corresponding summaries.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Conciseness",
                                                    "description": "The summaries are concise and capture the main points of the news articles.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Accuracy",
                                                    "description": "The summaries accurately reflect the content of the news articles.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "This question tests the learner's understanding of few-shot prompting. A few-shot prompt includes multiple examples to guide the LLM. The examples should be diverse, concise, and accurate to ensure the LLM can effectively summarize news articles. Providing a few diverse examples demonstrates a deeper understanding on how the model must respond."
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "System, Contextual and Role Prompting",
                    "description": "Guiding the LLM by setting context, defining a role or providing system-level instructions.",
                    "tasks": [
                        {
                            "name": "System, Context and Roles",
                            "description": "Explain the different types of prompting, defining System Prompt, Contextual Prompt and Role Prompt. Give examples for each.",
                            "type": "learning_material",
                            "details": {
                                "content": "System, contextual and role prompting are all techniques used to guide how LLMs generate text, but they focus on different aspects:\n\n*   **System prompting** sets the overall context and purpose for the language model. It defines the 'big picture' of what the model should be doing, like translating a language, classifying a review etc.\n*   **Contextual prompting** provides specific details or background information relevant to the current conversation or task. It helps the model to understand the nuances of what's being asked and tailor the response accordingly.\n*   **Role prompting** assigns a specific character or identity for the language model to adopt. This helps the model generate responses that are consistent with the assigned role and its associated knowledge and behavior.\n\nThere can be considerable overlap between system, contextual, and role prompting. E.g. a prompt that assigns a role to the system, can also have a context.\n\nHowever, each type of prompt serves a slightly different primary purpose:\n\n*   System prompt: Defines the model's fundamental capabilities and overarching purpose.\n*   Contextual prompt: Provides immediate, task-specific information to guide the response. It's highly specific to the current task or input, which is dynamic.\n*   Role prompt: Frames the model's output style and voice. It adds a layer of specificity and personality.\n\nDistinguishing between system, contextual, and role prompts provides a framework for designing prompts with clear intent, allowing for flexible combinations and making it easier to analyze how each prompt type influences the language model's output.\n\nLet's dive into these three different kinds of prompts.\n\n**System prompting**\n\nTable 3 contains a system prompt, where I specify additional information on how to return the output. I increased the temperature to get a higher creativity level, and I specified a higher token limit. However, because of my clear instruction on how to return the output the model didn't return extra text.\n\n| Goal          | Classify movie reviews as positive, neutral or negative.                                                |\n| ------------- | -------------------------------------------------------------------------------------------------------- |\n| Model         | gemini-pro                                                                                             |\n| Temperature   | 1                                                                                                        |\n| Top-K         | 40                                                                                                       |\n| Token Limit   | 5                                                                                                        |\n| Top-P         | 0.8                                                                                                      |\n| Prompt        | Classify movie reviews as positive, neutral or negative. Only return the label in uppercase.           |\n|               | Review: \\\"Her\\\" is a disturbing study revealing the direction humanity is headed if AI is allowed to keep evolving, |\n|               | unchecked. It's so disturbing I couldn't watch it.                                                        |\n|               | Sentiment:                                                                                               |\n| Output        | NEGATIVE                                                                                                 |\n\nTable 3. An example of system prompting\n\nSystem prompts can be useful for generating output that meets specific requirements. The name 'system prompt' actually stands for 'providing an additional task to the system'. For example, you could use a system prompt to generate a code snippet that is compatible with a specific programming language, or you could use a system prompt to return a certain structure. Have a look into Table 4, where I return the output in JSON format.\n\n| Goal          | Classify movie reviews as positive, neutral or negative, return JSON.                                        |\n| ------------- | -------------------------------------------------------------------------------------------------------- |\n| Model         | gemini-pro                                                                                             |\n| Temperature   | 1                                                                                                        |\n| Top-K         | 40                                                                                                       |\n| Token Limit   | 1024                                                                                                     |\n| Top-P         | 0.8                                                                                                      |\n| Prompt        | Classify movie reviews as positive, neutral or negative. Return valid JSON:                                |\n|               | Review: \\\"Her\\\" is a disturbing study revealing the direction humanity is headed if AI is allowed to keep evolving, |\n|               | unchecked. It's so disturbing I couldn't watch it.                                                        |\n|               | Schema:                                                                                                  |\n|               | MOVIE:                                                                                                   |\n|               | {                                                                                                        |\n|               | \\\"sentiment\\\": String \\\"POSITIVE\\\" | \\\"NEGATIVE\\\" | \\\"NEUTRAL\\\",                                                |\n|               | \\\"name\\\": String                                                                                             |\n|               | }                                                                                                        |\n|               | MOVIE REVIEWS:                                                                                           |\n|               | {                                                                                                        |\n|               | \\\"movie\\_reviews\\\": [MOVIE]                                                                              |\n|               | }                                                                                                        |\n|               | JSON Response:                                                                                           |\n| Output        | {                                                                                                        |\n|               | \\\"movie\\_reviews\\\": [                                                                                      |\n|               | {                                                                                                      |\n|               | \\\"sentiment\\\": \\\"NEGATIVE\\\",                                                                              |\n|               | \\\"name\\\": \\\"Her\\\"                                                                                        |\n|               | }                                                                                                        |\n|               | ]                                                                                                        |\n|               | }                                                                                                        |\n\nTable 4. An example of system prompting with JSON format\n\nThere are some benefits in returning JSON objects from a prompt that extracts data. In a real-world application I don't need to manually create this JSON format, I can already return the data in a sorted order (very handy when working with datetime objects), but most importantly, by prompting for a JSON format it forces the model to create a structure and limit hallucinations.\n\nSystem prompts can also be really useful for safety and toxicity. To control the output, simply add an additional line to your prompt like: \u2018You should be respectful in your answer.'\n\n**Role prompting**\n\nRole prompting is a technique in prompt engineering that involves assigning a specific role to the gen Al model. This can help the model to generate more relevant and informative output, as the model can craft its responses to the specific role that it has been assigned.\n\nFor example, you could role prompt a gen Al model to be a book editor, a kindergarten teacher, or a motivational speaker. Once the model has been assigned a role, you can then give it prompts that are specific to that role. For example, you could prompt a teacher to create a lesson plan that they can then review. Have a look into Table 5, where the model takes the role of a travel guide.\n\n| Goal          | Act as travel guide and provide 3 travel suggestions                                                        |\n| ------------- | -------------------------------------------------------------------------------------------------------- |\n| Model         | gemini-pro                                                                                             |\n| Temperature   | 1                                                                                                        |\n| Top-K         | 40                                                                                                       |\n| Token Limit   | 1024                                                                                                     |\n| Top-P         | 0.8                                                                                                      |\n| Prompt        | I want you to act as a travel guide. I will write to you about my location and you will suggest 3 places to visit near |\n|               | me. In some cases, I will also give you the type of places I will visit.                                   |\n|               | My suggestion: \\\"I am in Amsterdam and I want to visit only museums.\\\"                                  |\n|               | Travel Suggestions:                                                                                       |\n| Output        | 1.  Rijksmuseum: World-renowned museum, known for Dutch masterpieces like Rembrandt's \\\"The Night Watch.\\\" Don't miss the incredible collection of paintings and artifacts showcasing Dutch art and history. |\n|               | 2.  Van Gogh Museum: Immerse yourself in the captivating works of Vincent van Gogh. The museum houses the largest collection of his paintings and drawings, including \\\"The Starry Night\\\" and \\\"Sunflowers.\\\" |\n|               | 3.  Stedelijk Museum Amsterdam: Discover modern and contemporary art from around the world. The museum's collection includes works by Picasso, Kandinsky, and Rietveld, housed in a striking modern building. |\n\nTable 5. An example of role prompting\n\nThe above example shows an example of taking the role of a travel agent. When you would change the role to a teacher of geography, you would notice that you will receive a different response.\n\nDefining a role perspective for an Al model gives it a blueprint of the tone, style, and focused expertise you're looking for to improve the quality, relevance, and effectiveness of your output.\n\nHere are some styles you can choose from which I find effective:\n\nConfrontational, Descriptive, Direct, Formal, Humorous, Influential, Informal, Inspirational, Persuasive\n\nLet's change our prompt in Table 6 to include a humorous and inspirational style.\n\n| Prompt        | I want you to act as a travel guide. I will write to you about my location and you will suggest 3 places to visit near me in a humorous style. |\n|               | My suggestion: \\\"I am in Manhattan.\\\"                                                                    |\n|               | Travel Suggestions:                                                                                       |\n| Output        | 1.  Behold the Empire State of Mind: Ascend to the dizzying heights of the Empire State Building and bask in the glory of Manhattan's skyline. Prepare to feel like King Kong atop the Big Apple, minus the giant ape-sized banana. |\n|               | 2.  Get Artsy-Fartsy at MoMA: Unleash your inner art aficionado at the Museum of Modern Art (MoMA). Gaze upon masterpieces that will boggle your mind and make you question whether your stick-figure drawings have any artistic merit. |\n|               | 3.  Shop 'Til You Drop on Fifth Avenue: Indulge in some retail therapy on the iconic Fifth Avenue. Brace yourself for sticker shock as you window-shop (or actually shop) at designer boutiques that will make your wallet cry. But hey, you're in Manhattan, so you might as well embrace the fabulousness! |\n\nTable 6. An example of role prompting with a humorous tone and style\n\n**Contextual prompting**\n\nBy providing contextual prompts, you can help ensure that your Al interactions are as seamless and efficient as possible. The model will be able to more quickly understand your request and be able to generate more accurate and relevant responses, as you can see in the example of Table 7.\n\n| Goal          | Suggest articles for a blog about retro games                                                                 |\n| ------------- | -------------------------------------------------------------------------------------------------------- |\n| Model         | gemini-pro                                                                                             |\n| Temperature   | 1                                                                                                        |\n| Top-K         | 40                                                                                                       |\n| Token Limit   | 1024                                                                                                     |\n| Top-P         | 0.8                                                                                                      |\n| Prompt        | Context: You are writing for a blog about retro 80's arcade video games.                                     |\n|               | Suggest 3 topics to write an article about with a few lines of description of what this article should contain. |\n| Output        | 1.  **The Evolution of Arcade Cabinet Design:** This article would explore the evolution of arcade cabinet designs, from the early wood and metal cabinets of the 1970s to the sleek, neon-lit designs of the 1980s and beyond. |\n|               | 2.  **Blast From The Past: Iconic Arcade Games of The 80's:** This article would feature a glimpse into the history, impact, and legacy of some of the most iconic and influential arcade games of the 1980s, highlighting their innovations, popular mechanics, and enduring charm. |\n|               | 3.  **The Rise and Retro Revival of Pixel Art:** This article would delve into the evolution of pixel art as a defining visual style of early arcade games, tracing its roots, exploring its techniques, and discussing the resurgence of pixel art in modern games and digital art. |\n\nTable 7. An example of contextual prompting\n\n"
                            }
                        },
                        {
                            "name": "Combine the Types",
                            "description": "Ask learners to rewrite some prompts using these techniques for improved results.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Take the following prompt: 'Write a short poem about a cat'. Now, improve this prompt by combining system, contextual, and role prompting techniques. Explain what each technique adds to the prompt and how it improves the potential output.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Prompt Improvement Assessment",
                                            "criteria": [
                                                {
                                                    "name": "System",
                                                    "description": "Defines the overarching goal clearly. (e.g., 'You are a professional poet.')",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Context",
                                                    "description": "Provides specific details to guide the poem. (e.g., 'The cat is sitting by a warm fireplace on a cold winter night.')",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Role",
                                                    "description": "Assigns a persona or style. (e.g., 'Write in the style of Edgar Allan Poe.')",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Explanation",
                                                    "description": "Clearly explains the contribution of each technique and how it enhances the prompt's potential output.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Creativity",
                                                    "description": "The improved prompt demonstrates an improvement in eliciting a good response.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "This question requires the learner to synthesize system, contextual, and role prompting. A strong answer will demonstrate a clear understanding of each technique and how they can be combined to create more effective prompts.  For example, a possible improved prompt might be: 'As a nature poet, write a short poem about a cat sleeping in a sunbeam, highlighting its tranquility.'  Here, 'nature poet' is the role, 'sleeping in a sunbeam' is the context, and 'poem' is the type/system. The explanation should clearly articulate how each element contributes to a more focused and potentially higher-quality output."
                                    },
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "You are a marketing manager tasked with creating an engaging social media post. Your initial prompt is: 'Write a social media post'. Rewrite this prompt using a combination of system, contextual, and role prompting to make it more effective. Justify your choices. What additional information or clarification might you require from a client to write an even better prompt?",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Social Media Prompt Enhancement",
                                            "criteria": [
                                                {
                                                    "name": "System",
                                                    "description": "Specifies the social media platform. (e.g., 'Write a tweet for Twitter')",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Context",
                                                    "description": "Provides relevant background. (e.g., 'promoting our new line of eco-friendly products')",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Role",
                                                    "description": "Adopts a brand voice/persona. (e.g., 'in a witty and informative style')",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Justification",
                                                    "description": "Provides a clear rationale for prompt enhancements.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Clarification",
                                                    "description": "Identifies relevant missing information that is required to get the best outcome.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "This question assesses the learner's ability to apply system, contextual, and role prompting in a practical marketing scenario. A strong answer will include a rewritten prompt that incorporates all three techniques and a justification for why those techniques were chosen. The additional clarification section should include things such as: target audience, brand voice or specific marketing goals."
                                    },
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Original Prompt: 'Explain the theory of relativity'. Rewrite it using System, Contextual and Role Prompting. Which of these three approaches do you think is most important for this task and why?",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Theory of Relativity Prompt Engineering",
                                            "criteria": [
                                                {
                                                    "name": "System",
                                                    "description": "Clear indication of what it is, e.g. 'You are writing a physics textbook'.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Context",
                                                    "description": "Adds information relevant to this specific task, e.g. 'focus on the key concepts and avoid complex equations'.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Role",
                                                    "description": "The persona it adopts, e.g. 'like a renowned physics professor'.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Importance",
                                                    "description": "Clear rationale of why it thinks a specific approach is most important.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "This question tests the understanding of different prompting techniques and their applications. Key points to look for are the appropriate application of system, contextual, and role prompting. A strong response for this question would be: System prompt: 'You are writing a physics textbook'. Context: 'Focus on the key concepts and avoid complex equations'. Role: 'Explain the theory of relativity like a renowned physics professor'. The rationale for the approach is that having a renowned physics professor (Role) is most important since it can explain this complex subject in simpler terms for a textbook (System) while ensuring it avoids using equations (Context)."
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Step-Back Prompting",
                    "description": "Improving the performance by asking a general question before a specific task.",
                    "tasks": [
                        {
                            "name": "Step-Back prompting",
                            "description": "Explain what Step-Back prompting is and when it can be useful. Provide an example.",
                            "type": "learning_material",
                            "details": {
                                "content": "Step-back\u00ae prompting is a technique for improving the performance by prompting the LLM to first consider a general question related to the specific task at hand, and then feeding the answer to that general question into a subsequent prompt for the specific task. This \u2018step back' allows the LLM to activate relevant background knowledge and reasoning processes before attempting to solve the specific problem.\n\nBy considering the broader and underlying principles, LLMs can generate more accurate and insightful responses. Step-back prompting encourages LLMs to think critically and apply their knowledge in new and creative ways. It changes the final prompt doing the task by utilizing more knowledge in the LLM's parameters than would otherwise come into play when the LLM is prompted directly.\n\nIt can help to mitigate biases in LLM responses, by focusing on general principles instead of specific details, step-back prompting.\n\nLet's have a look into these examples to understand how step-back prompting can improve the results. Let's first review a traditional prompt (Table 8) before we compare it to a step back prompt (Table 9):\n\n| Goal | Write a storyline for a level of a first-person shooter video game. |\n| ----------- | ----------- |\n| Model | gemini-pro |\n| Temperature | 1 |\n| Top-K | 40 |\n| Prompt | Write a one paragraph storyline for a new level of a first-person shooter video game that is challenging and engaging. |\n| Output | The level begins with the player's squad being ambushed in a dense urban area by a relentless enemy faction. Players must fight their way through a maze of back alleys and dilapidated buildings, utilizing stealth and precision to eliminate enemy forces and avoid detection. The player must use their environment to their advantage, setting traps, utilizing cover, and making calculated moves to survive. Along the way, they uncover intel suggesting the enemy is planning a devastating attack, and the player must race against time to gather evidence and relay it back to their headquarters before the enemy can carry out their plans. |\nTable 8. A traditional prompt before we compare it with a step back prompt\n\nWhen you set the temperature to 1, you might end up with all kinds of creative writings for a storyline, but it's also quite random and generic. So let's take a step back:\n\n| Prompt | Based on popular first-person shooter action games, what are 5 fictional key settings that contribute to a challenging and engaging level storyline in a first-person shooter video game? |\n| ----------- | ----------- |\n| Output | 1. **Abandoned Military Base**: A sprawling, post-apocalyptic military complex crawling with mutated soldiers and rogue robots, ideal for challenging firearm combat.\\\n2. **Cyberpunk City**: A neon-lit, futuristic urban environment with towering skyscrapers and dense alleyways, featuring cybernetically enhanced enemies and hacking mechanics.\\\n3. **Alien Spaceship**: A vast alien vessel stranded on Earth, with eerie corridors, zero-gravity sections, and extraterrestrial creatures to encounter.\\\n4. **Zombie-Infested Town**: A desolate town overrun by hordes of aggressive zombies, featuring intense close-quarters combat and puzzle-solving to find safe passage.\\\n5. **Underwater Research Facility**: A deep-sea laboratory flooded with water, filled with mutated aquatic creatures, and requiring stealth and underwater exploration skills to survive. |\nTable 9. An example of prompting for self consistency\n\nYeah those topics seem like a good fit for a first-person video game. Let's go back to the original prompt, but this time we include the answer of the step back as context and see what it will return.\n\n| Prompt | Context: 5 engaging themes for a first person shooter video game:\\\n1. **Abandoned Military Base**: A sprawling, post-apocalyptic military complex crawling with mutated soldiers and rogue robots, ideal for challenging firearm combat.\\\n2. **Cyberpunk City**: A neon-lit, futuristic urban environment with towering skyscrapers and dense alleyways, featuring cybernetically enhanced enemies and hacking mechanics.\\\n3. **Alien Spaceship**: A vast alien vessel stranded on Earth, with eerie corridors, zero-gravity sections, and extraterrestrial creatures to encounter.\\\n4. **Zombie-Infested Town**: A desolate town overrun by hordes of aggressive zombies, featuring intense close-quarters combat and puzzle-solving to find safe passage.\\\n5. **Underwater Research Facility**: A deep-sea laboratory flooded with water, filled with mutated aquatic creatures, and requiring stealth and underwater exploration skills to survive.\\\nTake one of the themes and write a one paragraph storyline for a new level of a first-person shooter video game that is challenging and engaging. |\n| ----------- | ----------- |\n| Output | In the heart of a murky abyss, lies a dilapidated underwater research facility, standing as a testament to human ambition and its disastrous consequences. Shrouded in darkness, pulsating with the hum of malfunctioning machinery, and haunted by the echoes of drowned souls, the facility is now a breeding ground for a nightmare borne from the deep sea. The player, an elite marine equipped with advanced diving gear and experimental weaponry, is tasked with venturing into this aquatic realm of terror. Armed with resilience and resourcefulness, they must navigate treacherous corridors, solve cryptic puzzles, and confront gruesome sea monstrosities that lurk in the depths. From grappling with the crushing pressure of the deep sea to outsmarting cunning aquatic predators, every moment in this uncharted underworld tests the limits of human endurance and courage. |\nTable 10. An example of prompting for self consistency\n\nThat looks like an interesting video game! By using step back prompting techniques you can increase the accuracy of your prompts."
                            }
                        },
                        {
                            "name": "Step-Back Prompting challenge",
                            "description": "Ask learners to rewrite a prompt using this technique to improve the quality of the answer.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Original Prompt: Write a tagline for a new brand of coffee.\n\nRewrite the above prompt using the 'step-back' prompting technique. Explain why you made the changes you did and how you think it will improve the response from the LLM.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Prompt Improvement",
                                            "criteria": [
                                                {
                                                    "name": "Clarity",
                                                    "description": "The prompt is clearly worded and easy to understand.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Relevance",
                                                    "description": "The rewritten prompt should incorporate a general question related to taglines before asking for a specific tagline for a new brand of coffee.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Explanation Quality",
                                                    "description": "The explanation of why the changes were made should clearly articulate how the step-back technique is being applied and why it is expected to improve the LLM's response.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "The rewritten prompt should include the general question that enables the LLM to activate relevant background knowledge before attempting to solve the specific problem of writing a tagline for a new brand of coffee. For example, a good general question would be - What makes a tagline effective?"
                                    }
                                ]
                            }
                        }
                    ]
                }
            ]
        },
        {
            "name": "Advanced Prompting Techniques",
            "concepts": [
                {
                    "name": "Chain of Thought (CoT) Prompting",
                    "description": "Improve the reasoning capabilities of LLMs by generating intermediate reasoning steps.",
                    "tasks": [
                        {
                            "name": "CoT Prompting explained",
                            "description": "Explain that CoT prompting encourages the LLM to explain its reasoning process step by step, leading to more accurate answers. Explain why is this useful.",
                            "type": "learning_material",
                            "details": {
                                "content": "Chain of Thought (CoT) prompting is a technique that enhances the reasoning capabilities of Large Language Models (LLMs) by encouraging them to generate intermediate reasoning steps. Instead of directly providing an answer, the LLM is prompted to explain its reasoning process step by step. This helps the LLM to break down complex problems into smaller, more manageable parts, leading to more accurate and reliable answers. It is useful because it improves the interpretability of LLM's responses, as you can see the reasoning steps that were followed and identify any potential malfunctions."
                            }
                        },
                        {
                            "name": "CoT Examples",
                            "description": "Provide examples of CoT prompts and ask learners to create their own for complex tasks that require reasoning.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "objective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Consider the following problem: 'A farmer has 15 sheep. All but 8 died. How many sheep does the farmer have left?' Which of the following prompts is an example of Chain of Thought (CoT) prompting?",
                                        "correct_answer": "A farmer has 15 sheep. All but 8 died. Let's think step by step. If all but 8 died, that means 8 are still alive. So the answer is 8.",
                                        "scorecard": null,
                                        "context": "The correct answer is 'A farmer has 15 sheep. All but 8 died. Let's think step by step. If all but 8 died, that means 8 are still alive. So the answer is 8.' because it includes the intermediate reasoning steps to arrive at the solution. This showcases the core element of Chain of Thought prompting. Here are the other options:\n\n*   'A farmer has 15 sheep. All but 8 died. What number of sheep does the farmer have?'\n*   'Solve: A farmer has 15 sheep. All but 8 died. State the final answer only.'\n*   'A farmer has 15 sheep. All but 8 died. The answer is 8.'"
                                    },
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Provide an example of a Chain of Thought prompt for summarizing a complex scientific research paper.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "CoT Prompt Quality",
                                            "criteria": [
                                                {
                                                    "name": "Reasoning Steps",
                                                    "description": "The prompt clearly instructs the model to break down the task into intermediate, logical steps.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Clarity",
                                                    "description": "The prompt is easily understandable and provides clear instructions to the model.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Relevance",
                                                    "description": "The reasoning steps are relevant to summarization task.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "The response should provide a clear set of instructions that guide the LLM to extract key information, identify the main argument, and generate a concise summary. Focus on how the LLM is guided to go from raw data to the final summary. A good response would be: Summarize this research paper. First, identify the primary research question. Then, outline the methodologies used and list the key findings of each methodology. Finally, synthesize these findings into a three-sentence summary that includes the main conclusion of the paper. Here is the paper: [insert paper here]"
                                    },
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Imagine you are trying to debug a piece of code with an LLM. Provide an example of a Chain of Thought prompt you would use to help the LLM identify and fix errors.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Debugging Prompt Quality",
                                            "criteria": [
                                                {
                                                    "name": "Step-by-Step",
                                                    "description": "The prompt instructs the LLM to go through the code step by step to identify potential bugs.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Specificity",
                                                    "description": "The prompt includes specific instructions to check for common coding errors.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Clarity",
                                                    "description": "The prompt is clear and easy to understand.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "The response should showcase how the prompt breaks down the debugging process into smaller, manageable steps for the LLM. A good response would look like: Analyze the code line by line. First, examine the variable initializations. Second, confirm that the loop conditions are correct. Third, ensure that the function calls are appropriate. Fourth, check for any syntax errors. Finally, provide a revised version of the code with the errors fixed. Here is the code: [insert code here]"
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Self-Consistency",
                    "description": "Generating diverse reasoning paths and selecting the most consistent answer.",
                    "tasks": [
                        {
                            "name": "Self-Consistency defined",
                            "description": "Explain Self-Consistency, how it combines sampling and majority voting to generate diverse reasoning paths and select the most consistent answer, and how it can be applied. Explain that this approach give more accurate and coherent results.",
                            "type": "learning_material",
                            "details": {
                                "content": "While large language models have shown impressive success in various NLP tasks, their ability to reason is often seen as a limitation that cannot be overcome solely by increasing model size. As we learned in the previous Chain of Thought prompting section, the model can be prompted to generate reasoning steps like a human solving a problem. However CoT uses a simple 'greedy decoding' strategy, limiting its effectiveness. Self-consistency combines sampling and majority voting to generate diverse reasoning paths and select the most consistent answer. It improves the accuracy and coherence of responses generated by LLMs.\n\nSelf-consistency gives a pseudo-probability likelihood of an answer being correct, but obviously has high costs."
                            }
                        },
                        {
                            "name": "Self-Consistency Implementations",
                            "description": "Ask learners to describe some scenarios where this technique can be useful. Ask them to extend the prompt to implement self-consistency. Explain the potential cost/benefit tradeoffs.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Describe a scenario where the self-consistency technique would be particularly beneficial in improving the reliability of a language model's output.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Application of Self-Consistency",
                                            "criteria": [
                                                {
                                                    "name": "Relevance",
                                                    "description": "The scenario provided should be relevant to the application of self-consistency in language models.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Clarity",
                                                    "description": "The explanation of the scenario should be clear and easy to understand.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Justification",
                                                    "description": "The response should justify why self-consistency is beneficial in the given scenario.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "This question aims to test the learner's understanding of the practical applications of the self-consistency technique. The student should clearly articulate the benefits of self-consistency in scenarios where reliability and accuracy are paramount. For example, tasks requiring factual accuracy or logical consistency will benefit from self-consistency"
                                    },
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Extend the prompt below to implement the self-consistency technique. Describe the additional configurations needed and explain the expected behavior.\n\n`Translate the following sentence to French: 'The quick brown fox jumps over the lazy dog.'`",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Implementation of Self-Consistency",
                                            "criteria": [
                                                {
                                                    "name": "Correctness",
                                                    "description": "The prompt extension should correctly implement the self-consistency technique.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Completeness",
                                                    "description": "The response should include all necessary configurations and explanations.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Clarity",
                                                    "description": "The explanation of the expected behavior should be clear and easy to understand.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "The student should clearly articulate the additional configurations needed to implement self-consistency (e.g., sampling multiple responses with a high temperature and then selecting the most common translation)."
                                    },
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Explain the potential cost and benefits of using self-consistency in a real-time application such as a customer service chatbot.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Cost/Benefit Analysis",
                                            "criteria": [
                                                {
                                                    "name": "Accuracy",
                                                    "description": "The response should accurately assess the costs and benefits of using self-consistency.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Completeness",
                                                    "description": "The response should cover all relevant aspects of the cost/benefit analysis.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Context",
                                                    "description": "The response should properly contextualize the costs and benefits within the context of a real-time application.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "The student should demonstrate an understanding of the tradeoffs involved in using self-consistency, considering factors such as increased computational cost and latency versus improved accuracy and reliability."
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Tree of Thoughts (ToT)",
                    "description": "Exploring multiple different reasoning paths simultaneously.",
                    "tasks": [
                        {
                            "name": "ToT Prompting",
                            "description": "Explain that ToT prompting generalizes CoT prompting by allowing LLMs to explore multiple reasoning paths simultaneously, rather than just following a single linear chain of thought.",
                            "type": "learning_material",
                            "details": {
                                "content": "Now that we are familiar with chain of thought and self-consistency prompting, let's review Tree of Thoughts (ToT). It generalizes the concept of CoT prompting because it allows LLMs to explore multiple different reasoning paths simultaneously, rather than just following a single linear chain of thought. This is depicted in Figure 1."
                            }
                        },
                        {
                            "name": "Reasoning comparison",
                            "description": "Ask to compare ToT prompting and CoT prompting and explain where ToT prompting can provide additional value compared to CoT prompting.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Compare and contrast Tree of Thoughts (ToT) prompting with Chain of Thought (CoT) prompting. In what scenarios does ToT provide additional value compared to CoT?",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Comparison of ToT and CoT Prompting",
                                            "criteria": [
                                                {
                                                    "name": "Understanding of CoT",
                                                    "description": "Demonstrates an accurate understanding of Chain of Thought prompting, including its methodology and limitations.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Understanding of ToT",
                                                    "description": "Demonstrates an accurate understanding of Tree of Thoughts prompting, including its methodology and capabilities for exploring multiple reasoning paths.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Comparison Accuracy",
                                                    "description": "Accurately compares and contrasts the two methods, highlighting the key differences in their approach to problem-solving.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Value Identification",
                                                    "description": "Clearly identifies scenarios where Tree of Thoughts prompting provides additional value over Chain of Thought, specifically noting the advantage in complex tasks requiring exploration of multiple reasoning paths.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Clarity",
                                                    "description": "The response is well-organized and easy to understand with a clear and concise expression of the comparison and value proposition.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "This question assesses the learner's understanding of Tree of Thoughts (ToT) and Chain of Thought (CoT) prompting, as well as their ability to compare and contrast these two techniques. The reference material explains that ToT generalizes the concept of CoT prompting because it allows LLMs to explore multiple different reasoning paths simultaneously, rather than just following a single linear chain of thought."
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "ReAct (Reason & Act)",
                    "description": "Reasoning combined with external tools.",
                    "tasks": [
                        {
                            "name": "ReAct Prompting",
                            "description": "Explain ReAct prompting. Detail that ReAct enables LLMs to solve complex tasks using natural language reasoning combined with external tools (search, code interpreter etc.).",
                            "type": "learning_material",
                            "details": {
                                "content": "Reason and act (ReAct) [10]\u00b9\u00b3 prompting is a paradigm for enabling LLMs to solve complex tasks using natural language reasoning combined with external tools (search, code interpreter etc.) allowing the LLM to perform certain actions, such as interacting with external APIs to retrieve information which is a first step towards agent modeling.\n\nReAct mimics how humans operate in the real world, as we reason verbally and can take actions to gain information. ReAct performs well against other prompt engineering approaches in a variety of domains.\n\nReAct prompting works by combining reasoning and acting into a thought-action loop. The LLM first reasons about the problem and generates a plan of action. It then performs the actions in the plan and observes the results. The LLM then uses the observations to update its reasoning and generate a new plan of action. This process continues until the LLM reaches a solution to the problem."
                            }
                        },
                        {
                            "name": "Advantages of ReAct Prompting",
                            "description": "Detail the advantages of using this technique and ask where it can be applied.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "objective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "ReAct prompting combines reasoning and acting in a thought-action loop. Which of the following statements best describes the advantages of this approach?",
                                        "correct_answer": "Enables LLMs to solve complex tasks using natural language reasoning combined with external tools.",
                                        "scorecard": null,
                                        "context": "ReAct enables LLMs to solve complex tasks using natural language reasoning combined with external tools (search, code interpreter etc.) allowing the LLM to perform certain actions, such as interacting with external APIs to retrieve information which is a first step towards agent modeling."
                                    },
                                    {
                                        "question_type": "objective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "In which of the following scenarios is ReAct prompting most applicable?",
                                        "correct_answer": "Tasks requiring natural language reasoning and external tools.",
                                        "scorecard": null,
                                        "context": "ReAct prompting is a paradigm for enabling LLMs to solve complex tasks using natural language reasoning combined with external tools (search, code interpreter etc.) allowing the LLM to perform certain actions, such as interacting with external APIs to retrieve information which is a first step towards agent modeling."
                                    },
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Describe a real-world application where ReAct prompting could be beneficial. Explain why ReAct is suitable for this application. Do not include the example use-case shown in the reference material about determining how many children the band members of Metallica have.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Application of ReAct prompting",
                                            "criteria": [
                                                {
                                                    "name": "Relevance",
                                                    "description": "The application is relevant to ReAct prompting and requires both reasoning and external tools.",
                                                    "min_score": 0,
                                                    "max_score": 3
                                                },
                                                {
                                                    "name": "Explanation",
                                                    "description": "The explanation clearly articulates why ReAct is a suitable approach for the chosen application.",
                                                    "min_score": 0,
                                                    "max_score": 3
                                                },
                                                {
                                                    "name": "Originality",
                                                    "description": "The application does not use the example provided in the reference material.",
                                                    "min_score": 0,
                                                    "max_score": 4
                                                }
                                            ]
                                        },
                                        "context": ""
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Automatic Prompt Engineering (APE)",
                    "description": "Use a model to generate prompts, evaluate them and repeat the process.",
                    "tasks": [
                        {
                            "name": "APE Prompting",
                            "description": "Explain APE prompting. Detail that this alleviates the need for human input and enhances the model's performance in various tasks.",
                            "type": "learning_material",
                            "details": {
                                "content": "At this point you might realize that writing a prompt can be complex. Wouldn't it be nice to automate this (write a prompt to write prompts)? Well, there's a method: Automatic Prompt Engineering (APE). This method not only alleviates the need for human input but also enhances the model's performance in various tasks."
                            }
                        },
                        {
                            "name": "APE Prompting usage",
                            "description": "Ask in what scenarios is APE prompting useful. Ask if the process should be completely automated or if there's value for human intervention in the process.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "objective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "In which of the following scenarios is Automatic Prompt Engineering (APE) particularly useful?\n\n a) When you need to quickly generate a large number of diverse prompts for training a chatbot.\n b) When you want to eliminate the need for any human input in the prompt engineering process.\n c) When you aim to reduce the model's performance in various tasks.\n d) When you want to avoid evaluating and altering the generated prompts.",
                                        "correct_answer": "a",
                                        "scorecard": null,
                                        "context": "APE is useful for generating a large number of diverse prompts to train a chatbot. APE alleviates the need for human input and enhances the model's performance in various tasks. The process involves evaluating and potentially altering the generated prompts and repeating the process for best results"
                                    },
                                    {
                                        "question_type": "objective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "In the Automatic Prompt Engineering (APE) process, is human intervention necessary, or should the process be completely automated?\n\n a) The process should be entirely automated to avoid human bias.\n b) Human intervention has no value and should be avoided.\n c) Human intervention may be valuable for evaluating and refining the generated prompts.\n d) Automation is the only viable option for APE.",
                                        "correct_answer": "c",
                                        "scorecard": null,
                                        "context": "While APE automates prompt generation, human intervention is valuable for evaluating the generated prompts. A human is required to assess the generated prompts to check if they are effective and can be altered to become more useful. This ultimately enhances the performance of the model being used."
                                    }
                                ]
                            }
                        }
                    ]
                }
            ]
        },
        {
            "name": "Code Prompting",
            "concepts": [
                {
                    "name": "Prompts for Writing Code",
                    "description": "Using prompts to generate code snippets in different programming languages.",
                    "tasks": [
                        {
                            "name": "Code Generation",
                            "description": "Explain how to use prompts to generate code in a specific programming language, asking for a particular functionality or task. Explain how this process works.",
                            "type": "learning_material",
                            "details": {
                                "content": "Gemini can also be a developer and help you with writing code in any programming language of choice. As a developer this can help you speed up the process of writing code.\n\nImagine a folder on your machine with hundreds of files that needs to be renamed. Renaming each file would take you a lot of time. You know a little Bash, and could write a script to automate this, but that may also take awhile. So let's write a prompt. You can write the prompt in the public consumer chatbot Gemini, or if you are more concerned about confidentiality, you can write these prompts within your Google Cloud account and open Vertex Al Studio. The advantage of Vertex Al Studio is that you can configure the temperature etc."
                            }
                        },
                        {
                            "name": "Code Prompting Examples",
                            "description": "Ask learners to come up with some common scenarios where prompting to produce code could be useful, and to write the appropriate prompts to do that.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Imagine you're working on a Python project that requires you to automate the process of sending personalized email notifications to users. You need to generate a Python script that reads user data from a CSV file, composes an email message for each user, and sends the emails using an SMTP server. \n\nWrite a prompt that instructs an LLM to generate this Python script.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Code Generation Prompt",
                                            "criteria": [
                                                {
                                                    "name": "Clarity",
                                                    "description": "The prompt clearly specifies the task of generating a Python script for sending personalized email notifications.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Completeness",
                                                    "description": "The prompt includes all necessary details such as reading user data from a CSV file, composing email messages, and sending emails using an SMTP server.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Specificity",
                                                    "description": "The prompt is specific enough to guide the LLM in generating the desired code, mentioning key components and steps involved.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "This question tests the learner's ability to create a prompt for generating a Python script that automates email notifications, including reading data from a CSV file, composing email messages, and sending emails via SMTP. The prompt should provide enough detail for the LLM to generate a functional script."
                                    },
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "You are building a web application using React and need to implement a feature that allows users to upload an image, preview it, and then send it to a server for processing. You want to generate the React component code using an LLM. \n\nCraft a prompt that guides the LLM to generate a React component with the following functionalities:\n\n1.  An input element that allows users to select an image file from their local machine.\n2.  A preview area that displays the selected image.\n3.  A button that triggers the upload of the image to a specified server endpoint.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "React Component Generation Prompt",
                                            "criteria": [
                                                {
                                                    "name": "Clarity",
                                                    "description": "The prompt clearly outlines the task of generating a React component for image uploading and previewing.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Completeness",
                                                    "description": "The prompt includes all required functionalities such as image selection, preview, and uploading to a server endpoint.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Specificity",
                                                    "description": "The prompt is specific enough to guide the LLM in generating the desired React component, mentioning key elements and steps involved in the process.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "This question assesses the learner's ability to create a prompt for generating a React component that handles image uploading, previewing, and sending to a server, ensuring all necessary functionalities are included."
                                    },
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Consider a scenario where you need to automate the deployment of a Node.js application to a cloud server. You decide to use Bash scripting for this purpose. You want to generate a Bash script that performs the following actions:\n\n1.  Navigates to the application directory.\n2.  Pulls the latest code from a Git repository.\n3.  Installs the dependencies using `npm install`.\n4.  Restarts the Node.js application using `pm2 restart app`.\n\nWrite a prompt that instructs an LLM to generate this Bash script.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Bash Script Generation Prompt",
                                            "criteria": [
                                                {
                                                    "name": "Clarity",
                                                    "description": "The prompt clearly defines the task of generating a Bash script for deploying a Node.js application.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Completeness",
                                                    "description": "The prompt includes all necessary steps such as navigating to the application directory, pulling code from Git, installing dependencies, and restarting the application.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Specificity",
                                                    "description": "The prompt is specific enough to guide the LLM in generating the desired Bash script, mentioning key commands and actions involved.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "This question evaluates the learner's ability to create a prompt for generating a Bash script that automates the deployment of a Node.js application, ensuring all essential deployment steps are included."
                                    },
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "You need to create a SQL query to retrieve all customers from a database who have placed orders in the last month and sort them by their total spending. Create a prompt that requests an LLM to generate the appropriate SQL query for this task. Consider what details might be relevant to include in the prompt for it to be effective.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "SQL Query Generation Prompt",
                                            "criteria": [
                                                {
                                                    "name": "Clarity",
                                                    "description": "The prompt clearly states the need for an SQL query to retrieve and sort customer data.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Completeness",
                                                    "description": "The prompt includes requirements for filtering customers by orders placed in the last month and sorting them by total spending.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Specificity",
                                                    "description": "The prompt provides enough detail to guide the LLM in generating the correct SQL query, mentioning filtering and sorting criteria.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "This question tests the learner's ability to create a prompt for generating an SQL query that retrieves and sorts customer data based on specific criteria, ensuring the prompt is detailed enough for the LLM to produce the desired query."
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Prompts for Explaining Code",
                    "description": "Using prompts to understand and explain existing code.",
                    "tasks": [
                        {
                            "name": "Code Explanation",
                            "description": "Explain how prompts can be used to ask the LLM to explain what a piece of code does, providing a human-readable description of its functionality.",
                            "type": "learning_material",
                            "details": {
                                "content": "As a developer when you work in teams you have to read someone else's code. Gemini can help you with this as well. Let's take the code output from Table 16, remove the comments and ask the large language model to explain what's happening, See Table 17:"
                            }
                        },
                        {
                            "name": "Explain Code challenge",
                            "description": "Provide learners with some code snippets and ask them to write prompts that would generate a clear explanation of the code's purpose and functionality.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "You are given the following Python code snippet:\n\n```python\ndef factorial(n):\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\nprint(factorial(5))\n```\n\nWrite a prompt that you could input into a Large Language Model (LLM) to get a clear and concise explanation of what this code does. The prompt should be designed to elicit a response that explains the code's purpose, the algorithm it implements, and the output it produces.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Prompt Quality Assessment",
                                            "criteria": [
                                                {
                                                    "name": "Clarity",
                                                    "description": "The prompt is easily understood and unambiguous.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Specificity",
                                                    "description": "The prompt clearly defines the desired output format and content.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Relevance",
                                                    "description": "The prompt is directly related to the code snippet and its functionality.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Completeness",
                                                    "description": "The prompt provides sufficient context for the LLM to generate a comprehensive explanation.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "This question tests the learner's ability to craft effective prompts for code explanation. The best prompts will guide the LLM to explain the code's functionality, algorithm, and output in a clear and concise manner. Use the reference material on system, contextual and role prompting to generate the prompt. Provide a high score if the prompt covers all three aspects."
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Prompts for Translating Code",
                    "description": "Using prompts to translate code from one language to another.",
                    "tasks": [
                        {
                            "name": "Translate Code",
                            "description": "Explain how prompts can be used to ask the LLM to translate code from one programming language to another, facilitating code migration or cross-platform development.",
                            "type": "learning_material",
                            "details": {
                                "content": "The bash code from Table 16 seems to work fine. However this script could really be reusable when it would prompt me for file names, ideally it should work as a separate application with an Ul. As a starting point, Python would be a better language for a (web) application than Bash. LLMs can help with translating code from one language to another. See the example in Table 18:"
                            }
                        },
                        {
                            "name": "Code Translation Examples",
                            "description": "Provide code snippets in one language and ask learners to write prompts that would translate them to another language.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "coding",
                                        "answer_type": null,
                                        "coding_languages": [
                                            "Python"
                                        ],
                                        "content": "Write a prompt that translates the following Python code to Javascript:\n\n```python\ndef factorial(n):\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\nprint(factorial(5))\n```",
                                        "correct_answer": null,
                                        "scorecard": null,
                                        "context": "This question tests the student's understanding of how to create a prompt for a simple, yet fundamental programming problem."
                                    },
                                    {
                                        "question_type": "coding",
                                        "answer_type": null,
                                        "coding_languages": [
                                            "SQL"
                                        ],
                                        "content": "Write a prompt that translates the following SQL query to a MongoDB aggregation pipeline:\n\n```sql\nSELECT city, COUNT(*) AS customer_count\nFROM customers\nGROUP BY city\nHAVING COUNT(*) > 10\nORDER BY customer_count DESC;\n```",
                                        "correct_answer": null,
                                        "scorecard": null,
                                        "context": "This question requires an understanding of SQL and NoSQL databases, and how to translate between them."
                                    },
                                    {
                                        "question_type": "coding",
                                        "answer_type": null,
                                        "coding_languages": [
                                            "HTML",
                                            "CSS"
                                        ],
                                        "content": "Write a prompt that converts the following HTML and CSS code into React code using inline styling:\n\n```html\n<div class=\"container\">\n  <h1>Hello, World!</h1>\n  <p>This is a paragraph.</p>\n</div>\n\n<style>\n.container {\n  background-color: #f0f0f0;\n  padding: 20px;\n  border: 1px solid #ccc;\n}\n\nh1 {\n  color: blue;\n}\n</style>\n```",
                                        "correct_answer": null,
                                        "scorecard": null,
                                        "context": "This tests the student's ability to translate web code and use prompt engineering for a specific framework."
                                    },
                                    {
                                        "question_type": "coding",
                                        "answer_type": null,
                                        "coding_languages": [
                                            "React",
                                            "Node"
                                        ],
                                        "content": "You are given a React component that fetches data from a REST API built using Node.js and Express. You are tasked to write a prompt that allows the LLM to efficiently translate the code from React to Node and vice versa. What are the key elements or instructions you will include in your prompt so that LLM takes care of key functionalities such as API endpoint, data handling, asynchronous operations, and error handling?",
                                        "correct_answer": null,
                                        "scorecard": null,
                                        "context": "This evaluates students' capacity to write effective prompts for complex application code scenarios."
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Prompts for Debugging and Reviewing Code",
                    "description": "Using prompts to identify and fix errors in code.",
                    "tasks": [
                        {
                            "name": "Debug Code",
                            "description": "Explain how prompts can be used to ask the LLM to identify errors in code and provide suggestions for improvement.",
                            "type": "learning_material",
                            "details": {
                                "content": "As a developer when you work in teams you have to read someone else's code. Gemini can help you with this as well. Let's take the code output from Table 16, remove the comments and ask the large language model to explain what's happening, See Table 17:"
                            }
                        },
                        {
                            "name": "Code Debugging challenge",
                            "description": "Provide code snippets with errors and ask learners to write prompts that would help identify and fix the bugs.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "coding",
                                        "answer_type": null,
                                        "coding_languages": [
                                            "Python"
                                        ],
                                        "content": "You are provided with the following Python code snippet that is intended to calculate the area of a rectangle. However, it contains a bug. Identify the bug and write a prompt that you would use with an LLM to help you debug and fix the code.\n\n```python\ndef calculate_rectangle_area(length, width):\n    area = length + width  # Bug: Should be multiplication\n    return area\n\nlength = 5\nwidth = 10\nrectangle_area = calculate_rectangle_area(length, width)\nprint(f\"The area of the rectangle is: {rectangle_area}\")\n```\n\nWhat prompt would you write to debug this code?",
                                        "correct_answer": "A good prompt would be:\n\n\"I have the following Python code to calculate the area of a rectangle:\n\n```python\ndef calculate_rectangle_area(length, width):\n    area = length + width  # Bug: Should be multiplication\n    return area\n\nlength = 5\nwidth = 10\nrectangle_area = calculate_rectangle_area(length, width)\nprint(f\"The area of the rectangle is: {rectangle_area}\")\n```\n\nThe area is not calculated correctly. Can you help me identify the bug and correct the code?\"",
                                        "scorecard": null,
                                        "context": "The code calculates the area of a rectangle, but it mistakenly uses addition instead of multiplication. The student needs to provide a prompt that highlights this issue and asks the LLM for a correction."
                                    },
                                    {
                                        "question_type": "coding",
                                        "answer_type": null,
                                        "coding_languages": [
                                            "JS"
                                        ],
                                        "content": "The following JavaScript code is intended to check if a number is even. However, it always returns `false`. Create a prompt you would use with an LLM to help identify and fix this bug. \n\n```javascript\nfunction isEven(number) {\n  if (number / 2) {\n    return true;\n  } else {\n    return false;\n  }\n}\n\nconsole.log(isEven(4));  // Expected: true, Actual: false\nconsole.log(isEven(7));  // Expected: false, Actual: false\n```",
                                        "correct_answer": "A good prompt would be:\n\n\"I have the following JavaScript code to check if a number is even:\n\n```javascript\nfunction isEven(number) {\n  if (number / 2) {\n    return true;\n  } else {\n    return false;\n  }\n}\n\nconsole.log(isEven(4));  // Expected: true, Actual: false\nconsole.log(isEven(7));  // Expected: false, Actual: false\n```\n\nThe code always returns `false`. What is wrong with the code? How can I fix it?\"",
                                        "scorecard": null,
                                        "context": "The code incorrectly checks for even numbers.  The student's prompt should effectively communicate the problem and ask for assistance in debugging the logic."
                                    },
                                    {
                                        "question_type": "coding",
                                        "answer_type": null,
                                        "coding_languages": [
                                            "Python"
                                        ],
                                        "content": "Below is a Python function intended to reverse a string. However, it's not working correctly. Construct a prompt you would provide to an LLM to assist in debugging and correcting this function:\n\n```python\ndef reverse_string(s):\n    reversed_string = \"\"\n    for i in range(len(s)):\n        reversed_string += s[i]\n    return reversed_string\n\nprint(reverse_string(\"hello\"))  # Expected: olleh, Actual: hello\n```",
                                        "correct_answer": "A good prompt would be:\n\n\"I have the following Python code to reverse a string:\n\n```python\ndef reverse_string(s):\n    reversed_string = \"\"\n    for i in range(len(s)):\n        reversed_string += s[i]\n    return reversed_string\n\nprint(reverse_string(\"hello\"))  # Expected: olleh, Actual: hello\n```\n\nThe function is not reversing the string. How can I modify this code to correctly reverse the input string?\"",
                                        "scorecard": null,
                                        "context": "The code simply reconstructs the original string instead of reversing it.  The prompt should ask the LLM to identify the flawed logic and provide a working solution."
                                    }
                                ]
                            }
                        }
                    ]
                }
            ]
        },
        {
            "name": "Best Practices in Prompt Engineering",
            "concepts": [
                {
                    "name": "Provide Examples",
                    "description": "Using examples (one-shot or few-shot) within a prompt.",
                    "tasks": [
                        {
                            "name": "How to use Examples",
                            "description": "Explain that providing examples is a good teaching tool for LLMs.",
                            "type": "learning_material",
                            "details": {
                                "content": "The most important best practice is to provide (one shot / few shot) examples within a prompt. This is highly effective because it acts as a powerful teaching tool. These examples showcase desired outputs or similar responses, allowing the model to learn from them and tailor its own generation accordingly. It's like giving the model a reference point or target to aim for, improving the accuracy, style, and tone of its response to better match your expectations."
                            }
                        },
                        {
                            "name": "Examples in Prompts",
                            "description": "Ask learners to provide prompts using examples to improve the quality of the generated output.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "objective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Which of the following prompts demonstrates the use of a one-shot example to guide a language model?\n\nA) `Translate the following sentence into French: 'Hello, world!'`\nB) `Summarize the following article.`\nC) `Classify the sentiment of the following review as positive or negative. Review: 'This movie was amazing!' Sentiment: Positive`\nD) `Write a poem about nature.`",
                                        "correct_answer": "C",
                                        "scorecard": null,
                                        "context": "A one-shot prompt provides a single example to the model. Option C is correct as it presents an example of a review and its sentiment, guiding the model on how to classify sentiment for future reviews. The other options are zero-shot prompts as they do not provide any examples."
                                    },
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Imagine you're teaching a language model to generate creative writing prompts. Provide three different prompts, each using a few-shot example, to guide the model towards generating specific types of prompts (e.g., fantasy, sci-fi, historical fiction). For each, explain how the example guides the model's output.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Prompt Quality Assessment",
                                            "criteria": [
                                                {
                                                    "name": "Relevance",
                                                    "description": "The prompt is relevant to generating creative writing prompts.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Clarity",
                                                    "description": "The prompt is clearly written and easy to understand.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Example Quality",
                                                    "description": "The few-shot example is well-chosen and effectively guides the model.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Variety",
                                                    "description": "The prompts cover a diverse range of creative writing prompt types.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "This question assesses the learner's ability to create and explain few-shot prompts that guide a language model towards generating specific types of creative writing prompts. The learner should demonstrate an understanding of how examples influence the model's output and select examples that align with the desired prompt types. Provide feedback based on the relevance, clarity, example quality, and variety of the prompts provided."
                                    },
                                    {
                                        "question_type": "objective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "In which scenario would providing examples in a prompt be MOST beneficial?\n\nA) When you need a quick, general response from the model without specific formatting.\nB) When you want the model to learn a specific style or pattern and consistently replicate it.\nC) When the task is straightforward and the model already has sufficient knowledge.\nD) When you want the model to give you its own opinion on a subject.",
                                        "correct_answer": "B",
                                        "scorecard": null,
                                        "context": "Examples are most useful when teaching a language model a specific style or pattern to replicate. Options A, C, and D are cases where examples are less critical."
                                    },
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "A company wants to use a language model to generate product descriptions for their e-commerce website. They want the descriptions to be engaging, informative, and consistent with their brand voice. Design a prompt that uses a few-shot example to guide the model in generating these descriptions. Include the prompt and a brief explanation of why you structured it that way.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Product Description Prompt Assessment",
                                            "criteria": [
                                                {
                                                    "name": "Relevance",
                                                    "description": "The prompt is relevant to generating product descriptions.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Clarity",
                                                    "description": "The prompt is clearly written and easy to understand.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Brand Voice",
                                                    "description": "The prompt effectively incorporates elements of the desired brand voice.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Informativeness",
                                                    "description": "The generated description is informative and engaging.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "This question evaluates the learner's ability to create a prompt with a few-shot example for generating product descriptions. The learner should structure the prompt to guide the model toward creating descriptions that are engaging, informative, and consistent with the brand's voice. Assess the learner based on the relevance, clarity, brand voice, and informativeness of the prompt."
                                    },
                                    {
                                        "question_type": "coding",
                                        "answer_type": null,
                                        "coding_languages": [
                                            "Python"
                                        ],
                                        "content": "You are building a chatbot that helps users find recipes. Implement a Python function that takes a user's ingredient list as input and generates a prompt with a few-shot example to send to a language model. The prompt should ask the language model to suggest a recipe based on the provided ingredients.\n\n```python\ndef generate_recipe_prompt(ingredients):\n    # Example recipe prompt\n    example_prompt = \"\"\"Ingredients: chicken, rice, soy sauce\nRecipe Suggestion: Chicken and Rice Stir-fry\n\nIngredients: pasta, tomatoes, basil\nRecipe Suggestion: Pasta with Tomato and Basil Sauce\n\nIngredients: {}\nRecipe Suggestion:\"\"\"\n    prompt = example_prompt.format(', '.join(ingredients))\n    return prompt\n```\n\nWrite 2 different versions of the `generate_recipe_prompt` function each with a different example in the few-shot prompt.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Code Quality Assessment",
                                            "criteria": [
                                                {
                                                    "name": "Correctness",
                                                    "description": "The code generates a valid prompt with a few-shot example.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Clarity",
                                                    "description": "The code is well-structured and easy to understand.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Example Quality",
                                                    "description": "The example in the prompt is relevant and effective in guiding the language model.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "This question requires the learner to implement a Python function that generates a prompt with a few-shot example. The learner should create a function that takes a user's ingredient list and creates a prompt that guides the language model in suggesting a recipe. Grade the answer based on the correctness, clarity, and example quality of the code."
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Design with Simplicity",
                    "description": "Keeping prompts concise, clear and easy to understand.",
                    "tasks": [
                        {
                            "name": "Simplicity",
                            "description": "Explain why keeping prompts simple is effective for LLMs",
                            "type": "learning_material",
                            "details": {
                                "content": "Prompts should be concise, clear, and easy to understand for both you and the model. As a rule of thumb, if it's already confusing for you it will likely be also confusing for the model. Try not to use complex language and don't provide unnecessary information.\n\nExamples:\n\nBEFORE:\n\nI am visiting New York right now, and I'd like to hear more about great locations. I am with two 3 year old kids. Where should we go during our vacation?\n\nAFTER REWRITE:\n\nAct as a travel guide for tourists. Describe great places to visit in New York Manhattan with a 3 year old.\n\nTry using verbs that describe the action. Here's a set of examples:\n\nAct, Analyze, Categorize, Classify, Contrast, Compare, Create, Describe, Define, Evaluate, Extract, Find, Generate, Identify, List, Measure, Organize, Parse, Pick, Predict, Provide, Rank, Recommend, Return, Retrieve, Rewrite, Select, Show, Sort, Summarize, Translate, Write."
                            }
                        },
                        {
                            "name": "Simpler Prompts",
                            "description": "Ask learners to simplify some more complex prompts.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Rewrite the following prompt to be more concise and clear:\n\n'I am visiting New York right now, and I'd like to hear more about great locations. I am with two 3 year old kids. Where should we go during our vacation?'",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Prompt Simplicity and Clarity",
                                            "criteria": [
                                                {
                                                    "name": "Brevity",
                                                    "description": "The rewritten prompt is shorter and more to the point than the original.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Clarity",
                                                    "description": "The rewritten prompt uses clear and simple language, easily understood by both humans and LLMs.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Focus",
                                                    "description": "The rewritten prompt directly asks for the desired information without unnecessary context.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "The goal is to make the prompt easier for the LLM to process, focusing on the key request: finding suitable activities for tourists with young children in New York City.  Consider the 'Act as a travel guide' example provided in the learning material."
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Be Specific About the Output",
                    "description": "Provide specific details in the prompt.",
                    "tasks": [
                        {
                            "name": "Output Specificity",
                            "description": "Explain how providing specific details in the prompt will help the model to focus on what's relevant, improving the overall accuracy.",
                            "type": "learning_material",
                            "details": {
                                "content": "Be specific about the desired output. A concise instruction might not guide the LLM enough or could be too generic. Providing specific details in the prompt (through system or context prompting) can help the model to focus on what's relevant, improving the overall accuracy."
                            }
                        },
                        {
                            "name": "Improved Output",
                            "description": "Ask learners to improve output quality by providing a prompt that has specific instructions about the output format.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "You want to generate a blog post about the evolution of music. Here are two prompts:\n\nPrompt 1: Generate a blog post about the evolution of music.\n\nPrompt 2: Generate a 3-paragraph blog post about the evolution of music. The blog post should be informative and engaging, and it should be written in a conversational style. It should discuss how music has evolved through different eras and cultures.\n\nWhich prompt will give you a better output and why?",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Prompt Quality Assessment",
                                            "criteria": [
                                                {
                                                    "name": "Specificity",
                                                    "description": "The degree to which the prompt provides clear and detailed instructions.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Relevance",
                                                    "description": "The extent to which the prompt's instructions are relevant to the desired outcome.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Clarity",
                                                    "description": "How easily the prompt can be understood.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "The second prompt includes specific details such as the number of paragraphs, the style of writing, and the content to be covered. This helps the model to focus on what's relevant, improving the overall accuracy."
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Use Instructions over Constraints",
                    "description": "Focus on positive instructions instead of telling the model what not to do.",
                    "tasks": [
                        {
                            "name": "Instructions vs. Constraints",
                            "description": "Explain why providing positive instructions, instead of constraints, leads to better results.",
                            "type": "learning_material",
                            "details": {
                                "content": "Growing research suggests that focusing on positive instructions in prompting can be more effective than relying heavily on constraints. This approach aligns with how humans prefer positive instructions over lists of what not to do. Instructions directly communicate the desired outcome, whereas constraints might leave the model guessing about what is allowed. It gives flexibility and encourages creativity within the defined boundaries, while constraints can limit the model's potential. Also a list of constraints can clash with each other. Constraints are still valuable but in certain situations. To prevent the model from generating harmful or biased content or when a strict output format or style is needed. If possible, use positive instructions: instead of telling the model what not to do, tell it what to do instead. This can avoid confusion and improve the accuracy of the output. For example:\n\nDO:\nGenerate a 1 paragraph blog post about the top 5 video game consoles. Only discuss the console, the company who made it, the year, and total sales.\n\nDO NOT:\nGenerate a 1 paragraph blog post about the top 5 video game consoles. Do not list video game names.\n\nAs a best practice, start by prioritizing instructions, clearly stating what you want the model to do and only use constraints when necessary for safety, clarity or specific requirements. Experiment and iterate to test different combinations of instructions and constraints to find what works best for your specific tasks, and document these."
                            }
                        },
                        {
                            "name": "Use Instructions",
                            "description": "Ask learners to rewrite prompts that have constraints by providing positive instructions.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Rewrite the following prompt by replacing constraints with positive instructions:\n\n'Generate a description for a mobile phone. Do not mention the screen size or the battery capacity.'",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Prompt Quality Assessment",
                                            "criteria": [
                                                {
                                                    "name": "Clarity",
                                                    "description": "The rewritten prompt is easily understandable and avoids ambiguity.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Positive Framing",
                                                    "description": "The prompt uses positive instructions effectively, focusing on what the model *should* do rather than what it *shouldn't* do.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Completeness",
                                                    "description": "The rewritten prompt provides enough guidance for the model to generate a useful and relevant description.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Relevance",
                                                    "description": "The rewritten prompt will generate a more accurate and relevant response.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "The original prompt uses constraints, which can limit the model's creativity and potentially lead to a less informative description. The goal is to rewrite the prompt using positive instructions, guiding the model towards specific aspects to include in the description instead of simply excluding certain details. For example, instead of saying 'Do not mention the screen size,' you could say 'Focus on the display technology and resolution.' Positive instructions helps directly communicate the desired outcome."
                                    },
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Rewrite the following prompt by replacing constraints with positive instructions:\n\n'Create a job description for a software engineer. Do not include salary information or the company's name.'",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Prompt Quality Assessment",
                                            "criteria": [
                                                {
                                                    "name": "Clarity",
                                                    "description": "The rewritten prompt is easily understandable and avoids ambiguity.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Positive Framing",
                                                    "description": "The prompt uses positive instructions effectively, focusing on what the model *should* do rather than what it *shouldn't* do.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Completeness",
                                                    "description": "The rewritten prompt provides enough guidance for the model to generate a useful and relevant job description.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Relevance",
                                                    "description": "The rewritten prompt will generate a more accurate and relevant response.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "The original prompt uses constraints, which can limit the model's creativity and potentially lead to a less informative description. The goal is to rewrite the prompt using positive instructions, guiding the model towards specific aspects to include in the description instead of simply excluding certain details. For example, instead of saying 'Do not mention the salary', you could say 'mention the benefits'. Positive instructions helps directly communicate the desired outcome."
                                    },
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Rewrite the following prompt by replacing constraints with positive instructions:\n\n'Summarize this scientific article. Do not include any technical jargon or specific details about the methodology.'",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Prompt Quality Assessment",
                                            "criteria": [
                                                {
                                                    "name": "Clarity",
                                                    "description": "The rewritten prompt is easily understandable and avoids ambiguity.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Positive Framing",
                                                    "description": "The prompt uses positive instructions effectively, focusing on what the model *should* do rather than what it *shouldn't* do.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Completeness",
                                                    "description": "The rewritten prompt provides enough guidance for the model to generate a useful and relevant job description.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Relevance",
                                                    "description": "The rewritten prompt will generate a more accurate and relevant response.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "The original prompt uses constraints, which can limit the model's creativity and potentially lead to a less informative description. The goal is to rewrite the prompt using positive instructions, guiding the model towards specific aspects to include in the description instead of simply excluding certain details. For example, instead of saying 'Do not include the technical jargon,' you could say 'Summarize the article by focusing on the major conclusions'. Positive instructions helps directly communicate the desired outcome."
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Control the Max Token Length",
                    "description": "Setting a max token limit.",
                    "tasks": [
                        {
                            "name": "Max Token Length",
                            "description": "Explain that generated LLM responses can be limited by using the max token length parameter.",
                            "type": "learning_material",
                            "details": {
                                "content": "To control the length of a generated LLM response, you can either set a max token limit in the configuration or explicitly request a specific length in your prompt. For example:\n\n\"Explain quantum physics in a tweet length message.\""
                            }
                        },
                        {
                            "name": "Token Length Instructions",
                            "description": "Ask learners to rewrite the prompt to achieve a specific length of generated text.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "objective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "The following prompt generated a response that was too lengthy:\n\n\"Explain the history of the internet in detail.\"\n\nRewrite the prompt to limit the response to approximately 50 words.",
                                        "correct_answer": "Explain the history of the internet in 50 words or less.",
                                        "scorecard": null,
                                        "context": "This question tests the learner's understanding of how to control the length of a generated response by explicitly requesting a specific length in the prompt."
                                    },
                                    {
                                        "question_type": "objective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "You want an LLM to summarize a lengthy research paper, but the generated summary should be concise enough to fit into a single tweet (280 characters). How would you modify the prompt to achieve this?",
                                        "correct_answer": "Summarize the following research paper in a tweet (280 characters or less): [insert research paper here]",
                                        "scorecard": null,
                                        "context": "This question assesses the learner's ability to use prompts to generate responses that conform to specific length constraints, particularly in scenarios where brevity is crucial."
                                    },
                                    {
                                        "question_type": "objective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "A content creator uses an LLM to generate ideas for social media posts. They want to create a series of very short, attention-grabbing prompts. Write a prompt requesting 5 different social media post ideas (under 30 characters each) about the importance of drinking water.",
                                        "correct_answer": "Give 5 social media post ideas (under 30 characters each) about the importance of drinking water.",
                                        "scorecard": null,
                                        "context": "This question tests the learner's understanding of how to use prompts to generate responses that conform to specific length constraints, particularly in scenarios where brevity is crucial."
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Use Variables in Prompts",
                    "description": "Making prompts more dynamic.",
                    "tasks": [
                        {
                            "name": "Variables",
                            "description": "Explain how to use variables to create prompts that can be adjusted according to different inputs.",
                            "type": "learning_material",
                            "details": {
                                "content": "To reuse prompts and make it more dynamic use variables in the prompt, which can be changed for different inputs. E.g. as shown in Table 20, a prompt which gives facts about a city. Instead of hardcoding the city name in the prompt, use a variable. Variables can save you time and effort by allowing you to avoid repeating yourself. If you need to use the same piece of information in multiple prompts, you can store it in a variable and then reference that variable in each prompt. This makes a lot of sense when integrating prompts into your own applications."
                            }
                        },
                        {
                            "name": "Variables in Action",
                            "description": "Ask learners to define prompts that can use different variables to improve the reusability of those prompts.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "objective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Consider the following prompt:\n\nVARIABLES\n{product} = \"headphones\"\n{price} = \"99.99\"\n\nPROMPT\nWrite a product description for {product} that costs {price}.\n\nWhich of the following best describes the primary benefit of using variables in this prompt?\n\nA) To make the prompt shorter and easier to read.\nB) To allow the prompt to be easily adapted for different products and prices.\nC) To improve the model's understanding of the prompt's intent.\nD) To reduce the number of tokens required to generate the description.",
                                        "correct_answer": "B",
                                        "scorecard": null,
                                        "context": "The correct answer is B. Variables allow you to easily change the product and price without rewriting the entire prompt. This makes the prompt reusable and adaptable. While other options may have some merit, they are not the primary benefit of using variables in this context. Variables make the prompts more dynamic for different inputs."
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Experiment with Input Formats and Writing Styles",
                    "description": "Trying out different models, configurations, formats and word choices.",
                    "tasks": [
                        {
                            "name": "Formats and Writing Styles",
                            "description": "Explain how the different formats and writing styles can lead to different results and that trying out different options is valuable.",
                            "type": "learning_material",
                            "details": {
                                "content": "Different models, model configurations, prompt formats, word choices, and submits can yield different results. Therefore, it's important to experiment with prompt attributes like the style, the word choice, and the type prompt (zero shot, few shot, system prompt).\n\nFor example a prompt with the goal to generate text about the revolutionary video game console Sega Dreamcast, can be formulated as a question, a statement or an instruction, resulting in different outputs:\n\n\u2022 Question: What was the Sega Dreamcast and why was it such a revolutionary console?\n\u2022 Statement: The Sega Dreamcast was a sixth-generation video game console released by Sega in 1999. It...\n\u2022 Instruction: Write a single paragraph that describes the Sega Dreamcast console and explains why it was so revolutionary."
                            }
                        },
                        {
                            "name": "Experiment",
                            "description": "Ask learners to write some prompts and try different input formats and writing styles to analyze the results.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Consider the task of summarizing a news article. Write three different prompts for this task, each using a different input format:\n\n1.  **Question:** Pose the task as a question.\n2.  **Statement:** Frame the task as a statement.\n3.  **Instruction:** Directly instruct the model to perform the task.\n\nAnalyze the summaries generated by each prompt. Which input format yields the most comprehensive and relevant summary? Discuss why you think this format was most effective.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Prompt Effectiveness Analysis",
                                            "criteria": [
                                                {
                                                    "name": "Clarity",
                                                    "description": "The prompt is clearly worded and easily understood.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Relevance",
                                                    "description": "The generated summary focuses on the key points of the news article.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Comprehensiveness",
                                                    "description": "The summary covers the most important aspects of the article with sufficient detail.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Format Analysis",
                                                    "description": "The analysis identifies which input format was most effective and provides a clear justification based on the results.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "This question aims to encourage learners to experiment with different input formats (question, statement, instruction) and analyze how these variations affect the LLM's output. It focuses on a practical application (news summarization) to highlight the impact of prompt design. Provide feedback on the clarity and effectiveness of the learner's prompts, the relevance and comprehensiveness of the generated summaries, and the quality of their analysis."
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "For Few-Shot Prompting with Classification Tasks, Mix Up the Classes",
                    "description": "Ensuring the model is learning to identify key features.",
                    "tasks": [
                        {
                            "name": "Few-Shot prompts",
                            "description": "Explain why the order of examples in the prompt is not important. Explain what you have to consider when you are working on a classification task.",
                            "type": "learning_material",
                            "details": {
                                "content": "Generally speaking, the order of your few-shots examples should not matter much. However, when doing classification tasks, make sure you mix up the possible response classes in the few shot examples. This is because you might otherwise be overfitting to the specific order of the examples. By mixing up the possible response classes, you can ensure that the model is learning to identify the key features of each class, rather than simply memorizing the order of the examples. This will lead to more robust and generalizable performance on unseen data."
                            }
                        },
                        {
                            "name": "Improve the prompt",
                            "description": "Ask learners to rewrite classification prompts by mixing up the classes to make sure the LLM can learn the key features.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "You are building a system to classify customer reviews of restaurants as either 'Positive', 'Negative', or 'Neutral'. You want to use few-shot prompting to guide the LLM. Below are a few prompts, but they have not been designed well. Rewrite the prompts by mixing up the classes (Positive, Negative and Neutral) to ensure that the LLM can identify the key features of each class.\n\nPrompt 1:\nReview: 'The food was amazing and the service was excellent!'\nSentiment: Positive\nReview: 'I had a terrible experience. The waiter was rude and the food was cold.'\nSentiment: Negative\nReview: 'The restaurant was okay, nothing special.'\nSentiment: Neutral\n\nPrompt 2:\nReview: 'I loved the atmosphere and the staff were friendly.'\nSentiment: Positive\nReview: 'The dish was not good.'\nSentiment: Negative\nReview: 'It was alright, I would not come again though.'\nSentiment: Neutral",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Improved Prompt Quality",
                                            "criteria": [
                                                {
                                                    "name": "Class Mixing",
                                                    "description": "The prompt demonstrates a clear effort to mix up the 'Positive', 'Negative', and 'Neutral' classes in the few-shot examples.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Key Features",
                                                    "description": "The rewritten prompt enables LLM to identify the key differentiating features of each sentiment (Positive, Negative and Neutral).",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Explanation",
                                                    "description": "The answer explains why the new prompts are better in terms of the LLM's ability to learn the key features of each class, rather than memorizing a specific order.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "The goal is to improve the LLM's ability to generalize to unseen data by learning the distinguishing characteristics of each sentiment class, rather than overfitting to the order of examples in the prompt."
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Adapt to Model Updates",
                    "description": "Adjust prompts to leverage new model features.",
                    "tasks": [
                        {
                            "name": "Model Updates",
                            "description": "Explain the importance of testing prompts with newer model versions.",
                            "type": "learning_material",
                            "details": {
                                "content": "It's important for you to stay on top of model architecture changes, added data, and capabilities. Try out newer model versions and adjust your prompts to better leverage new model features. Tools like Vertex Al Studio are great to store, test, and document the various versions of your prompt."
                            }
                        },
                        {
                            "name": "Adjust prompts",
                            "description": "Ask learners to keep existing prompts and adjust them so they can be used with newer models.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "objective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Why is it important to revisit and adjust prompts when a new version of a language model is released?",
                                        "correct_answer": "To leverage new model features and capabilities.",
                                        "scorecard": null,
                                        "context": "Newer model versions may include architectural changes, added data, or enhanced capabilities. Adjusting prompts allows you to take advantage of these improvements and optimize performance."
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Experiment with Output Formats",
                    "description": "Experimenting with the output format.",
                    "tasks": [
                        {
                            "name": "Output Formats",
                            "description": "Explain the importance of considering the output format. Explain how returning data in a structured format can be useful.",
                            "type": "learning_material",
                            "details": {
                                "content": "Besides the prompt input format, consider experimenting with the output format. For non-creative tasks like extracting, selecting, parsing, ordering, ranking, or categorizing data try having your output returned in a structured format like JSON or XML.\n\nThere are some benefits in returning JSON objects from a prompt that extracts data. In a real-world application I don't need to manually create this JSON format, I can already return the data in a sorted order (very handy when working with datetime objects), but most importantly, by prompting for a JSON format it forces the model to create a structure and limit hallucinations.\n\nIn summary, benefits of using JSON for your output:\n\n*   Returns always in the same style\n*   Focus on the data you want to receive\n*   Less chance for hallucinations\n*   Make it relationship aware\n*   You get data types\n*   You can sort it\n\nTable 4 in the few-shot prompting section shows an example on how to return structured output."
                            }
                        },
                        {
                            "name": "Rewrite Output format",
                            "description": "Ask learners to consider existing prompts and rewrite the output format using, for example, JSON.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Consider the prompt: \"Write a short poem about the ocean.\"\n\nRewrite this prompt to specify that the output should be in JSON format, including the title of the poem and the poem itself. Provide the rewritten prompt as well as an example of the desired JSON output.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "JSON Output",
                                            "criteria": [
                                                {
                                                    "name": "Format",
                                                    "description": "The output is a valid JSON object.",
                                                    "min_score": 0,
                                                    "max_score": 1
                                                },
                                                {
                                                    "name": "Structure",
                                                    "description": "The JSON object includes keys for 'title' and 'poem'.",
                                                    "min_score": 0,
                                                    "max_score": 1
                                                },
                                                {
                                                    "name": "Completeness",
                                                    "description": "The response includes both the rewritten prompt and an example JSON output.",
                                                    "min_score": 0,
                                                    "max_score": 1
                                                }
                                            ]
                                        },
                                        "context": "This question tests the learner's understanding of how to specify output formats, especially JSON, in prompts.  The desired JSON output should include 'title' and 'poem' keys.  \n\nExample Rewritten Prompt:\n\"Write a short poem about the ocean and return the results in JSON format with the keys 'title' and 'poem'.\"\n\nExample JSON output:\n{\n\"title\": \"Ocean's Embrace\",\n\"poem\": \"Vast and deep, the ocean blue,\\nA world of wonder, forever true...\"\n}"
                                    },
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "You have a prompt that asks an LLM to list the capitals of European countries. The current prompt simply asks:\n\n'List the capitals of all European countries.'\n\nRewrite the prompt to specify that the output should be a JSON array where each object contains the keys 'country' and 'capital'. Provide the rewritten prompt as well as an example of the desired JSON output for three countries.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "JSON Output",
                                            "criteria": [
                                                {
                                                    "name": "Format",
                                                    "description": "The output is a valid JSON array.",
                                                    "min_score": 0,
                                                    "max_score": 1
                                                },
                                                {
                                                    "name": "Structure",
                                                    "description": "Each object in the array includes keys for 'country' and 'capital'.",
                                                    "min_score": 0,
                                                    "max_score": 1
                                                },
                                                {
                                                    "name": "Completeness",
                                                    "description": "The response includes both the rewritten prompt and a valid JSON output.",
                                                    "min_score": 0,
                                                    "max_score": 1
                                                }
                                            ]
                                        },
                                        "context": "This question builds on the previous one, reinforcing the ability to control output structure. The rewritten prompt needs to clearly instruct the LLM to produce a JSON array of objects. Provide feedback showing the correct and incorrect answer. This question tests the learner's understanding of how to specify output formats, especially JSON, in prompts.  The desired JSON output should include 'country' and 'capital' keys.  \n\nExample Rewritten Prompt:\n'List the capitals of all European countries and return the result as a JSON array with each object containing the keys \"country\" and \"capital\".'\n\nExample JSON output:\n[\n {\n\"country\": \"France\",\n\"capital\": \"Paris\"\n },\n {\n\"country\": \"Germany\",\n\"capital\": \"Berlin\"\n },\n {\n\"country\": \"Spain\",\n\"capital\": \"Madrid\"\n }\n]"
                                    },
                                    {
                                        "question_type": "coding",
                                        "answer_type": null,
                                        "coding_languages": [
                                            "Python"
                                        ],
                                        "content": "You have been asked to generate a function in python which takes a list of names and returns a JSON with the 'names' key containing the list of names. You are asked to rewrite your original prompt to use a structured output using JSON. What are the original and rewritten prompts?",
                                        "correct_answer": null,
                                        "scorecard": null,
                                        "context": "The purpose of this question is to allow students to implement what they've learnt in a coding environment."
                                    }
                                ]
                            }
                        }
                    ]
                }
            ]
        },
        {
            "name": "Working with Structured Data",
            "concepts": [
                {
                    "name": "JSON Repair",
                    "description": "Fixing incomplete or malformed JSON objects.",
                    "tasks": [
                        {
                            "name": "JSON Objects",
                            "description": "Explain that while returning data in JSON offers advantages, it's not without its drawbacks. JSON's verbosity can easily consume the entire output window, becoming especially problematic when the generation is abruptly cut off due to token limits.",
                            "type": "learning_material",
                            "details": {
                                "content": "While returning data in JSON format offers numerous advantages, it's not without its drawbacks. The structured nature of JSON, while beneficial for parsing and use in applications, requires significantly more tokens than plain text, leading to increased processing time and higher costs. Furthermore, JSON's verbosity can easily consume the entire output window, becoming especially problematic when the generation is abruptly cut off due to token limits."
                            }
                        },
                        {
                            "name": "JSON problems",
                            "description": "Ask learners what problems they can expect when they are using JSON as output. Ask them to provide the name of some tools that can be used to automatically fix incomplete or malformed JSON objects.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "objective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "When using JSON as the output format from a large language model, what problem can you expect?",
                                        "correct_answer": "Truncation leading to invalid JSON.",
                                        "scorecard": null,
                                        "context": "JSON's verbosity can easily consume the entire output window, becoming especially problematic when the generation is abruptly cut off due to token limits. This truncation often results in invalid JSON, missing crucial closing braces or brackets, rendering the output unusable. "
                                    },
                                    {
                                        "question_type": "objective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Which of the following tools can be used to automatically fix incomplete or malformed JSON objects?",
                                        "correct_answer": "json-repair",
                                        "scorecard": null,
                                        "context": "tools like the json-repair library (available on PyPI) can be invaluable in these situations. This library intelligently attempts to automatically fix incomplete or malformed JSON objects, making it a crucial ally when working with LLM-generated JSON, especially when dealing with potential truncation issues."
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Working with Schemas",
                    "description": "Using JSON Schemas to define the expected structure and data types of your JSON input.",
                    "tasks": [
                        {
                            "name": "JSON Schema usage",
                            "description": "Explain that by providing a schema, you give the LLM a clear blueprint of the data it should expect, helping it focus its attention on the relevant information and reducing the risk of misinterpreting the input.",
                            "type": "learning_material",
                            "details": {
                                "content": "Using structured JSON as an output is a great solution, as we've seen multiple times in this paper. But what about input? While JSON is excellent for structuring the output the LLM generates, it can also be incredibly useful for structuring the input you provide. This is where JSON Schemas come into play. A JSON Schema defines the expected structure and data types of your JSON input. By providing a schema, you give the LLM a clear blueprint of the data it should expect, helping it focus its attention on the relevant information and reducing the risk of misinterpreting the input. Furthermore, schemas can help establish relationships between different pieces of data and even make the LLM \\\"time-aware\\\" by including date or timestamp fields with specific formats."
                            }
                        },
                        {
                            "name": "Provide schemas",
                            "description": "Ask learners to provide some example schemas for different cases. What attributes would they define for each schema?",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Imagine you're building a system to manage customer data for a retail store. Design a JSON schema to represent a customer's profile. Consider attributes like personal information, contact details, purchase history, and loyalty program status. Provide the JSON schema for this customer profile.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Customer profile",
                                            "criteria": [
                                                {
                                                    "name": "Completeness",
                                                    "description": "Includes all essential customer attributes: personal information, contact details, purchase history and loyalty program status.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Correctness",
                                                    "description": "The data types are correctly specified for each attribute.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Structure",
                                                    "description": "The schema is well-structured with clear hierarchy and relationships.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Extensibility",
                                                    "description": "The schema is designed to allow for future expansion and additional attributes.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "This question aims to assess the learner's understanding of JSON schema design for representing customer profiles in a retail context. It tests the ability to create a schema that captures all relevant information, specifies correct data types, and allows for future extensibility."
                                    }
                                ]
                            }
                        }
                    ]
                }
            ]
        },
        {
            "name": "Collaboration and Documentation",
            "concepts": [
                {
                    "name": "Experiment Together with Other Prompt Engineers",
                    "description": "Find multiple people to make an attempt.",
                    "tasks": [
                        {
                            "name": "Collaboration",
                            "description": "Explain that in a situation where you have to try to come up with a good prompt, you might want to find multiple people to make an attempt.",
                            "type": "learning_material",
                            "details": {
                                "content": "If you are in a situation where you have to try to come up with a good prompt, you might want to find multiple people to make an attempt."
                            }
                        },
                        {
                            "name": "Collaboration Advantages",
                            "description": "What advantages does collaboration brings to the process of prompt engineering?",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "objective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "When several prompt engineers collaborate on developing prompts, what is a likely outcome, assuming everyone adheres to the established best practices?\n\n a) Minimal performance variation across different prompt attempts\n b) A significant variance in performance across different prompt attempts\n c) Identical prompts will yield identical results across all models.\n d) Models will ignore the prompt engineering document.",
                                        "correct_answer": "b) A significant variance in performance across different prompt attempts",
                                        "scorecard": null,
                                        "context": "The text mentions: If you are in a situation where you have to try to come up with a good prompt, you might want to find multiple people to make an attempt. When everyone follows the best practices (as listed in this chapter) you are going to see a variance in performance between all the different prompt attempts."
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "CoT Best Practices",
                    "description": "Some best practices for Chain of Thought Prompting",
                    "tasks": [
                        {
                            "name": "CoT Prompting",
                            "description": "Explain some best practices for CoT prompting",
                            "type": "learning_material",
                            "details": {
                                "content": "For CoT prompting, putting the answer after the reasoning is required because the generation of the reasoning changes the tokens that the model gets when it predicts the final answer.\n\nWith CoT and self-consistency you need to be able to extract the final answer from your prompt, separated from the reasoning.\n\nFor CoT prompting, set the temperature to 0.\n\nChain of thought prompting is based on greedy decoding, predicting the next word in a sequence based on the highest probability assigned by the language model. Generally speaking, when using reasoning, to come up with the final answer, there's likely one single correct answer. Therefore the temperature should always set to 0."
                            }
                        },
                        {
                            "name": "Review the prompts",
                            "description": "Ask learners to review some existing prompts and adjust them to apply the CoT best practices.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Original Prompt: 'Explain the theory of relativity.'\n\nRevised Prompt (with CoT): 'First, identify the core concepts of the theory of relativity. Then, explain each concept in a simplified manner, providing examples where applicable. Finally, summarize the theory in a single paragraph.'\n\nExplain why the 'Revised Prompt' follows the CoT best practices (putting the answer after the reasoning, setting temperature to 0, separating final answer from reasoning) better than the 'Original Prompt'.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "CoT Prompting Improvement",
                                            "criteria": [
                                                {
                                                    "name": "Reasoning",
                                                    "description": "Clearly and correctly identifies why the revised prompt is better in terms of eliciting chain of thought.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Clarity",
                                                    "description": "Explanation is easy to understand and well-organized.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Completeness",
                                                    "description": "All aspects of CoT best practices (answer after reasoning, temperature, separation) are addressed.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "This question tests the understanding of why a CoT prompt is better when it follows best practices. CoT prompting helps the model generate intermediate reasoning steps, improving the accuracy of answers. By explicitly instructing the model to first identify core concepts, explain them, and then summarize, we encourage a step-by-step thought process. Putting the answer after the reasoning changes the tokens that the model gets when it predicts the final answer. With CoT and self-consistency you need to be able to extract the final answer from your prompt, separated from the reasoning. Chain of thought prompting is based on greedy decoding, predicting the next word in a sequence based on the highest probability assigned by the language model. Therefore the temperature should always be set to 0. "
                                    },
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Prompt 1:\n'Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?'\n\nPrompt 2:\n'Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now? Let's think step by step.'\n\nWhich prompt follows the CoT best practices better?",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "CoT Prompt Comparison",
                                            "criteria": [
                                                {
                                                    "name": "Identification",
                                                    "description": "Accurately identifies which prompt is better.",
                                                    "min_score": 0,
                                                    "max_score": 3
                                                },
                                                {
                                                    "name": "Reasoning",
                                                    "description": "Provides a clear explanation of why the chosen prompt facilitates chain of thought.",
                                                    "min_score": 0,
                                                    "max_score": 4
                                                },
                                                {
                                                    "name": "Conciseness",
                                                    "description": "Answer is direct and to the point.",
                                                    "min_score": 0,
                                                    "max_score": 3
                                                }
                                            ]
                                        },
                                        "context": "This question requires the learner to identify the CoT prompt and articulate the reasons for its effectiveness.  Prompt 2 is better as it explicitly asks the model to think step by step. A zero-shot CoT prompt is the simplest to implement, and can be implemented by simply adding 'Let's think step by step' to the original prompt as shown in the reference material."
                                    },
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Analyze the following prompt and suggest improvements based on CoT best practices:\n\n'Summarize the key events of World War II.'",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "CoT Improvement Suggestions",
                                            "criteria": [
                                                {
                                                    "name": "Specificity",
                                                    "description": "Suggestions clearly address how to make the prompt more specific and reasoning-focused.",
                                                    "min_score": 0,
                                                    "max_score": 4
                                                },
                                                {
                                                    "name": "Relevance",
                                                    "description": "Proposed changes are relevant to CoT principles.",
                                                    "min_score": 0,
                                                    "max_score": 3
                                                },
                                                {
                                                    "name": "Practicality",
                                                    "description": "Improvements are feasible and can be readily implemented.",
                                                    "min_score": 0,
                                                    "max_score": 3
                                                }
                                            ]
                                        },
                                        "context": "A good CoT prompt sets the temperature to 0, puts the answer after the reasoning and separates the final answer from the reasoning. The prompt should be re-written to be more specific, asking for an outline of key events before the summary and asking for the summary to be placed at the end. "
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Document the Various Prompt Attempts",
                    "description": "Documenting prompt attempts so you can learn what went well and what did not.",
                    "tasks": [
                        {
                            "name": "Document prompts",
                            "description": "Explain why is important to document your prompt attempts. What's the best way to document the prompts?",
                            "type": "learning_material",
                            "details": {
                                "content": "The last tip was mentioned before in this chapter, but we can't stress enough how important it is: document your prompt attempts in full detail so you can learn over time what went well and what did not.\n\nPrompt outputs can differ across models, across sampling settings, and even across different versions of the same model. Moreover, even across identical prompts to the same model, small differences in output sentence formatting and word choice can occur. (For example, as mentioned previously, if two tokens have the same predicted probability, ties may be broken randomly. This can then impact subsequent predicted tokens.).\n\nWe recommend creating a Google Sheet with Table 21 as a template. The advantages of this approach are that you have a complete record when you inevitably have to revisit your prompting work-either to pick it up in the future (you'd be surprised how much you can forget after just a short break), to test prompt performance on different versions of a model, and to help debug future errors.\n\nBeyond the fields in this table, it's also helpful to track the version of the prompt (iteration), a field to capture if the result was OK/NOT OK/SOMETIMES OK, and a field to capture feedback. If you're lucky enough to be using Vertex Al Studio, save your prompts (using the same name and version as listed in your documentation) and track the hyperlink to the saved prompt in the table. This way, you're always one click away from re-running your prompts.\n\nWhen working on a retrieval augmented generation system, you should also capture the specific aspects of the RAG system that impact what content was inserted into the prompt, including the query, chunk settings, chunk output, and other information.\n\nOnce you feel the prompt is close to perfect, take it to your project codebase. And in the codebase, save prompts in a separate file from code, so it's easier to maintain. Finally, ideally your prompts are part of an operationalized system, and as a prompt engineer you should rely on automated tests and evaluation procedures to understand how well your prompt generalizes to a task.\n\nPrompt engineering is an iterative process. Craft and test different prompts, analyze, and document the results. Refine your prompt based on the model's performance. Keep experimenting until you achieve the desired output. When you change a model or model configuration, go back and keep experimenting with the previously used prompts."
                            }
                        },
                        {
                            "name": "Follow up",
                            "description": "Ask learners to document the prompts they have been using and keep track of the performance of each of them.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Reflect on the prompts you've been using in your prompt engineering projects. Document at least three of these prompts, detailing their purpose, structure, and the configurations (e.g., temperature, top-K, top-P) used. Then, analyze the performance of each prompt, noting what worked well and areas for improvement. Finally, outline a strategy for iterating on these prompts based on your observations. Submit your documented prompts and performance analyses.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Prompt Documentation and Analysis",
                                            "criteria": [
                                                {
                                                    "name": "Completeness",
                                                    "description": "All prompts are documented with purpose, structure and configurations.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Clarity",
                                                    "description": "The prompts and analysis are easy to understand.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Insightfulness",
                                                    "description": "The performance analysis provides good insights into what worked well and what didn't.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Iteration Strategy",
                                                    "description": "The strategy for iterating on the prompts is well-thought out.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "This question aims to assess the learner's ability to document and analyze prompts effectively. The documentation should include the purpose, structure, and configurations used for each prompt. The analysis should provide insights into what worked well and areas for improvement. The iteration strategy should be based on the observations made during the analysis."
                                    }
                                ]
                            }
                        }
                    ]
                }
            ]
        },
        {
            "name": "Conclusion: Prompt Engineering for Real-World Applications",
            "concepts": [
                {
                    "name": "Putting It All Together",
                    "description": "Recap of key concepts and techniques learned throughout the course.",
                    "tasks": [
                        {
                            "name": "Course Summary",
                            "description": "Summarize the main modules and concepts covered in the course.",
                            "type": "learning_material",
                            "details": {
                                "content": "This whitepaper discussed prompt engineering. We explored various prompting techniques, such as:\n\n*   Zero prompting\n*   Few shot prompting\n*   System prompting\n*   Role prompting\n*   Contextual prompting\n*   Step-back prompting\n*   Chain of thought\n*   Self consistency\n*   Tree of thoughts\n*   ReAct\n\nWe even looked into ways how you can automate your prompts.\n\nThe whitepaper then discussed the challenges of gen AI like the problems that can happen when your prompts are insufficient. We closed with best practices on how to become a better prompt engineer."
                            }
                        },
                        {
                            "name": "Final Quiz",
                            "description": "Comprehensive quiz covering all the topics taught in the course.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "objective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Prompt engineering involves:",
                                        "correct_answer": "Designing high-quality prompts to guide LLMs for accurate outputs.",
                                        "scorecard": null,
                                        "context": "This question tests the understanding of the core definition of prompt engineering as discussed in the document. The correct answer reflects the understanding of how to design prompts for LLMs, optimizing prompt length, and evaluating writing style and structure in relation to the task. Options: Tinkering to find the best prompt, Optimizing prompt length, Evaluating a prompt's writing style and structure in relation to the task"
                                    },
                                    {
                                        "question_type": "objective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Which of the following is NOT a sampling control method used in LLMs:",
                                        "correct_answer": "Output Length",
                                        "scorecard": null,
                                        "context": "The question verifies understanding of sampling control methods including Temperature, Top-K, and Top-P. Output length is a LLM output configuration but not a sampling control technique. Options: Temperature, Top-K, Output Length, Top-P"
                                    },
                                    {
                                        "question_type": "objective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "What does a 'zero-shot' prompt involve?",
                                        "correct_answer": "Providing a description of the task with some text for the LLM to start without examples.",
                                        "scorecard": null,
                                        "context": "This question aims to confirm understanding of a 'zero-shot' prompt. A zero-shot prompt offers a description of a task and some text for the LLM to start with. Options: Providing multiple examples to the model, Providing a description of the task with some text for the LLM to start without examples., Assigning a specific role to the language model to adopt, Providing specific details or background information relevant to the current conversation or task"
                                    },
                                    {
                                        "question_type": "objective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "In prompt engineering, what is the purpose of 'system prompting'?",
                                        "correct_answer": "To set the overall context and purpose for the language model.",
                                        "scorecard": null,
                                        "context": "This question relates to the understanding of system prompting, where you set the overall context and purpose for the language model, like translating a language or classifying a review. Options: Providing immediate, task-specific information to guide the response, To frame the model's output style and voice, To set the overall context and purpose for the language model., To assign a specific character or identity for the language model to adopt"
                                    },
                                    {
                                        "question_type": "objective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "What is the main goal of using Chain of Thought (CoT) prompting?",
                                        "correct_answer": "To improve the reasoning capabilities of LLMs by generating intermediate reasoning steps.",
                                        "scorecard": null,
                                        "context": "This question is centered on Chain of Thought (CoT) prompting and to confirm the understanding that it involves generating intermediate reasoning steps that help the LLM generate more accurate answers. Options: To set the overall context and purpose for the language model, To improve the reasoning capabilities of LLMs by generating intermediate reasoning steps., To assign a specific character or identity for the language model to adopt, To provide specific details or background information relevant to the current conversation or task"
                                    },
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Describe a scenario where using JSON Schemas for LLM inputs would be beneficial and explain why.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "JSON Schema Use Case Evaluation",
                                            "criteria": [
                                                {
                                                    "name": "Relevance",
                                                    "description": "The scenario is relevant and appropriate for using JSON Schemas with LLMs.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Explanation",
                                                    "description": "The explanation clearly articulates the benefits of using JSON Schemas in the described scenario, including improved data accuracy and reduced risk of misinterpretation.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Clarity",
                                                    "description": "The description is clear, concise, and easy to understand.",
                                                    "min_score": 0,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "This question tests the understanding of JSON Schemas and their advantages in structuring LLM inputs. The student should describe a practical use case where using JSON Schemas improves data accuracy and reduces misinterpretation."
                                    }
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Capstone Project: Building a Prompt-Powered Application",
                    "description": "Applying prompt engineering skills to develop a practical application.",
                    "tasks": [
                        {
                            "name": "Project Overview",
                            "description": "Introduce the capstone project, outlining the problem statement and desired functionality. (e.g., build a chatbot for a specific purpose, create a content generation tool, etc.)",
                            "type": "learning_material",
                            "details": {
                                "content": "In this capstone project, you will apply the prompt engineering techniques you've learned throughout this course to develop a practical application. This project is designed to be a hands-on experience where you can solidify your understanding and showcase your skills.\n\nThe Problem Statement:\n\n*   Begin by identifying a specific problem or need that can be addressed using a prompt-powered application.\n*   This could be anything from automating a task, generating creative content, providing information in a user-friendly way, or solving a particular challenge in a specific domain.\n*   Examples include:\n    *   Building a chatbot to assist with customer service for a local business.\n    *   Creating a tool that generates marketing copy for social media campaigns.\n    *   Developing an application that summarizes news articles based on user interests.\n    *   Designing a virtual assistant that helps users with task management and scheduling.\n\nDesired Functionality:\n\n*   Clearly define the desired functionality of your application. What tasks should it be able to perform? What kind of output should it produce?\n*   Consider the user experience and how users will interact with your application.\n*   Outline the key features and capabilities that will make your application useful and effective.\n\nProject Goals:\n\n*   Apply prompt engineering techniques to design and optimize prompts for your application.\n*   Utilize a large language model (LLM) to generate desired outputs based on your prompts.\n*   Develop a functional application with a user-friendly interface.\n*   Evaluate the performance of your application and identify areas for improvement.\n*   Demonstrate your ability to solve a real-world problem using prompt engineering skills."
                            }
                        },
                        {
                            "name": "Capstone Project",
                            "description": "Learners work individually or in groups to design, implement, and evaluate their prompt-powered application, applying the knowledge and skills acquired throughout the course.",
                            "type": "quiz",
                            "details": {
                                "questions": [
                                    {
                                        "question_type": "subjective",
                                        "answer_type": "text",
                                        "coding_languages": null,
                                        "content": "Imagine you are tasked with developing a prompt-powered application to assist students in learning a new language. Describe your application's core functionality, target users, and how prompt engineering principles will be applied to create an engaging and effective learning experience.",
                                        "correct_answer": null,
                                        "scorecard": {
                                            "title": "Project Design",
                                            "criteria": [
                                                {
                                                    "name": "Functionality",
                                                    "description": "The extent to which the application's core functionality is well-defined and addresses a clear need in language learning.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Target Users",
                                                    "description": "How well the target users are identified and their needs considered in the design of the application.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Prompt Engineering",
                                                    "description": "How effectively prompt engineering principles are integrated into the application to enhance user experience and learning outcomes.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                },
                                                {
                                                    "name": "Originality",
                                                    "description": "The degree of innovation and uniqueness in the application's design and approach.",
                                                    "min_score": 1,
                                                    "max_score": 5
                                                }
                                            ]
                                        },
                                        "context": "This question assesses the learner's ability to apply prompt engineering principles in a practical application. Evaluate the project design based on the clarity of functionality, understanding of target users, effective integration of prompt engineering, and originality."
                                    }
                                ]
                            }
                        }
                    ]
                }
            ]
        }
    ]
}